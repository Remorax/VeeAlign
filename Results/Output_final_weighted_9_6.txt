2020-09-15 15:49:38.953576: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-09-15 15:49:42.070233: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1
2020-09-15 15:49:42.184313: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:14:00.0 name: Tesla K80 computeCapability: 3.7
coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s
2020-09-15 15:49:42.184406: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-09-15 15:49:42.186462: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
2020-09-15 15:49:42.187967: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2020-09-15 15:49:42.188413: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2020-09-15 15:49:42.190284: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2020-09-15 15:49:42.191733: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10
2020-09-15 15:49:42.191981: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudnn.so.7'; dlerror: libcudnn.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/share/lsf-9.1.3/10.1/linux3.10-glibc2.17-x86_64/lib:=/opt/share/gcc-4.9.2_rhel6/x86_64/lib/:/opt/share/gcc-4.9.2_rhel6/x86_64/lib64:/opt/share/Python-3.6.2/x86_64/lib:=/opt/share/gcc-5.4.0/x86_64/lib/:/opt/share/gcc-5.4.0/x86_64/lib64:/opt/share/isl-0.17/x86_64/lib/:/opt/share/protobuf-3.1.0/x86_64/lib/:/opt/share/leveldb-1.19/x86_64/lib/:/opt/share/boost-1.62.0/x86_64/lib/:/opt/share/torch-7/x86_64/install/lib:/opt/share/Python-2.7.12/x86_64/lib:/opt/share/Python-3.5.2/x86_64/lib:/opt/share/cuDNN-v5.1-8.0/cuda/lib64:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/opt/share/cuda-8.0/
2020-09-15 15:49:42.192005: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1753] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2020-09-15 15:49:42.192321: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-09-15 15:49:42.199797: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2600110000 Hz
2020-09-15 15:49:42.199976: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x561da4214140 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-09-15 15:49:42.199994: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-09-15 15:49:42.201942: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-09-15 15:49:42.201975: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      
Prefix path:  /dccstor/cogfin/arvind/da/VeeAlign/
Ontologies being aligned are:  [('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/confOf.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/conference.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/conference.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/conference.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/conference.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/confOf.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/conference.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/confOf.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/confOf.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/confOf.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl')]
Total number of extracted unique classes and properties from entire RA set:  829
Constructing abbrevation resolution dict....
Results after abbreviation resolution:  {'PC': 'Program Committee', 'OC': 'Organizing Committee'}
Resolving abbreviations...
Number of entities: 119639
Starting sliding window evaluation...
Step 0/7
Val onto:  [('confof', 'ekaw')] test_onto:  [('conference', 'sigkdd')]
Training size: 112565 Testing size: 3871
Epoch: 0 Idx: 0 Loss: 0.21463997265152213
Epoch: 0 Idx: 5000 Loss: 0.00843308183026669
Epoch: 1 Idx: 0 Loss: 0.005298754591414706
Epoch: 1 Idx: 5000 Loss: 0.007634280997679998
Epoch: 2 Idx: 0 Loss: 0.015109279305196212
Epoch: 2 Idx: 5000 Loss: 0.014225312775566913
Epoch: 3 Idx: 0 Loss: 0.010052802857712791
Epoch: 3 Idx: 5000 Loss: 0.021193658149309205
Epoch: 4 Idx: 0 Loss: 0.017374544559304886
Epoch: 4 Idx: 5000 Loss: 0.011467716155960608
Epoch: 5 Idx: 0 Loss: 0.02951218980206822
Epoch: 5 Idx: 5000 Loss: 0.0128659311797459
Epoch: 6 Idx: 0 Loss: 0.02095741070994291
Epoch: 6 Idx: 5000 Loss: 0.016550649894136477
Epoch: 7 Idx: 0 Loss: 0.008782421168747428
Epoch: 7 Idx: 5000 Loss: 0.010537252240900612
Epoch: 8 Idx: 0 Loss: 0.01012686623203833
Epoch: 8 Idx: 5000 Loss: 0.008269522153871646
Epoch: 9 Idx: 0 Loss: 0.009406785507133593
Epoch: 9 Idx: 5000 Loss: 0.010634646567926061
Epoch: 10 Idx: 0 Loss: 0.01084590283834808
Epoch: 10 Idx: 5000 Loss: 0.019715287758036784
Epoch: 11 Idx: 0 Loss: 0.023286634422162625
Epoch: 11 Idx: 5000 Loss: 0.01614772321664691
Epoch: 12 Idx: 0 Loss: 0.0032905172995698504
Epoch: 12 Idx: 5000 Loss: 0.02250876833491975
Epoch: 13 Idx: 0 Loss: 0.008735898355846643
Epoch: 13 Idx: 5000 Loss: 0.02402739242807228
Epoch: 14 Idx: 0 Loss: 0.013611877585966204
Epoch: 14 Idx: 5000 Loss: 0.009406835000394072
Epoch: 15 Idx: 0 Loss: 0.026101966418618942
Epoch: 15 Idx: 5000 Loss: 0.019417506961240186
Epoch: 16 Idx: 0 Loss: 0.009462339970451327
Epoch: 16 Idx: 5000 Loss: 0.013013841301214612
Epoch: 17 Idx: 0 Loss: 0.014411608313691721
Epoch: 17 Idx: 5000 Loss: 0.010341973329910463
Epoch: 18 Idx: 0 Loss: 0.016616424469626568
Epoch: 18 Idx: 5000 Loss: 0.020281013686057458
Epoch: 19 Idx: 0 Loss: 0.02709409046117357
Epoch: 19 Idx: 5000 Loss: 0.015365047186080616
Epoch: 20 Idx: 0 Loss: 0.00725274949830663
Epoch: 20 Idx: 5000 Loss: 0.014384966325984204
Epoch: 21 Idx: 0 Loss: 0.022374657097864726
Epoch: 21 Idx: 5000 Loss: 0.020358286400401233
Epoch: 22 Idx: 0 Loss: 0.026049532173139284
Epoch: 22 Idx: 5000 Loss: 0.04825907636168394
Epoch: 23 Idx: 0 Loss: 0.030541695543668496
Epoch: 23 Idx: 5000 Loss: 0.009642785466824655
Epoch: 24 Idx: 0 Loss: 0.021166836759098136
Epoch: 24 Idx: 5000 Loss: 0.014956005336005739
Epoch: 25 Idx: 0 Loss: 0.017551819056556967
Epoch: 25 Idx: 5000 Loss: 0.025179277522583584
Epoch: 26 Idx: 0 Loss: 0.0427442454427415
Epoch: 26 Idx: 5000 Loss: 0.017252025759830508
Epoch: 27 Idx: 0 Loss: 0.016268765047393663
Epoch: 27 Idx: 5000 Loss: 0.011208483936318805
Epoch: 28 Idx: 0 Loss: 0.0118384185405805
Epoch: 28 Idx: 5000 Loss: 0.020919610420302062
Epoch: 29 Idx: 0 Loss: 0.021135555990556557
Epoch: 29 Idx: 5000 Loss: 0.0062367077268586195
Epoch: 30 Idx: 0 Loss: 0.01869727690926723
Epoch: 30 Idx: 5000 Loss: 0.004502848311226505
Epoch: 31 Idx: 0 Loss: 0.014398354063247389
Epoch: 31 Idx: 5000 Loss: 0.03156982459562395
Epoch: 32 Idx: 0 Loss: 0.039252015780588644
Epoch: 32 Idx: 5000 Loss: 0.005480226721866159
Epoch: 33 Idx: 0 Loss: 0.029300770198047436
Epoch: 33 Idx: 5000 Loss: 0.016966908333529745
Epoch: 34 Idx: 0 Loss: 0.02244572629406446
Epoch: 34 Idx: 5000 Loss: 0.026106794893096454
Epoch: 35 Idx: 0 Loss: 0.008654987476366014
Epoch: 35 Idx: 5000 Loss: 0.016394712343929495
Epoch: 36 Idx: 0 Loss: 0.01343649275260598
Epoch: 36 Idx: 5000 Loss: 0.01631086786781381
Epoch: 37 Idx: 0 Loss: 0.035380279711325305
Epoch: 37 Idx: 5000 Loss: 0.00975304308932325
Epoch: 38 Idx: 0 Loss: 0.015304690999891372
Epoch: 38 Idx: 5000 Loss: 0.016016260184349
Epoch: 39 Idx: 0 Loss: 0.006665234895708465
Epoch: 39 Idx: 5000 Loss: 0.009846071352494126
Epoch: 40 Idx: 0 Loss: 0.008759775997461278
Epoch: 40 Idx: 5000 Loss: 0.015069512674370632
Epoch: 41 Idx: 0 Loss: 0.018352530140394827
Epoch: 41 Idx: 5000 Loss: 0.011438420637874262
Epoch: 42 Idx: 0 Loss: 0.007118176504355777
Epoch: 42 Idx: 5000 Loss: 0.01403551977898666
Epoch: 43 Idx: 0 Loss: 0.020954006230969283
Epoch: 43 Idx: 5000 Loss: 0.010148803152094751
Epoch: 44 Idx: 0 Loss: 0.010790690510786308
Epoch: 44 Idx: 5000 Loss: 0.012060177047576344
Epoch: 45 Idx: 0 Loss: 0.00862795790603742
Epoch: 45 Idx: 5000 Loss: 0.01744443598322394
Epoch: 46 Idx: 0 Loss: 0.016882568266890476
Epoch: 46 Idx: 5000 Loss: 0.01545502403062842
Epoch: 47 Idx: 0 Loss: 0.011411482453984455
Epoch: 47 Idx: 5000 Loss: 0.012585424018852317
Epoch: 48 Idx: 0 Loss: 0.015090092349297297
Epoch: 48 Idx: 5000 Loss: 0.03299377040470742
Epoch: 49 Idx: 0 Loss: 0.009919477609930047
Epoch: 49 Idx: 5000 Loss: 0.02026421101249498
Len (direct inputs):  429
Inputs len 2744 15 3856
Len (direct inputs):  1127
Starting sliding window evaluation...
Step 2/7
Val onto:  [('conference', 'ekaw')] test_onto:  [('cmt', 'sigkdd')]
Training size: 111450 Testing size: 2364
Epoch: 0 Idx: 0 Loss: 0.1316527086322346
Epoch: 0 Idx: 5000 Loss: 0.013444540467053614
Epoch: 1 Idx: 0 Loss: 0.012332173197461686
Epoch: 1 Idx: 5000 Loss: 0.011530392036782423
Epoch: 2 Idx: 0 Loss: 0.028333111826342156
Epoch: 2 Idx: 5000 Loss: 0.015405667498906993
Epoch: 3 Idx: 0 Loss: 0.014446272948893234
Epoch: 3 Idx: 5000 Loss: 0.018384722022076933
Epoch: 4 Idx: 0 Loss: 0.02513502590095481
Epoch: 4 Idx: 5000 Loss: 0.009630613712442901
Epoch: 5 Idx: 0 Loss: 0.010855897242457562
Epoch: 5 Idx: 5000 Loss: 0.03941431029190314
Epoch: 6 Idx: 0 Loss: 0.012064096971551048
Epoch: 6 Idx: 5000 Loss: 0.03971037626051029
Epoch: 7 Idx: 0 Loss: 0.010177245320171037
Epoch: 7 Idx: 5000 Loss: 0.03854532688777747
Epoch: 8 Idx: 0 Loss: 0.021551673681176012
Epoch: 8 Idx: 5000 Loss: 0.034858777292152636
Epoch: 9 Idx: 0 Loss: 0.014514190566387484
Epoch: 9 Idx: 5000 Loss: 0.007111825652409585
Epoch: 10 Idx: 0 Loss: 0.010994281983467628
Epoch: 10 Idx: 5000 Loss: 0.01418560431835272
Epoch: 11 Idx: 0 Loss: 0.011994215695964854
Epoch: 11 Idx: 5000 Loss: 0.00977904977264162
Epoch: 12 Idx: 0 Loss: 0.008766712643006325
Epoch: 12 Idx: 5000 Loss: 0.016717941660243752
Epoch: 13 Idx: 0 Loss: 0.00820876293173867
Epoch: 13 Idx: 5000 Loss: 0.010692888616368395
Epoch: 14 Idx: 0 Loss: 0.006637942798212438
Epoch: 14 Idx: 5000 Loss: 0.016682389597860087
Epoch: 15 Idx: 0 Loss: 0.017240481558843227
Epoch: 15 Idx: 5000 Loss: 0.011976310346540705
Epoch: 16 Idx: 0 Loss: 0.013742936314102977
Epoch: 16 Idx: 5000 Loss: 0.01904767151190444
Epoch: 17 Idx: 0 Loss: 0.012455494744128598
Epoch: 17 Idx: 5000 Loss: 0.005959170811691982
Epoch: 18 Idx: 0 Loss: 0.014943825918573945
Epoch: 18 Idx: 5000 Loss: 0.011895229696539828
Epoch: 19 Idx: 0 Loss: 0.022115481952408873
Epoch: 19 Idx: 5000 Loss: 0.006264166014162126
Epoch: 20 Idx: 0 Loss: 0.008191170971611365
Epoch: 20 Idx: 5000 Loss: 0.02166428593557823
Epoch: 21 Idx: 0 Loss: 0.01772492581910842
Epoch: 21 Idx: 5000 Loss: 0.019982208236509817
Epoch: 22 Idx: 0 Loss: 0.031620951609668865
Epoch: 22 Idx: 5000 Loss: 0.014420848302461178
Epoch: 23 Idx: 0 Loss: 0.006426272170572697
Epoch: 23 Idx: 5000 Loss: 0.008689634605191562
Epoch: 24 Idx: 0 Loss: 0.009471787593813827
Epoch: 24 Idx: 5000 Loss: 0.009008933468694339
Epoch: 25 Idx: 0 Loss: 0.02039597823802452
Epoch: 25 Idx: 5000 Loss: 0.019874383366887738
Epoch: 26 Idx: 0 Loss: 0.00783215273199052
Epoch: 26 Idx: 5000 Loss: 0.009951713761750029
Epoch: 27 Idx: 0 Loss: 0.017522860056358266
Epoch: 27 Idx: 5000 Loss: 0.017248907659429758
Epoch: 28 Idx: 0 Loss: 0.006628361865542276
Epoch: 28 Idx: 5000 Loss: 0.020500061955262487
Epoch: 29 Idx: 0 Loss: 0.005288553970934234
Epoch: 29 Idx: 5000 Loss: 0.03308478209432114
Epoch: 30 Idx: 0 Loss: 0.030032581445614562
Epoch: 30 Idx: 5000 Loss: 0.03542003959344655
Epoch: 31 Idx: 0 Loss: 0.017234509490247744
Epoch: 31 Idx: 5000 Loss: 0.017587797114538457
Epoch: 32 Idx: 0 Loss: 0.008194957950363147
Epoch: 32 Idx: 5000 Loss: 0.009815923387218466
Epoch: 33 Idx: 0 Loss: 0.01124904619263738
Epoch: 33 Idx: 5000 Loss: 0.009339612608898193
Epoch: 34 Idx: 0 Loss: 0.009769318511409584
Epoch: 34 Idx: 5000 Loss: 0.013267125321809793
Epoch: 35 Idx: 0 Loss: 0.014192765163090659
Epoch: 35 Idx: 5000 Loss: 0.009322855361933023
Epoch: 36 Idx: 0 Loss: 0.03291244038279748
Epoch: 36 Idx: 5000 Loss: 0.009467817218615318
Epoch: 37 Idx: 0 Loss: 0.017382277101659804
Epoch: 37 Idx: 5000 Loss: 0.013934197697158267
Epoch: 38 Idx: 0 Loss: 0.01097822101925359
Epoch: 38 Idx: 5000 Loss: 0.010120827942299578
Epoch: 39 Idx: 0 Loss: 0.008673200403551108
Epoch: 39 Idx: 5000 Loss: 0.018680845131343618
Epoch: 40 Idx: 0 Loss: 0.004089037390460391
Epoch: 40 Idx: 5000 Loss: 0.020423239970150227
Epoch: 41 Idx: 0 Loss: 0.01472455717061794
Epoch: 41 Idx: 5000 Loss: 0.02803506424434731
Epoch: 42 Idx: 0 Loss: 0.007672443356598264
Epoch: 42 Idx: 5000 Loss: 0.03660595521837104
Epoch: 43 Idx: 0 Loss: 0.043712087608190724
Epoch: 43 Idx: 5000 Loss: 0.011933602690616807
Epoch: 44 Idx: 0 Loss: 0.01517292412097741
Epoch: 44 Idx: 5000 Loss: 0.010520846324288178
Epoch: 45 Idx: 0 Loss: 0.01173766851004742
Epoch: 45 Idx: 5000 Loss: 0.011333897050809727
Epoch: 46 Idx: 0 Loss: 0.011686745892703838
Epoch: 46 Idx: 5000 Loss: 0.013280929461874163
Epoch: 47 Idx: 0 Loss: 0.012311589542920559
Epoch: 47 Idx: 5000 Loss: 0.0307526386181929
Epoch: 48 Idx: 0 Loss: 0.018861629374061308
Epoch: 48 Idx: 5000 Loss: 0.023290868108407373
Epoch: 49 Idx: 0 Loss: 0.016916423926614133
Epoch: 49 Idx: 5000 Loss: 0.012838692883601333
Len (direct inputs):  1737
Inputs len 1372 12 2352
Len (direct inputs):  992
Starting sliding window evaluation...
Step 4/7
Val onto:  [('ekaw', 'sigkdd')] test_onto:  [('cmt', 'conference')]
Training size: 111356 Testing size: 4145
Epoch: 0 Idx: 0 Loss: 0.13153861419438742
Epoch: 0 Idx: 5000 Loss: 0.01343377307150144
Epoch: 1 Idx: 0 Loss: 0.015621104861952116
Epoch: 1 Idx: 5000 Loss: 0.009196528495014218
Epoch: 2 Idx: 0 Loss: 0.01450024542989932
Epoch: 2 Idx: 5000 Loss: 0.011912697504848772
Epoch: 3 Idx: 0 Loss: 0.0183830819556458
Epoch: 3 Idx: 5000 Loss: 0.017265031346088183
Epoch: 4 Idx: 0 Loss: 0.02484989392782551
Epoch: 4 Idx: 5000 Loss: 0.017516395033558455
Epoch: 5 Idx: 0 Loss: 0.029932779623147145
Epoch: 5 Idx: 5000 Loss: 0.019136705596686416
Epoch: 6 Idx: 0 Loss: 0.009163164707944956
Epoch: 6 Idx: 5000 Loss: 0.040552340621843125
Epoch: 7 Idx: 0 Loss: 0.013900822440615567
Epoch: 7 Idx: 5000 Loss: 0.014163985132019908
Epoch: 8 Idx: 0 Loss: 0.010325450434955728
Epoch: 8 Idx: 5000 Loss: 0.02450532151748045
Epoch: 9 Idx: 0 Loss: 0.01866602530099017
Epoch: 9 Idx: 5000 Loss: 0.025948514276667608
Epoch: 10 Idx: 0 Loss: 0.014298535212302933
Epoch: 10 Idx: 5000 Loss: 0.0055724021398138165
Epoch: 11 Idx: 0 Loss: 0.01757992635825644
Epoch: 11 Idx: 5000 Loss: 0.004725450578589545
Epoch: 12 Idx: 0 Loss: 0.03498359469047507
Epoch: 12 Idx: 5000 Loss: 0.019024668555835126
Epoch: 13 Idx: 0 Loss: 0.011768167725336762
Epoch: 13 Idx: 5000 Loss: 0.012875218607891746
Epoch: 14 Idx: 0 Loss: 0.011093194725158198
Epoch: 14 Idx: 5000 Loss: 0.02433300672104647
Epoch: 15 Idx: 0 Loss: 0.009564526633947376
Epoch: 15 Idx: 5000 Loss: 0.02201939176947562
Epoch: 16 Idx: 0 Loss: 0.042098590404427386
Epoch: 16 Idx: 5000 Loss: 0.0333295675881253
Epoch: 17 Idx: 0 Loss: 0.012163090670598804
Epoch: 17 Idx: 5000 Loss: 0.017380404718110542
Epoch: 18 Idx: 0 Loss: 0.028365593452385165
Epoch: 18 Idx: 5000 Loss: 0.019049698105972762
Epoch: 19 Idx: 0 Loss: 0.015113163050100903
Epoch: 19 Idx: 5000 Loss: 0.008025514111886508
Epoch: 20 Idx: 0 Loss: 0.006407042753494128
Epoch: 20 Idx: 5000 Loss: 0.018709871866205263
Epoch: 21 Idx: 0 Loss: 0.008459390668185601
Epoch: 21 Idx: 5000 Loss: 0.031605577068643656
Epoch: 22 Idx: 0 Loss: 0.019992699453660265
Epoch: 22 Idx: 5000 Loss: 0.015280261036930021
Epoch: 23 Idx: 0 Loss: 0.013916651946012184
Epoch: 23 Idx: 5000 Loss: 0.013197574293394368
Epoch: 24 Idx: 0 Loss: 0.011062821425952768
Epoch: 24 Idx: 5000 Loss: 0.00649625200759299
Epoch: 25 Idx: 0 Loss: 0.016609046352289717
Epoch: 25 Idx: 5000 Loss: 0.016754052606059347
Epoch: 26 Idx: 0 Loss: 0.010855514862294077
Epoch: 26 Idx: 5000 Loss: 0.02397766634896893
Epoch: 27 Idx: 0 Loss: 0.014162078974647097
Epoch: 27 Idx: 5000 Loss: 0.009220004045399946
Epoch: 28 Idx: 0 Loss: 0.02149982474971392
Epoch: 28 Idx: 5000 Loss: 0.008450179928650915
Epoch: 29 Idx: 0 Loss: 0.028584296601875062
Epoch: 29 Idx: 5000 Loss: 0.009189553691216906
Epoch: 30 Idx: 0 Loss: 0.010502430663384119
Epoch: 30 Idx: 5000 Loss: 0.024797100976392247
Epoch: 31 Idx: 0 Loss: 0.011066216050210595
Epoch: 31 Idx: 5000 Loss: 0.012010508532820204
Epoch: 32 Idx: 0 Loss: 0.006455845806619892
Epoch: 32 Idx: 5000 Loss: 0.029047666748585024
Epoch: 33 Idx: 0 Loss: 0.0066727138632845144
Epoch: 33 Idx: 5000 Loss: 0.02291704386561267
Epoch: 34 Idx: 0 Loss: 0.03275227681810787
Epoch: 34 Idx: 5000 Loss: 0.008191636631791871
Epoch: 35 Idx: 0 Loss: 0.013071198339823188
Epoch: 35 Idx: 5000 Loss: 0.01311466905672612
Epoch: 36 Idx: 0 Loss: 0.007250539780726637
Epoch: 36 Idx: 5000 Loss: 0.02702058875148397
Epoch: 37 Idx: 0 Loss: 0.008569044801106436
Epoch: 37 Idx: 5000 Loss: 0.018731358935643858
Epoch: 38 Idx: 0 Loss: 0.009735981056225907
Epoch: 38 Idx: 5000 Loss: 0.011154990911586997
Epoch: 39 Idx: 0 Loss: 0.021569657945201257
Epoch: 39 Idx: 5000 Loss: 0.011171650993936869
Epoch: 40 Idx: 0 Loss: 0.024930553186181106
Epoch: 40 Idx: 5000 Loss: 0.015335175270006971
Epoch: 41 Idx: 0 Loss: 0.008503195112587113
Epoch: 41 Idx: 5000 Loss: 0.011540427972906957
Epoch: 42 Idx: 0 Loss: 0.020483957605288528
Epoch: 42 Idx: 5000 Loss: 0.008527808857000453
Epoch: 43 Idx: 0 Loss: 0.008788546240326277
Epoch: 43 Idx: 5000 Loss: 0.010794605780765685
Epoch: 44 Idx: 0 Loss: 0.01525744150903527
Traceback (most recent call last):
  File "main.py", line 514, in <module>
    outputs = model(node_elems, inp_elems)
  File "/u/harshitk/miniconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "main.py", line 312, in forward
    feature_emb = self.name_embedding(features[i]) #  dim: (2, batch_size, max_types, max_paths, max_pathlen, 512)
  File "/u/harshitk/miniconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/u/harshitk/miniconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/sparse.py", line 126, in forward
    self.norm_type, self.scale_grad_by_freq, self.sparse)
  File "/u/harshitk/miniconda3/envs/py37/lib/python3.7/site-packages/torch/nn/functional.py", line 1814, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
KeyboardInterrupt

------------------------------------------------------------
Sender: LSF System <rer@dccxc237>
Subject: Job 4066882: <python main.py 6 9 False True> in cluster <dcc> Exited

Job <python main.py 6 9 False True> was submitted from host <dccxl004> by user <harshitk> in cluster <dcc> at Tue Sep 15 15:48:40 2020
Job was executed on host(s) <dccxc237>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep 15 15:49:37 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/VeeAlign/src> was used as the working directory.
Started at Tue Sep 15 15:49:37 2020
Terminated at Wed Sep 16 04:38:40 2020
Results reported at Wed Sep 16 04:38:40 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python main.py 6 9 False True
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   46138.96 sec.
    Max Memory :                                 2950 MB
    Average Memory :                             2740.58 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40467.00 MB
    Max Swap :                                   2 MB
    Max Processes :                              3
    Max Threads :                                13
    Run time :                                   46170 sec.
    Turnaround time :                            46200 sec.

The output (if any) is above this job summary.

