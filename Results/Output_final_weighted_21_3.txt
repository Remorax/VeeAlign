2020-09-15 15:48:42.250818: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-09-15 15:48:49.142613: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1
2020-09-15 15:48:49.253998: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:1e:00.0 name: Tesla K80 computeCapability: 3.7
coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s
2020-09-15 15:48:49.254073: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-09-15 15:48:49.256428: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
2020-09-15 15:48:49.276014: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2020-09-15 15:48:49.311964: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2020-09-15 15:48:49.361105: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2020-09-15 15:48:49.386096: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10
2020-09-15 15:48:49.386663: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudnn.so.7'; dlerror: libcudnn.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/share/lsf-9.1.3/10.1/linux3.10-glibc2.17-x86_64/lib:=/opt/share/gcc-4.9.2_rhel6/x86_64/lib/:/opt/share/gcc-4.9.2_rhel6/x86_64/lib64:/opt/share/Python-3.6.2/x86_64/lib:=/opt/share/gcc-5.4.0/x86_64/lib/:/opt/share/gcc-5.4.0/x86_64/lib64:/opt/share/isl-0.17/x86_64/lib/:/opt/share/protobuf-3.1.0/x86_64/lib/:/opt/share/leveldb-1.19/x86_64/lib/:/opt/share/boost-1.62.0/x86_64/lib/:/opt/share/torch-7/x86_64/install/lib:/opt/share/Python-2.7.12/x86_64/lib:/opt/share/Python-3.5.2/x86_64/lib:/opt/share/cuDNN-v5.1-8.0/cuda/lib64:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/opt/share/cuda-8.0/
2020-09-15 15:48:49.386687: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1753] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2020-09-15 15:48:49.387180: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-09-15 15:48:49.422325: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2600180000 Hz
2020-09-15 15:48:49.422601: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55d73d42f830 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-09-15 15:48:49.422622: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-09-15 15:48:49.425389: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-09-15 15:48:49.425422: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      
Prefix path:  /dccstor/cogfin/arvind/da/VeeAlign/
Ontologies being aligned are:  [('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/confOf.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/conference.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/conference.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/conference.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/conference.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/confOf.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/conference.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/confOf.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/confOf.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/confOf.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl')]
Total number of extracted unique classes and properties from entire RA set:  829
Constructing abbrevation resolution dict....
Results after abbreviation resolution:  {'PC': 'Program Committee', 'OC': 'Organizing Committee'}
Resolving abbreviations...
Number of entities: 119639
Starting sliding window evaluation...
Step 0/7
Val onto:  [('confof', 'ekaw')] test_onto:  [('conference', 'sigkdd')]
Training size: 112565 Testing size: 3871
Epoch: 0 Idx: 0 Loss: 0.18075643682790854
Epoch: 0 Idx: 5000 Loss: 0.006064920022720102
Epoch: 1 Idx: 0 Loss: 0.012737344688802707
Epoch: 1 Idx: 5000 Loss: 0.028802462391228866
Epoch: 2 Idx: 0 Loss: 0.008047002314539373
Epoch: 2 Idx: 5000 Loss: 0.026605284726207705
Epoch: 3 Idx: 0 Loss: 0.04709799345100102
Epoch: 3 Idx: 5000 Loss: 0.022931185575040394
Epoch: 4 Idx: 0 Loss: 0.011224108149949348
Epoch: 4 Idx: 5000 Loss: 0.011211972743884954
Epoch: 5 Idx: 0 Loss: 0.01793029015859146
Epoch: 5 Idx: 5000 Loss: 0.008655847299613868
Epoch: 6 Idx: 0 Loss: 0.011672894928010283
Epoch: 6 Idx: 5000 Loss: 0.028540731915901348
Epoch: 7 Idx: 0 Loss: 0.017263023714304028
Epoch: 7 Idx: 5000 Loss: 0.03157434743822507
Epoch: 8 Idx: 0 Loss: 0.020636221811836636
Epoch: 8 Idx: 5000 Loss: 0.00836439774531829
Epoch: 9 Idx: 0 Loss: 0.010130148113897735
Epoch: 9 Idx: 5000 Loss: 0.013522806329180173
Epoch: 10 Idx: 0 Loss: 0.00947013137250733
Epoch: 10 Idx: 5000 Loss: 0.01564549742387784
Epoch: 11 Idx: 0 Loss: 0.008327604153283758
Epoch: 11 Idx: 5000 Loss: 0.019227597356968867
Epoch: 12 Idx: 0 Loss: 0.006867237049724121
Epoch: 12 Idx: 5000 Loss: 0.015923967745225334
Epoch: 13 Idx: 0 Loss: 0.012352198400042468
Epoch: 13 Idx: 5000 Loss: 0.007253656279419424
Epoch: 14 Idx: 0 Loss: 0.0093337001227358
Epoch: 14 Idx: 5000 Loss: 0.01754356717955509
Epoch: 15 Idx: 0 Loss: 0.01665440624204574
Epoch: 15 Idx: 5000 Loss: 0.011751659863013063
Epoch: 16 Idx: 0 Loss: 0.01720973966379564
Epoch: 16 Idx: 5000 Loss: 0.01254535547263962
Epoch: 17 Idx: 0 Loss: 0.03382693682040545
Epoch: 17 Idx: 5000 Loss: 0.022014336708353583
Epoch: 18 Idx: 0 Loss: 0.017337547585687403
Epoch: 18 Idx: 5000 Loss: 0.010284622300839714
Epoch: 19 Idx: 0 Loss: 0.02500239148269897
Epoch: 19 Idx: 5000 Loss: 0.01022975899486932
Epoch: 20 Idx: 0 Loss: 0.0095700707148625
Epoch: 20 Idx: 5000 Loss: 0.030454072950299706
Epoch: 21 Idx: 0 Loss: 0.015829942581900066
Epoch: 21 Idx: 5000 Loss: 0.04836146949968634
Epoch: 22 Idx: 0 Loss: 0.024780764774009985
Epoch: 22 Idx: 5000 Loss: 0.011279252243620582
Epoch: 23 Idx: 0 Loss: 0.01891378689973576
Epoch: 23 Idx: 5000 Loss: 0.010802721608488539
Epoch: 24 Idx: 0 Loss: 0.012566981476532374
Epoch: 24 Idx: 5000 Loss: 0.023270793806674836
Epoch: 25 Idx: 0 Loss: 0.022167578413771403
Epoch: 25 Idx: 5000 Loss: 0.009790313006479857
Epoch: 26 Idx: 0 Loss: 0.006665858597485711
Epoch: 26 Idx: 5000 Loss: 0.012187564748181211
Epoch: 27 Idx: 0 Loss: 0.012170908355966608
Epoch: 27 Idx: 5000 Loss: 0.04031148957425434
Epoch: 28 Idx: 0 Loss: 0.012876322137126259
Epoch: 28 Idx: 5000 Loss: 0.010441005757897219
Epoch: 29 Idx: 0 Loss: 0.03390999344662641
Epoch: 29 Idx: 5000 Loss: 0.02293978468998088
Epoch: 30 Idx: 0 Loss: 0.014559749441327128
Epoch: 30 Idx: 5000 Loss: 0.006296064605175618
Epoch: 31 Idx: 0 Loss: 0.03621465808334329
Epoch: 31 Idx: 5000 Loss: 0.012131967633165913
Epoch: 32 Idx: 0 Loss: 0.03928293396822631
Epoch: 32 Idx: 5000 Loss: 0.014143481341987733
Epoch: 33 Idx: 0 Loss: 0.019592756388785038
Epoch: 33 Idx: 5000 Loss: 0.007883200673619738
Epoch: 34 Idx: 0 Loss: 0.010627978392550237
Epoch: 34 Idx: 5000 Loss: 0.005489931048825113
Epoch: 35 Idx: 0 Loss: 0.009215627038714373
Epoch: 35 Idx: 5000 Loss: 0.028010106275612637
Epoch: 36 Idx: 0 Loss: 0.018564609014902398
Epoch: 36 Idx: 5000 Loss: 0.014763196773268539
Epoch: 37 Idx: 0 Loss: 0.033539253599565805
Epoch: 37 Idx: 5000 Loss: 0.012452546513547606
Epoch: 38 Idx: 0 Loss: 0.011136655570232096
Epoch: 38 Idx: 5000 Loss: 0.02609190561702194
Epoch: 39 Idx: 0 Loss: 0.012452678211851496
Epoch: 39 Idx: 5000 Loss: 0.01323932822881767
Epoch: 40 Idx: 0 Loss: 0.028999987410098835
Epoch: 40 Idx: 5000 Loss: 0.012518760629808223
Epoch: 41 Idx: 0 Loss: 0.020013461767842633
Epoch: 41 Idx: 5000 Loss: 0.022808667907594164
Epoch: 42 Idx: 0 Loss: 0.023361617787134494
Epoch: 42 Idx: 5000 Loss: 0.02550178638774824
Epoch: 43 Idx: 0 Loss: 0.004489453728691594
Epoch: 43 Idx: 5000 Loss: 0.019624844643479168
Epoch: 44 Idx: 0 Loss: 0.012109445458373292
Epoch: 44 Idx: 5000 Loss: 0.03610909565155755
Epoch: 45 Idx: 0 Loss: 0.007154794238189977
Epoch: 45 Idx: 5000 Loss: 0.010462008658637412
Epoch: 46 Idx: 0 Loss: 0.009478728900049227
Epoch: 46 Idx: 5000 Loss: 0.014202227084624108
Epoch: 47 Idx: 0 Loss: 0.006719978594539497
Epoch: 47 Idx: 5000 Loss: 0.023673381139931098
Epoch: 48 Idx: 0 Loss: 0.014556235436492914
Epoch: 48 Idx: 5000 Loss: 0.01567883130887397
Epoch: 49 Idx: 0 Loss: 0.021746254074781837
Epoch: 49 Idx: 5000 Loss: 0.017286847871958298
Len (direct inputs):  429
Inputs len 2744 15 3856
Len (direct inputs):  1127
Starting sliding window evaluation...
Step 2/7
Val onto:  [('conference', 'ekaw')] test_onto:  [('cmt', 'sigkdd')]
Training size: 111450 Testing size: 2364
Epoch: 0 Idx: 0 Loss: 0.14885122310796245
Epoch: 0 Idx: 5000 Loss: 0.01238550283900992
Epoch: 1 Idx: 0 Loss: 0.015795546396667116
Epoch: 1 Idx: 5000 Loss: 0.01599813784693918
Epoch: 2 Idx: 0 Loss: 0.007920912950009504
Epoch: 2 Idx: 5000 Loss: 0.01009115584241907
Epoch: 3 Idx: 0 Loss: 0.006607460455944014
Epoch: 3 Idx: 5000 Loss: 0.015301707641054393
Epoch: 4 Idx: 0 Loss: 0.012203868432357875
Epoch: 4 Idx: 5000 Loss: 0.011888906101085853
Epoch: 5 Idx: 0 Loss: 0.02116124505990235
Epoch: 5 Idx: 5000 Loss: 0.01122009943650088
Epoch: 6 Idx: 0 Loss: 0.01009379753545764
Epoch: 6 Idx: 5000 Loss: 0.00885831154159199
Epoch: 7 Idx: 0 Loss: 0.00862911675069953
Epoch: 7 Idx: 5000 Loss: 0.025737553858130577
Epoch: 8 Idx: 0 Loss: 0.014090147798659568
Epoch: 8 Idx: 5000 Loss: 0.03614188196632731
Epoch: 9 Idx: 0 Loss: 0.010401415955823284
Epoch: 9 Idx: 5000 Loss: 0.008588003436888028
Epoch: 10 Idx: 0 Loss: 0.0070477050655959605
Epoch: 10 Idx: 5000 Loss: 0.022558044602074938
Epoch: 11 Idx: 0 Loss: 0.01777443301737124
Epoch: 11 Idx: 5000 Loss: 0.017133706378473217
Epoch: 12 Idx: 0 Loss: 0.01199193800510493
Epoch: 12 Idx: 5000 Loss: 0.03202253407947242
Epoch: 13 Idx: 0 Loss: 0.008175755641278953
Epoch: 13 Idx: 5000 Loss: 0.006783700663737364
Epoch: 14 Idx: 0 Loss: 0.009159845015617224
Epoch: 14 Idx: 5000 Loss: 0.02867444363766402
Epoch: 15 Idx: 0 Loss: 0.02493318335288424
Epoch: 15 Idx: 5000 Loss: 0.012831062137102795
Epoch: 16 Idx: 0 Loss: 0.03274994073614396
Epoch: 16 Idx: 5000 Loss: 0.017606551781939914
Epoch: 17 Idx: 0 Loss: 0.010696466406345903
Epoch: 17 Idx: 5000 Loss: 0.014245430608620036
Epoch: 18 Idx: 0 Loss: 0.013791977036027386
Epoch: 18 Idx: 5000 Loss: 0.009233230851735724
Epoch: 19 Idx: 0 Loss: 0.01127222878245693
Epoch: 19 Idx: 5000 Loss: 0.030849395665375534
Epoch: 20 Idx: 0 Loss: 0.01732953447425741
Epoch: 20 Idx: 5000 Loss: 0.028298282469114717
Epoch: 21 Idx: 0 Loss: 0.006125430780504119
Epoch: 21 Idx: 5000 Loss: 0.036634620481229514
Epoch: 22 Idx: 0 Loss: 0.02718709879412473
Epoch: 22 Idx: 5000 Loss: 0.00946239102264915
Epoch: 23 Idx: 0 Loss: 0.015038170245856405
Epoch: 23 Idx: 5000 Loss: 0.012257750487956782
Epoch: 24 Idx: 0 Loss: 0.028126394007248673
Epoch: 24 Idx: 5000 Loss: 0.0158807180045738
Epoch: 25 Idx: 0 Loss: 0.015547227148986442
Epoch: 25 Idx: 5000 Loss: 0.026646509394197722
Epoch: 26 Idx: 0 Loss: 0.009502300417438445
Epoch: 26 Idx: 5000 Loss: 0.017567688163902154
Epoch: 27 Idx: 0 Loss: 0.01402316937002939
Epoch: 27 Idx: 5000 Loss: 0.01085826789920225
Epoch: 28 Idx: 0 Loss: 0.009371592223258757
Epoch: 28 Idx: 5000 Loss: 0.008671178993077329
Epoch: 29 Idx: 0 Loss: 0.012934912660230353
Epoch: 29 Idx: 5000 Loss: 0.02760126609763176
Epoch: 30 Idx: 0 Loss: 0.02920354695282771
Epoch: 30 Idx: 5000 Loss: 0.01879231912728418
Epoch: 31 Idx: 0 Loss: 0.023424610628292784
Epoch: 31 Idx: 5000 Loss: 0.0134661053336739
Epoch: 32 Idx: 0 Loss: 0.02792160378037916
Epoch: 32 Idx: 5000 Loss: 0.014422222607583914
Epoch: 33 Idx: 0 Loss: 0.007852679700193066
Epoch: 33 Idx: 5000 Loss: 0.012632893091022172
Epoch: 34 Idx: 0 Loss: 0.00478390239146201
Epoch: 34 Idx: 5000 Loss: 0.006872969389668281
Epoch: 35 Idx: 0 Loss: 0.020151749916002694
Epoch: 35 Idx: 5000 Loss: 0.018193394157470986
Epoch: 36 Idx: 0 Loss: 0.011358797643995947
Epoch: 36 Idx: 5000 Loss: 0.01074322406948355
Epoch: 37 Idx: 0 Loss: 0.007029118218205308
Epoch: 37 Idx: 5000 Loss: 0.007020625435127308
Epoch: 38 Idx: 0 Loss: 0.010003645861708639
Epoch: 38 Idx: 5000 Loss: 0.015501103634898324
Epoch: 39 Idx: 0 Loss: 0.014889365413720101
Epoch: 39 Idx: 5000 Loss: 0.01105897629592227
Epoch: 40 Idx: 0 Loss: 0.011576131500919228
Epoch: 40 Idx: 5000 Loss: 0.012294208744172842
Epoch: 41 Idx: 0 Loss: 0.02070940413925411
Epoch: 41 Idx: 5000 Loss: 0.016556830377303145
Epoch: 42 Idx: 0 Loss: 0.015989222140915763
Epoch: 42 Idx: 5000 Loss: 0.04465095937394925
Epoch: 43 Idx: 0 Loss: 0.02042280715236594
Epoch: 43 Idx: 5000 Loss: 0.008062623074818734
Epoch: 44 Idx: 0 Loss: 0.021946820789120296
Epoch: 44 Idx: 5000 Loss: 0.01809483068070271
Epoch: 45 Idx: 0 Loss: 0.024991654400411588
Epoch: 45 Idx: 5000 Loss: 0.008241798527621717
Epoch: 46 Idx: 0 Loss: 0.013613385364060338
Epoch: 46 Idx: 5000 Loss: 0.013981862976110808
Epoch: 47 Idx: 0 Loss: 0.02754813855910663
Epoch: 47 Idx: 5000 Loss: 0.02478285696814734
Epoch: 48 Idx: 0 Loss: 0.021641484174367353
Epoch: 48 Idx: 5000 Loss: 0.008878367546739298
Epoch: 49 Idx: 0 Loss: 0.011167816160451626
Epoch: 49 Idx: 5000 Loss: 0.026075845196302214
Len (direct inputs):  1737
Inputs len 1372 12 2352
Len (direct inputs):  992
Starting sliding window evaluation...
Step 4/7
Val onto:  [('ekaw', 'sigkdd')] test_onto:  [('cmt', 'conference')]
Training size: 111356 Testing size: 4145
Epoch: 0 Idx: 0 Loss: 0.15947915637852741
Epoch: 0 Idx: 5000 Loss: 0.010335380362468546
Epoch: 1 Idx: 0 Loss: 0.008384295539692886
Epoch: 1 Idx: 5000 Loss: 0.02853662667587737
Epoch: 2 Idx: 0 Loss: 0.033592036395808315
Epoch: 2 Idx: 5000 Loss: 0.01586989889525518
Epoch: 3 Idx: 0 Loss: 0.010951404847765099
Epoch: 3 Idx: 5000 Loss: 0.014842303860294547
Epoch: 4 Idx: 0 Loss: 0.013280601493131225
Epoch: 4 Idx: 5000 Loss: 0.03414502616999178
Epoch: 5 Idx: 0 Loss: 0.029064276218399014
Epoch: 5 Idx: 5000 Loss: 0.007992226331613367
Epoch: 6 Idx: 0 Loss: 0.01313277757850281
Epoch: 6 Idx: 5000 Loss: 0.02821419666423026
Epoch: 7 Idx: 0 Loss: 0.007100640752113971
Epoch: 7 Idx: 5000 Loss: 0.026052160599614096
Epoch: 8 Idx: 0 Loss: 0.007495448205208144
Epoch: 8 Idx: 5000 Loss: 0.022097251366131172
Epoch: 9 Idx: 0 Loss: 0.011418504042576971
Epoch: 9 Idx: 5000 Loss: 0.011944713680617168
Epoch: 10 Idx: 0 Loss: 0.025647835299706163
Epoch: 10 Idx: 5000 Loss: 0.018003341742172108
Epoch: 11 Idx: 0 Loss: 0.011121539479609224
Epoch: 11 Idx: 5000 Loss: 0.007433010012401412
Epoch: 12 Idx: 0 Loss: 0.009578717529144794
Epoch: 12 Idx: 5000 Loss: 0.02966100626477627
Epoch: 13 Idx: 0 Loss: 0.009642421517765891
Epoch: 13 Idx: 5000 Loss: 0.011475795688620316
Epoch: 14 Idx: 0 Loss: 0.013399428579671541
Epoch: 14 Idx: 5000 Loss: 0.01278310043003983
Epoch: 15 Idx: 0 Loss: 0.022887601301978415
Epoch: 15 Idx: 5000 Loss: 0.008814249112111865
Epoch: 16 Idx: 0 Loss: 0.010876693943792058
Epoch: 16 Idx: 5000 Loss: 0.01913311789249726
Epoch: 17 Idx: 0 Loss: 0.01594041670040584
Epoch: 17 Idx: 5000 Loss: 0.00841108077366511
Epoch: 18 Idx: 0 Loss: 0.006057113404446553
Epoch: 18 Idx: 5000 Loss: 0.01565378758162006
Epoch: 19 Idx: 0 Loss: 0.013289023571986799
Epoch: 19 Idx: 5000 Loss: 0.011348888556127144
Epoch: 20 Idx: 0 Loss: 0.023385505560649715
Epoch: 20 Idx: 5000 Loss: 0.022752454085440636
Epoch: 21 Idx: 0 Loss: 0.011688360035967477
Epoch: 21 Idx: 5000 Loss: 0.028488666115746093
Epoch: 22 Idx: 0 Loss: 0.013702672330466759
Epoch: 22 Idx: 5000 Loss: 0.037177295914893195
Epoch: 23 Idx: 0 Loss: 0.00886467625083451
Epoch: 23 Idx: 5000 Loss: 0.016869256331771235
Epoch: 24 Idx: 0 Loss: 0.026756917956313825
Epoch: 24 Idx: 5000 Loss: 0.016926265133449546
Epoch: 25 Idx: 0 Loss: 0.010449628907138584
Epoch: 25 Idx: 5000 Loss: 0.024630590085301936
Epoch: 26 Idx: 0 Loss: 0.025604129251766843
Epoch: 26 Idx: 5000 Loss: 0.0145696379585659
Epoch: 27 Idx: 0 Loss: 0.008965513740599576
Epoch: 27 Idx: 5000 Loss: 0.02618150253750705
Epoch: 28 Idx: 0 Loss: 0.014454611469440874
Epoch: 28 Idx: 5000 Loss: 0.01642932147256198
Epoch: 29 Idx: 0 Loss: 0.014699530888725276
Epoch: 29 Idx: 5000 Loss: 0.02395097683044893
Traceback (most recent call last):
  File "main.py", line 514, in <module>
    outputs = model(node_elems, inp_elems)
  File "/u/harshitk/miniconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "main.py", line 312, in forward
    feature_emb = self.name_embedding(features[i]) #  dim: (2, batch_size, max_types, max_paths, max_pathlen, 512)
  File "/u/harshitk/miniconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/u/harshitk/miniconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/sparse.py", line 126, in forward
    self.norm_type, self.scale_grad_by_freq, self.sparse)
  File "/u/harshitk/miniconda3/envs/py37/lib/python3.7/site-packages/torch/nn/functional.py", line 1814, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
KeyboardInterrupt

------------------------------------------------------------
Sender: LSF System <rer@dccxc252>
Subject: Job 4066805: <python main.py 3 21 False True> in cluster <dcc> Exited

Job <python main.py 3 21 False True> was submitted from host <dccxl004> by user <harshitk> in cluster <dcc> at Tue Sep 15 15:48:36 2020
Job was executed on host(s) <dccxc252>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep 15 15:48:37 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/VeeAlign/src> was used as the working directory.
Started at Tue Sep 15 15:48:37 2020
Terminated at Wed Sep 16 04:38:40 2020
Results reported at Wed Sep 16 04:38:40 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python main.py 3 21 False True
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   46132.71 sec.
    Max Memory :                                 2932 MB
    Average Memory :                             2719.71 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40485.00 MB
    Max Swap :                                   2 MB
    Max Processes :                              3
    Max Threads :                                13
    Run time :                                   46223 sec.
    Turnaround time :                            46204 sec.

The output (if any) is above this job summary.

