2020-09-15 15:48:39.065605: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-09-15 15:48:42.551512: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1
2020-09-15 15:48:42.666329: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:10:00.0 name: Tesla K80 computeCapability: 3.7
coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s
2020-09-15 15:48:42.666406: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-09-15 15:48:42.668523: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
2020-09-15 15:48:42.670032: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2020-09-15 15:48:42.670391: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2020-09-15 15:48:42.672668: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2020-09-15 15:48:42.674108: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10
2020-09-15 15:48:42.674318: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudnn.so.7'; dlerror: libcudnn.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/share/lsf-9.1.3/10.1/linux3.10-glibc2.17-x86_64/lib:=/opt/share/gcc-4.9.2_rhel6/x86_64/lib/:/opt/share/gcc-4.9.2_rhel6/x86_64/lib64:/opt/share/Python-3.6.2/x86_64/lib:=/opt/share/gcc-5.4.0/x86_64/lib/:/opt/share/gcc-5.4.0/x86_64/lib64:/opt/share/isl-0.17/x86_64/lib/:/opt/share/protobuf-3.1.0/x86_64/lib/:/opt/share/leveldb-1.19/x86_64/lib/:/opt/share/boost-1.62.0/x86_64/lib/:/opt/share/torch-7/x86_64/install/lib:/opt/share/Python-2.7.12/x86_64/lib:/opt/share/Python-3.5.2/x86_64/lib:/opt/share/cuDNN-v5.1-8.0/cuda/lib64:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/opt/share/cuda-8.0/
2020-09-15 15:48:42.674341: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1753] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2020-09-15 15:48:42.674686: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-09-15 15:48:42.682650: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2599845000 Hz
2020-09-15 15:48:42.682839: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x556bcd48afd0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-09-15 15:48:42.682861: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-09-15 15:48:42.684666: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-09-15 15:48:42.684688: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      
Prefix path:  /dccstor/cogfin/arvind/da/VeeAlign/
Ontologies being aligned are:  [('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/confOf.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/conference.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/conference.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/conference.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/conference.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/confOf.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/conference.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/confOf.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/confOf.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/confOf.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl')]
Total number of extracted unique classes and properties from entire RA set:  829
Constructing abbrevation resolution dict....
Results after abbreviation resolution:  {'PC': 'Program Committee', 'OC': 'Organizing Committee'}
Resolving abbreviations...
Number of entities: 119639
Starting sliding window evaluation...
Step 0/7
Val onto:  [('confof', 'ekaw')] test_onto:  [('conference', 'sigkdd')]
Training size: 112565 Testing size: 3871
Epoch: 0 Idx: 0 Loss: 0.19294074519427734
Epoch: 0 Idx: 5000 Loss: 0.006645537085906705
Epoch: 1 Idx: 0 Loss: 0.014307965397057299
Epoch: 1 Idx: 5000 Loss: 0.007242379132025992
Epoch: 2 Idx: 0 Loss: 0.007964588199297119
Epoch: 2 Idx: 5000 Loss: 0.02307578693737072
Epoch: 3 Idx: 0 Loss: 0.009180321279836031
Epoch: 3 Idx: 5000 Loss: 0.019294378510734723
Epoch: 4 Idx: 0 Loss: 0.04028405881999998
Epoch: 4 Idx: 5000 Loss: 0.00654618937312815
Epoch: 5 Idx: 0 Loss: 0.018235894411492246
Epoch: 5 Idx: 5000 Loss: 0.013274469491741266
Epoch: 6 Idx: 0 Loss: 0.014982578365170428
Epoch: 6 Idx: 5000 Loss: 0.01210679191234437
Epoch: 7 Idx: 0 Loss: 0.024794525254513533
Epoch: 7 Idx: 5000 Loss: 0.0169971402226095
Epoch: 8 Idx: 0 Loss: 0.009565859320975359
Epoch: 8 Idx: 5000 Loss: 0.007619509979416971
Epoch: 9 Idx: 0 Loss: 0.023818829466571746
Epoch: 9 Idx: 5000 Loss: 0.010811308685396419
Epoch: 10 Idx: 0 Loss: 0.0133548279028282
Epoch: 10 Idx: 5000 Loss: 0.01987552743822857
Epoch: 11 Idx: 0 Loss: 0.023903647628863336
Epoch: 11 Idx: 5000 Loss: 0.014729765687113545
Epoch: 12 Idx: 0 Loss: 0.00952878698579781
Epoch: 12 Idx: 5000 Loss: 0.019177455679136094
Epoch: 13 Idx: 0 Loss: 0.021459338282390217
Epoch: 13 Idx: 5000 Loss: 0.011557722988912254
Epoch: 14 Idx: 0 Loss: 0.0144771891119292
Epoch: 14 Idx: 5000 Loss: 0.014104241491467091
Epoch: 15 Idx: 0 Loss: 0.023363114744236146
Epoch: 15 Idx: 5000 Loss: 0.017276507997463275
Epoch: 16 Idx: 0 Loss: 0.019017461074631443
Epoch: 16 Idx: 5000 Loss: 0.012475319179497856
Epoch: 17 Idx: 0 Loss: 0.017116605006506778
Epoch: 17 Idx: 5000 Loss: 0.024747156464009518
Epoch: 18 Idx: 0 Loss: 0.018200410895805964
Epoch: 18 Idx: 5000 Loss: 0.0052054831977849225
Epoch: 19 Idx: 0 Loss: 0.014777501610187954
Epoch: 19 Idx: 5000 Loss: 0.016222088948653166
Epoch: 20 Idx: 0 Loss: 0.011447646223970105
Epoch: 20 Idx: 5000 Loss: 0.04128900449416817
Epoch: 21 Idx: 0 Loss: 0.011643857378941504
Epoch: 21 Idx: 5000 Loss: 0.04518104494992716
Epoch: 22 Idx: 0 Loss: 0.031539614472006094
Epoch: 22 Idx: 5000 Loss: 0.046615228399562336
Epoch: 23 Idx: 0 Loss: 0.02153189008966492
Epoch: 23 Idx: 5000 Loss: 0.05091126138253071
Epoch: 24 Idx: 0 Loss: 0.008714711050978057
Epoch: 24 Idx: 5000 Loss: 0.009194948472000902
Epoch: 25 Idx: 0 Loss: 0.011080511264537272
Epoch: 25 Idx: 5000 Loss: 0.015216314420977808
Epoch: 26 Idx: 0 Loss: 0.008382307421052837
Epoch: 26 Idx: 5000 Loss: 0.02145137627020777
Epoch: 27 Idx: 0 Loss: 0.006218003923908613
Epoch: 27 Idx: 5000 Loss: 0.028651811234461567
Epoch: 28 Idx: 0 Loss: 0.009141180611821294
Epoch: 28 Idx: 5000 Loss: 0.015243696614612355
Epoch: 29 Idx: 0 Loss: 0.007969171318086285
Epoch: 29 Idx: 5000 Loss: 0.0053323482712451815
Epoch: 30 Idx: 0 Loss: 0.01138158146620385
Epoch: 30 Idx: 5000 Loss: 0.008444042308632438
Epoch: 31 Idx: 0 Loss: 0.01844719657033728
Epoch: 31 Idx: 5000 Loss: 0.009844248694147892
Epoch: 32 Idx: 0 Loss: 0.03634378241291812
Epoch: 32 Idx: 5000 Loss: 0.03263180463964845
Epoch: 33 Idx: 0 Loss: 0.014549714084608846
Epoch: 33 Idx: 5000 Loss: 0.02534259103667111
Epoch: 34 Idx: 0 Loss: 0.013770726692156171
Epoch: 34 Idx: 5000 Loss: 0.01769217083094364
Epoch: 35 Idx: 0 Loss: 0.010978598935965781
Epoch: 35 Idx: 5000 Loss: 0.011260477575280354
Epoch: 36 Idx: 0 Loss: 0.028997271869350055
Epoch: 36 Idx: 5000 Loss: 0.013784548902663112
Epoch: 37 Idx: 0 Loss: 0.012558158313454207
Epoch: 37 Idx: 5000 Loss: 0.015759800453124403
Epoch: 38 Idx: 0 Loss: 0.013544325173413804
Epoch: 38 Idx: 5000 Loss: 0.030816220939357403
Epoch: 39 Idx: 0 Loss: 0.010557344951888311
Epoch: 39 Idx: 5000 Loss: 0.020115022159512882
Epoch: 40 Idx: 0 Loss: 0.024610604047615108
Epoch: 40 Idx: 5000 Loss: 0.024625515122949596
Epoch: 41 Idx: 0 Loss: 0.021199434344641034
Epoch: 41 Idx: 5000 Loss: 0.013897509122237283
Epoch: 42 Idx: 0 Loss: 0.0093483466042305
Epoch: 42 Idx: 5000 Loss: 0.01164387037779205
Epoch: 43 Idx: 0 Loss: 0.034691326172330755
Epoch: 43 Idx: 5000 Loss: 0.012501845274078684
Epoch: 44 Idx: 0 Loss: 0.02275356779934739
Epoch: 44 Idx: 5000 Loss: 0.02413813120530362
Epoch: 45 Idx: 0 Loss: 0.015884102567796675
Epoch: 45 Idx: 5000 Loss: 0.014342014602574966
Epoch: 46 Idx: 0 Loss: 0.009858292149973535
Epoch: 46 Idx: 5000 Loss: 0.0190976215224353
Epoch: 47 Idx: 0 Loss: 0.01889596569665068
Epoch: 47 Idx: 5000 Loss: 0.009785068361007874
Epoch: 48 Idx: 0 Loss: 0.025881647776254478
Epoch: 48 Idx: 5000 Loss: 0.039964390291234955
Epoch: 49 Idx: 0 Loss: 0.029651581205863704
Epoch: 49 Idx: 5000 Loss: 0.01585749933835283
Len (direct inputs):  429
Inputs len 2744 15 3856
Len (direct inputs):  1127
Starting sliding window evaluation...
Step 2/7
Val onto:  [('conference', 'ekaw')] test_onto:  [('cmt', 'sigkdd')]
Training size: 111450 Testing size: 2364
Epoch: 0 Idx: 0 Loss: 0.15144000148304915
Epoch: 0 Idx: 5000 Loss: 0.00706266514731803
Epoch: 1 Idx: 0 Loss: 0.005044966834930617
Epoch: 1 Idx: 5000 Loss: 0.018858513148067878
Epoch: 2 Idx: 0 Loss: 0.015574434569136434
Epoch: 2 Idx: 5000 Loss: 0.034203084892590746
Epoch: 3 Idx: 0 Loss: 0.011237977311425546
Epoch: 3 Idx: 5000 Loss: 0.011393906364614893
Epoch: 4 Idx: 0 Loss: 0.01117851669928615
Epoch: 4 Idx: 5000 Loss: 0.008170449791074532
Epoch: 5 Idx: 0 Loss: 0.034017347406750616
Epoch: 5 Idx: 5000 Loss: 0.030037073572484054
Epoch: 6 Idx: 0 Loss: 0.01224829697559099
Epoch: 6 Idx: 5000 Loss: 0.014771989640109395
Epoch: 7 Idx: 0 Loss: 0.013888908616092047
Epoch: 7 Idx: 5000 Loss: 0.021667532271916177
Epoch: 8 Idx: 0 Loss: 0.019112826666989238
Epoch: 8 Idx: 5000 Loss: 0.01869370404976403
Epoch: 9 Idx: 0 Loss: 0.012047344515576106
Epoch: 9 Idx: 5000 Loss: 0.028743788097370063
Epoch: 10 Idx: 0 Loss: 0.012883180832080576
Epoch: 10 Idx: 5000 Loss: 0.015187338471182215
Epoch: 11 Idx: 0 Loss: 0.015046600922453817
Epoch: 11 Idx: 5000 Loss: 0.007457216755997407
Epoch: 12 Idx: 0 Loss: 0.009021484118981826
Epoch: 12 Idx: 5000 Loss: 0.007236685205890417
Epoch: 13 Idx: 0 Loss: 0.03362065208055527
Epoch: 13 Idx: 5000 Loss: 0.012349451208212115
Epoch: 14 Idx: 0 Loss: 0.008492470203115103
Epoch: 14 Idx: 5000 Loss: 0.007157758033305366
Epoch: 15 Idx: 0 Loss: 0.00818401473632038
Epoch: 15 Idx: 5000 Loss: 0.009957256600551974
Epoch: 16 Idx: 0 Loss: 0.012211702622919948
Epoch: 16 Idx: 5000 Loss: 0.02706543187486237
Epoch: 17 Idx: 0 Loss: 0.0248552922302618
Epoch: 17 Idx: 5000 Loss: 0.009722352906083809
Epoch: 18 Idx: 0 Loss: 0.011183796788265855
Epoch: 18 Idx: 5000 Loss: 0.013580036949620405
Epoch: 19 Idx: 0 Loss: 0.019731024398590577
Epoch: 19 Idx: 5000 Loss: 0.003946739984519229
Epoch: 20 Idx: 0 Loss: 0.016074244818728507
Epoch: 20 Idx: 5000 Loss: 0.017346461664651307
Epoch: 21 Idx: 0 Loss: 0.01230298334424778
Epoch: 21 Idx: 5000 Loss: 0.015811318817760572
Epoch: 22 Idx: 0 Loss: 0.023075641517074127
Epoch: 22 Idx: 5000 Loss: 0.008721661476632156
Epoch: 23 Idx: 0 Loss: 0.017574486247081588
Epoch: 23 Idx: 5000 Loss: 0.022373431713320298
Epoch: 24 Idx: 0 Loss: 0.02247781024259009
Epoch: 24 Idx: 5000 Loss: 0.013648496174889087
Epoch: 25 Idx: 0 Loss: 0.04875012619971207
Epoch: 25 Idx: 5000 Loss: 0.01411975651308614
Epoch: 26 Idx: 0 Loss: 0.010148102297837469
Epoch: 26 Idx: 5000 Loss: 0.0074350188046305235
Epoch: 27 Idx: 0 Loss: 0.030411136717619115
Epoch: 27 Idx: 5000 Loss: 0.011864198248586302
Epoch: 28 Idx: 0 Loss: 0.011384060657362284
Epoch: 28 Idx: 5000 Loss: 0.012579084769049047
Epoch: 29 Idx: 0 Loss: 0.00803764602165011
Epoch: 29 Idx: 5000 Loss: 0.020603987828518036
Epoch: 30 Idx: 0 Loss: 0.008927859426245618
Epoch: 30 Idx: 5000 Loss: 0.017381811663479296
Epoch: 31 Idx: 0 Loss: 0.01058872118078294
Epoch: 31 Idx: 5000 Loss: 0.013858186977460666
Epoch: 32 Idx: 0 Loss: 0.03341400014268211
Epoch: 32 Idx: 5000 Loss: 0.01299822513968104
Epoch: 33 Idx: 0 Loss: 0.011129116697529823
Epoch: 33 Idx: 5000 Loss: 0.010646891946693587
Epoch: 34 Idx: 0 Loss: 0.014097266513547197
Epoch: 34 Idx: 5000 Loss: 0.012367534122708199
Epoch: 35 Idx: 0 Loss: 0.012591464739857096
Epoch: 35 Idx: 5000 Loss: 0.02839547355271039
Epoch: 36 Idx: 0 Loss: 0.023456578491968486
Epoch: 36 Idx: 5000 Loss: 0.03961990321394633
Epoch: 37 Idx: 0 Loss: 0.011852629593398015
Epoch: 37 Idx: 5000 Loss: 0.01876976837162796
Epoch: 38 Idx: 0 Loss: 0.0151456746586557
Epoch: 38 Idx: 5000 Loss: 0.010879912923748784
Epoch: 39 Idx: 0 Loss: 0.01296216440119963
Epoch: 39 Idx: 5000 Loss: 0.006770755066914465
Epoch: 40 Idx: 0 Loss: 0.014512905867493493
Epoch: 40 Idx: 5000 Loss: 0.011579419355336882
Epoch: 41 Idx: 0 Loss: 0.05465734939992233
Epoch: 41 Idx: 5000 Loss: 0.005625107790301077
Epoch: 42 Idx: 0 Loss: 0.01152524188149908
Epoch: 42 Idx: 5000 Loss: 0.012954453698422016
Epoch: 43 Idx: 0 Loss: 0.01640696169386051
Epoch: 43 Idx: 5000 Loss: 0.027847159972844912
Epoch: 44 Idx: 0 Loss: 0.027844838757121914
Epoch: 44 Idx: 5000 Loss: 0.020150173659404906
Epoch: 45 Idx: 0 Loss: 0.017774583542621444
Epoch: 45 Idx: 5000 Loss: 0.016958448542732964
Epoch: 46 Idx: 0 Loss: 0.012221388974496006
Epoch: 46 Idx: 5000 Loss: 0.01797677272248329
Epoch: 47 Idx: 0 Loss: 0.009509197228013534
Epoch: 47 Idx: 5000 Loss: 0.013162320324365095
Epoch: 48 Idx: 0 Loss: 0.018023690208497117
Epoch: 48 Idx: 5000 Loss: 0.02097280201425703
Epoch: 49 Idx: 0 Loss: 0.008863957824731008
Epoch: 49 Idx: 5000 Loss: 0.007014427799446851
Len (direct inputs):  1737
Inputs len 1372 12 2352
Len (direct inputs):  992
Starting sliding window evaluation...
Step 4/7
Val onto:  [('ekaw', 'sigkdd')] test_onto:  [('cmt', 'conference')]
Training size: 111356 Testing size: 4145
Epoch: 0 Idx: 0 Loss: 0.1364865557933543
Epoch: 0 Idx: 5000 Loss: 0.015839814371931823
Epoch: 1 Idx: 0 Loss: 0.011350100817503546
Epoch: 1 Idx: 5000 Loss: 0.012120933336908916
Epoch: 2 Idx: 0 Loss: 0.005527369052463317
Epoch: 2 Idx: 5000 Loss: 0.008660441598943877
Epoch: 3 Idx: 0 Loss: 0.009013224627149161
Epoch: 3 Idx: 5000 Loss: 0.023431977745782446
Epoch: 4 Idx: 0 Loss: 0.01349882745903348
Epoch: 4 Idx: 5000 Loss: 0.013194637977685768
Epoch: 5 Idx: 0 Loss: 0.010273152347626942
Epoch: 5 Idx: 5000 Loss: 0.007846807038479786
Epoch: 6 Idx: 0 Loss: 0.009892272553173004
Epoch: 6 Idx: 5000 Loss: 0.006263977073812461
Epoch: 7 Idx: 0 Loss: 0.013044552930715141
Epoch: 7 Idx: 5000 Loss: 0.022107833452142515
Epoch: 8 Idx: 0 Loss: 0.004410629496975944
Epoch: 8 Idx: 5000 Loss: 0.008654163211248426
Epoch: 9 Idx: 0 Loss: 0.009399476679405534
Epoch: 9 Idx: 5000 Loss: 0.02325899268716393
Epoch: 10 Idx: 0 Loss: 0.02769388881578208
Epoch: 10 Idx: 5000 Loss: 0.009891515215211876
Epoch: 11 Idx: 0 Loss: 0.022297016184235616
Epoch: 11 Idx: 5000 Loss: 0.01907192889791654
Epoch: 12 Idx: 0 Loss: 0.009132785468292604
Epoch: 12 Idx: 5000 Loss: 0.01012448924198123
Epoch: 13 Idx: 0 Loss: 0.015089682342448497
Epoch: 13 Idx: 5000 Loss: 0.009016042481000314
Epoch: 14 Idx: 0 Loss: 0.012812355187219612
Epoch: 14 Idx: 5000 Loss: 0.014420848499362968
Epoch: 15 Idx: 0 Loss: 0.007187171854722237
Epoch: 15 Idx: 5000 Loss: 0.012727909617286199
Epoch: 16 Idx: 0 Loss: 0.011179091369588667
Epoch: 16 Idx: 5000 Loss: 0.010344453011354095
Epoch: 17 Idx: 0 Loss: 0.011754959647516928
Epoch: 17 Idx: 5000 Loss: 0.01362699901381502
Epoch: 18 Idx: 0 Loss: 0.011617307257896912
Epoch: 18 Idx: 5000 Loss: 0.006350366241981952
Epoch: 19 Idx: 0 Loss: 0.020625196113926273
Epoch: 19 Idx: 5000 Loss: 0.003733433133724867
Epoch: 20 Idx: 0 Loss: 0.027230630871057405
Epoch: 20 Idx: 5000 Loss: 0.02376865465133808
Epoch: 21 Idx: 0 Loss: 0.009581438243030644
Epoch: 21 Idx: 5000 Loss: 0.006236402824930925
Epoch: 22 Idx: 0 Loss: 0.02519198198619258
Epoch: 22 Idx: 5000 Loss: 0.0333201798349424
Epoch: 23 Idx: 0 Loss: 0.01259771003858803
Epoch: 23 Idx: 5000 Loss: 0.014540762779040019
Epoch: 24 Idx: 0 Loss: 0.014804184014423936
Epoch: 24 Idx: 5000 Loss: 0.012783730033923135
Epoch: 25 Idx: 0 Loss: 0.008321029667849545
Epoch: 25 Idx: 5000 Loss: 0.01584201885674364
Epoch: 26 Idx: 0 Loss: 0.010451935276066483
Epoch: 26 Idx: 5000 Loss: 0.02531412886614797
Epoch: 27 Idx: 0 Loss: 0.010548751795249577
Epoch: 27 Idx: 5000 Loss: 0.021253925913526792
Epoch: 28 Idx: 0 Loss: 0.006215097335066699
Epoch: 28 Idx: 5000 Loss: 0.015565130922476971
Epoch: 29 Idx: 0 Loss: 0.012467004169704473
Epoch: 29 Idx: 5000 Loss: 0.017865402441120216
Epoch: 30 Idx: 0 Loss: 0.014606430970179326
Epoch: 30 Idx: 5000 Loss: 0.011325317959173332
Epoch: 31 Idx: 0 Loss: 0.02166320249877539
Epoch: 31 Idx: 5000 Loss: 0.022147241686477113
Epoch: 32 Idx: 0 Loss: 0.006295732746245975
Epoch: 32 Idx: 5000 Loss: 0.010458183766826255
Epoch: 33 Idx: 0 Loss: 0.013022959887396972
Epoch: 33 Idx: 5000 Loss: 0.020028485337037216
Epoch: 34 Idx: 0 Loss: 0.018178690505302915
Epoch: 34 Idx: 5000 Loss: 0.0034849141492557106
Epoch: 35 Idx: 0 Loss: 0.007533468427893495
Epoch: 35 Idx: 5000 Loss: 0.01145375288840332
Epoch: 36 Idx: 0 Loss: 0.014606382232851142
Epoch: 36 Idx: 5000 Loss: 0.013318946940770451
Epoch: 37 Idx: 0 Loss: 0.04781825466605417
Epoch: 37 Idx: 5000 Loss: 0.0075792510937355345
Epoch: 38 Idx: 0 Loss: 0.021446078553226132
Epoch: 38 Idx: 5000 Loss: 0.013833400251709331
Epoch: 39 Idx: 0 Loss: 0.012228458591861512
Epoch: 39 Idx: 5000 Loss: 0.008354056694698731
Epoch: 40 Idx: 0 Loss: 0.030174988988505225
Epoch: 40 Idx: 5000 Loss: 0.008397343207525077
Epoch: 41 Idx: 0 Loss: 0.029309288340498944
Epoch: 41 Idx: 5000 Loss: 0.00724995961259742
Epoch: 42 Idx: 0 Loss: 0.010941444785172411
Epoch: 42 Idx: 5000 Loss: 0.012526208262548442
Epoch: 43 Idx: 0 Loss: 0.012821653944250382
Epoch: 43 Idx: 5000 Loss: 0.009986170767036657
Epoch: 44 Idx: 0 Loss: 0.030153982145800107
Epoch: 44 Idx: 5000 Loss: 0.042635632603977354
Epoch: 45 Idx: 0 Loss: 0.017630818803173156
Epoch: 45 Idx: 5000 Loss: 0.017705647906456994
Epoch: 46 Idx: 0 Loss: 0.008759715354442808
Epoch: 46 Idx: 5000 Loss: 0.03395522010574813
Epoch: 47 Idx: 0 Loss: 0.011422610420953796
Epoch: 47 Idx: 5000 Loss: 0.02681827097939828
Epoch: 48 Idx: 0 Loss: 0.013210173162293334
Epoch: 48 Idx: 5000 Loss: 0.00735129829971163
Epoch: 49 Idx: 0 Loss: 0.015316250547097569
Epoch: 49 Idx: 5000 Loss: 0.009182767321174692
Len (direct inputs):  561
Inputs len 1568 15 4130
Len (direct inputs):  2577
Starting sliding window evaluation...
Step 6/7
Val onto:  [('edas', 'sigkdd')] test_onto:  [('conference', 'edas')]
Training size: 106045 Testing size: 7817
Epoch: 0 Idx: 0 Loss: 0.22666435776491772
Epoch: 0 Idx: 5000 Loss: 0.020144508603829436
Epoch: 1 Idx: 0 Loss: 0.008088361731562963
Epoch: 1 Idx: 5000 Loss: 0.01645686473332445
Epoch: 2 Idx: 0 Loss: 0.01951748260493072
Traceback (most recent call last):
  File "main.py", line 514, in <module>
    outputs = model(node_elems, inp_elems)
  File "/u/harshitk/miniconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "main.py", line 312, in forward
    feature_emb = self.name_embedding(features[i]) #  dim: (2, batch_size, max_types, max_paths, max_pathlen, 512)
  File "/u/harshitk/miniconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/u/harshitk/miniconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/sparse.py", line 126, in forward
    self.norm_type, self.scale_grad_by_freq, self.sparse)
  File "/u/harshitk/miniconda3/envs/py37/lib/python3.7/site-packages/torch/nn/functional.py", line 1814, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
KeyboardInterrupt

------------------------------------------------------------
Sender: LSF System <rer@dccxc219>
Subject: Job 4066800: <python main.py 3 18 False False> in cluster <dcc> Exited

Job <python main.py 3 18 False False> was submitted from host <dccxl004> by user <harshitk> in cluster <dcc> at Tue Sep 15 15:48:35 2020
Job was executed on host(s) <dccxc219>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep 15 15:48:36 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/VeeAlign/src> was used as the working directory.
Started at Tue Sep 15 15:48:36 2020
Terminated at Wed Sep 16 04:38:38 2020
Results reported at Wed Sep 16 04:38:38 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python main.py 3 18 False False
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   46110.10 sec.
    Max Memory :                                 2918 MB
    Average Memory :                             2720.55 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40499.00 MB
    Max Swap :                                   2 MB
    Max Processes :                              3
    Max Threads :                                13
    Run time :                                   46202 sec.
    Turnaround time :                            46203 sec.

The output (if any) is above this job summary.

