2020-09-15 15:49:37.428974: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-09-15 15:49:40.994788: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1
2020-09-15 15:49:41.106245: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:13:00.0 name: Tesla K80 computeCapability: 3.7
coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s
2020-09-15 15:49:41.106336: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-09-15 15:49:41.108462: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
2020-09-15 15:49:41.109948: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2020-09-15 15:49:41.110804: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2020-09-15 15:49:41.112850: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2020-09-15 15:49:41.114419: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10
2020-09-15 15:49:41.114752: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudnn.so.7'; dlerror: libcudnn.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/share/lsf-9.1.3/10.1/linux3.10-glibc2.17-x86_64/lib:=/opt/share/gcc-4.9.2_rhel6/x86_64/lib/:/opt/share/gcc-4.9.2_rhel6/x86_64/lib64:/opt/share/Python-3.6.2/x86_64/lib:=/opt/share/gcc-5.4.0/x86_64/lib/:/opt/share/gcc-5.4.0/x86_64/lib64:/opt/share/isl-0.17/x86_64/lib/:/opt/share/protobuf-3.1.0/x86_64/lib/:/opt/share/leveldb-1.19/x86_64/lib/:/opt/share/boost-1.62.0/x86_64/lib/:/opt/share/torch-7/x86_64/install/lib:/opt/share/Python-2.7.12/x86_64/lib:/opt/share/Python-3.5.2/x86_64/lib:/opt/share/cuDNN-v5.1-8.0/cuda/lib64:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/opt/share/cuda-8.0/
2020-09-15 15:49:41.114776: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1753] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2020-09-15 15:49:41.115122: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-09-15 15:49:41.122321: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2599925000 Hz
2020-09-15 15:49:41.122738: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x557bc28b3aa0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-09-15 15:49:41.122762: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-09-15 15:49:41.124813: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-09-15 15:49:41.124857: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      
Prefix path:  /dccstor/cogfin/arvind/da/VeeAlign/
Ontologies being aligned are:  [('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/confOf.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/conference.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/conference.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/conference.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/conference.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/confOf.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/conference.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/confOf.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/confOf.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/confOf.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl')]
Total number of extracted unique classes and properties from entire RA set:  829
Constructing abbrevation resolution dict....
Results after abbreviation resolution:  {'PC': 'Program Committee', 'OC': 'Organizing Committee'}
Resolving abbreviations...
Number of entities: 119639
Starting sliding window evaluation...
Step 0/7
Val onto:  [('confof', 'ekaw')] test_onto:  [('conference', 'sigkdd')]
Training size: 112565 Testing size: 3871
Epoch: 0 Idx: 0 Loss: 0.18427127534680848
Epoch: 0 Idx: 5000 Loss: 0.02557890713721121
Epoch: 1 Idx: 0 Loss: 0.014800110286620356
Epoch: 1 Idx: 5000 Loss: 0.016905417056358667
Epoch: 2 Idx: 0 Loss: 0.0068019262516027775
Epoch: 2 Idx: 5000 Loss: 0.021569500306060002
Epoch: 3 Idx: 0 Loss: 0.00938597938659221
Epoch: 3 Idx: 5000 Loss: 0.013299292684610298
Epoch: 4 Idx: 0 Loss: 0.007356385817018121
Epoch: 4 Idx: 5000 Loss: 0.00579437238560049
Epoch: 5 Idx: 0 Loss: 0.014598240282032193
Epoch: 5 Idx: 5000 Loss: 0.02257221558331736
Epoch: 6 Idx: 0 Loss: 0.020178965104606837
Epoch: 6 Idx: 5000 Loss: 0.01023061124913856
Epoch: 7 Idx: 0 Loss: 0.01347109169679511
Epoch: 7 Idx: 5000 Loss: 0.023338182261083532
Epoch: 8 Idx: 0 Loss: 0.010249603384862228
Epoch: 8 Idx: 5000 Loss: 0.0103098690785638
Epoch: 9 Idx: 0 Loss: 0.016600754238507952
Epoch: 9 Idx: 5000 Loss: 0.01025353424722353
Epoch: 10 Idx: 0 Loss: 0.027594506036744747
Epoch: 10 Idx: 5000 Loss: 0.01002402951691034
Epoch: 11 Idx: 0 Loss: 0.010101967802397065
Epoch: 11 Idx: 5000 Loss: 0.03963380739965802
Epoch: 12 Idx: 0 Loss: 0.008396687336327808
Epoch: 12 Idx: 5000 Loss: 0.0105183294853596
Epoch: 13 Idx: 0 Loss: 0.015209945887041193
Epoch: 13 Idx: 5000 Loss: 0.011340288065800747
Epoch: 14 Idx: 0 Loss: 0.012168481621363345
Epoch: 14 Idx: 5000 Loss: 0.007693008797783743
Epoch: 15 Idx: 0 Loss: 0.015739590140906562
Epoch: 15 Idx: 5000 Loss: 0.011373248778014025
Epoch: 16 Idx: 0 Loss: 0.009229789802010281
Epoch: 16 Idx: 5000 Loss: 0.007566012635315422
Epoch: 17 Idx: 0 Loss: 0.013194556324335964
Epoch: 17 Idx: 5000 Loss: 0.009583414592382416
Epoch: 18 Idx: 0 Loss: 0.023983330683198262
Epoch: 18 Idx: 5000 Loss: 0.011347523949443949
Epoch: 19 Idx: 0 Loss: 0.02168268870067956
Epoch: 19 Idx: 5000 Loss: 0.02795845895800488
Epoch: 20 Idx: 0 Loss: 0.03480212901090656
Epoch: 20 Idx: 5000 Loss: 0.03977449746605098
Epoch: 21 Idx: 0 Loss: 0.008184727181755797
Epoch: 21 Idx: 5000 Loss: 0.013263439748177997
Epoch: 22 Idx: 0 Loss: 0.05443358386914563
Epoch: 22 Idx: 5000 Loss: 0.021628061111790404
Epoch: 23 Idx: 0 Loss: 0.019192767507057125
Epoch: 23 Idx: 5000 Loss: 0.011469156288618013
Epoch: 24 Idx: 0 Loss: 0.01073746873157607
Epoch: 24 Idx: 5000 Loss: 0.01582473393142396
Epoch: 25 Idx: 0 Loss: 0.014496768234370625
Epoch: 25 Idx: 5000 Loss: 0.022301782350820983
Epoch: 26 Idx: 0 Loss: 0.010569551506097006
Epoch: 26 Idx: 5000 Loss: 0.02465096920923231
Epoch: 27 Idx: 0 Loss: 0.008481419104213458
Epoch: 27 Idx: 5000 Loss: 0.016111842693507794
Epoch: 28 Idx: 0 Loss: 0.021745662831443646
Epoch: 28 Idx: 5000 Loss: 0.022642837608544786
Epoch: 29 Idx: 0 Loss: 0.01359482785029156
Epoch: 29 Idx: 5000 Loss: 0.039275188255258484
Epoch: 30 Idx: 0 Loss: 0.013346459620370435
Epoch: 30 Idx: 5000 Loss: 0.007449626862839979
Epoch: 31 Idx: 0 Loss: 0.012994203290825106
Epoch: 31 Idx: 5000 Loss: 0.01990509817382029
Epoch: 32 Idx: 0 Loss: 0.035133455852889725
Epoch: 32 Idx: 5000 Loss: 0.010245649520713844
Epoch: 33 Idx: 0 Loss: 0.01288216940130377
Epoch: 33 Idx: 5000 Loss: 0.020851371361428284
Epoch: 34 Idx: 0 Loss: 0.01395862063694912
Epoch: 34 Idx: 5000 Loss: 0.014045127152318765
Epoch: 35 Idx: 0 Loss: 0.007957478762757202
Epoch: 35 Idx: 5000 Loss: 0.028070808363001715
Epoch: 36 Idx: 0 Loss: 0.008473013465399873
Epoch: 36 Idx: 5000 Loss: 0.0127916816554802
Epoch: 37 Idx: 0 Loss: 0.016153847541150783
Epoch: 37 Idx: 5000 Loss: 0.01503236712696383
Epoch: 38 Idx: 0 Loss: 0.02026682686282672
Epoch: 38 Idx: 5000 Loss: 0.008152293569318684
Epoch: 39 Idx: 0 Loss: 0.012424900090212868
Epoch: 39 Idx: 5000 Loss: 0.011573302341558006
Epoch: 40 Idx: 0 Loss: 0.006653116703346876
Epoch: 40 Idx: 5000 Loss: 0.007602180414243398
Epoch: 41 Idx: 0 Loss: 0.02172733855683435
Epoch: 41 Idx: 5000 Loss: 0.02133287753104385
Epoch: 42 Idx: 0 Loss: 0.021136790102357996
Epoch: 42 Idx: 5000 Loss: 0.014875399867498381
Epoch: 43 Idx: 0 Loss: 0.03377915312636846
Epoch: 43 Idx: 5000 Loss: 0.015103444220212592
Epoch: 44 Idx: 0 Loss: 0.03567532008912449
Epoch: 44 Idx: 5000 Loss: 0.01863869179539817
Epoch: 45 Idx: 0 Loss: 0.011425966174464414
Epoch: 45 Idx: 5000 Loss: 0.028996044363429948
Epoch: 46 Idx: 0 Loss: 0.009627756673718239
Epoch: 46 Idx: 5000 Loss: 0.00850772218698371
Epoch: 47 Idx: 0 Loss: 0.011879749127463084
Epoch: 47 Idx: 5000 Loss: 0.02893877120753648
Epoch: 48 Idx: 0 Loss: 0.0066730118927227095
Epoch: 48 Idx: 5000 Loss: 0.00603130383760932
Epoch: 49 Idx: 0 Loss: 0.014715056996135126
Epoch: 49 Idx: 5000 Loss: 0.010730854777080155
Len (direct inputs):  429
Inputs len 2744 15 3856
Len (direct inputs):  1127
Starting sliding window evaluation...
Step 2/7
Val onto:  [('conference', 'ekaw')] test_onto:  [('cmt', 'sigkdd')]
Training size: 111450 Testing size: 2364
Epoch: 0 Idx: 0 Loss: 0.1564688841462242
Epoch: 0 Idx: 5000 Loss: 0.02017760133559284
Epoch: 1 Idx: 0 Loss: 0.007072707053622949
Epoch: 1 Idx: 5000 Loss: 0.028794390843509467
Epoch: 2 Idx: 0 Loss: 0.030364976140904852
Epoch: 2 Idx: 5000 Loss: 0.012921463105109568
Epoch: 3 Idx: 0 Loss: 0.008187472472930698
Epoch: 3 Idx: 5000 Loss: 0.01530181266264462
Epoch: 4 Idx: 0 Loss: 0.011083469660697048
Epoch: 4 Idx: 5000 Loss: 0.02284250057154816
Epoch: 5 Idx: 0 Loss: 0.01026628853860994
Epoch: 5 Idx: 5000 Loss: 0.021481582286344884
Epoch: 6 Idx: 0 Loss: 0.01334431708155644
Epoch: 6 Idx: 5000 Loss: 0.014121651823657514
Epoch: 7 Idx: 0 Loss: 0.007928749153649257
Epoch: 7 Idx: 5000 Loss: 0.01072633552761389
Epoch: 8 Idx: 0 Loss: 0.023427326931610468
Epoch: 8 Idx: 5000 Loss: 0.03555811767929506
Epoch: 9 Idx: 0 Loss: 0.009436324470894274
Epoch: 9 Idx: 5000 Loss: 0.009608960090623344
Epoch: 10 Idx: 0 Loss: 0.009273167031913748
Epoch: 10 Idx: 5000 Loss: 0.017772973320338642
Epoch: 11 Idx: 0 Loss: 0.01697691848149645
Epoch: 11 Idx: 5000 Loss: 0.004993419189527035
Epoch: 12 Idx: 0 Loss: 0.010105870505457292
Epoch: 12 Idx: 5000 Loss: 0.01512746379193718
Epoch: 13 Idx: 0 Loss: 0.023977483671721957
Epoch: 13 Idx: 5000 Loss: 0.011469091105304724
Epoch: 14 Idx: 0 Loss: 0.010599717575048427
Epoch: 14 Idx: 5000 Loss: 0.018887571709802074
Epoch: 15 Idx: 0 Loss: 0.012564171500734599
Epoch: 15 Idx: 5000 Loss: 0.02060714635983328
Epoch: 16 Idx: 0 Loss: 0.04536638530085198
Epoch: 16 Idx: 5000 Loss: 0.019043820638960873
Epoch: 17 Idx: 0 Loss: 0.015929345348730226
Epoch: 17 Idx: 5000 Loss: 0.02762642978797236
Epoch: 18 Idx: 0 Loss: 0.02363529605576239
Epoch: 18 Idx: 5000 Loss: 0.01829345128307211
Epoch: 19 Idx: 0 Loss: 0.020964017318563906
Epoch: 19 Idx: 5000 Loss: 0.02101323806673734
Epoch: 20 Idx: 0 Loss: 0.0170696663696004
Epoch: 20 Idx: 5000 Loss: 0.01841374501387896
Epoch: 21 Idx: 0 Loss: 0.00862338556633196
Epoch: 21 Idx: 5000 Loss: 0.020108725029045198
Epoch: 22 Idx: 0 Loss: 0.027231597443560237
Epoch: 22 Idx: 5000 Loss: 0.01079378177540777
Epoch: 23 Idx: 0 Loss: 0.009576655787796295
Epoch: 23 Idx: 5000 Loss: 0.008816165906369312
Epoch: 24 Idx: 0 Loss: 0.03427874429474778
Epoch: 24 Idx: 5000 Loss: 0.013344215029526795
Epoch: 25 Idx: 0 Loss: 0.00631702965477274
Epoch: 25 Idx: 5000 Loss: 0.01036881439384365
Epoch: 26 Idx: 0 Loss: 0.014936029905970365
Epoch: 26 Idx: 5000 Loss: 0.018214721362553372
Epoch: 27 Idx: 0 Loss: 0.01794880886718369
Epoch: 27 Idx: 5000 Loss: 0.01108596896983287
Epoch: 28 Idx: 0 Loss: 0.015540802789756891
Epoch: 28 Idx: 5000 Loss: 0.018163933808365517
Epoch: 29 Idx: 0 Loss: 0.020716621216147382
Epoch: 29 Idx: 5000 Loss: 0.018811455002298572
Epoch: 30 Idx: 0 Loss: 0.008027174292819142
Epoch: 30 Idx: 5000 Loss: 0.01988552680691478
Epoch: 31 Idx: 0 Loss: 0.012303504593093626
Epoch: 31 Idx: 5000 Loss: 0.02933011493849577
Epoch: 32 Idx: 0 Loss: 0.016020434891957924
Epoch: 32 Idx: 5000 Loss: 0.014835892052846862
Epoch: 33 Idx: 0 Loss: 0.03385752488550153
Epoch: 33 Idx: 5000 Loss: 0.007604513690942234
Epoch: 34 Idx: 0 Loss: 0.0069262562099330375
Epoch: 34 Idx: 5000 Loss: 0.016052204142361574
Epoch: 35 Idx: 0 Loss: 0.03292875102777597
Epoch: 35 Idx: 5000 Loss: 0.011358955491919762
Epoch: 36 Idx: 0 Loss: 0.011014746647379245
Epoch: 36 Idx: 5000 Loss: 0.007584900660476986
Epoch: 37 Idx: 0 Loss: 0.01252251682541066
Epoch: 37 Idx: 5000 Loss: 0.012841340342635267
Epoch: 38 Idx: 0 Loss: 0.008204603935453969
Epoch: 38 Idx: 5000 Loss: 0.011064786443597641
Epoch: 39 Idx: 0 Loss: 0.010688546111635088
Epoch: 39 Idx: 5000 Loss: 0.009868487851657415
Epoch: 40 Idx: 0 Loss: 0.011922571590861525
Epoch: 40 Idx: 5000 Loss: 0.03828633204345718
Epoch: 41 Idx: 0 Loss: 0.02801727233651847
Epoch: 41 Idx: 5000 Loss: 0.006471517452301518
Epoch: 42 Idx: 0 Loss: 0.011500596808457803
Epoch: 42 Idx: 5000 Loss: 0.010653982817796115
Epoch: 43 Idx: 0 Loss: 0.02963075791701268
Epoch: 43 Idx: 5000 Loss: 0.010934928540971202
Epoch: 44 Idx: 0 Loss: 0.01609734224860247
Epoch: 44 Idx: 5000 Loss: 0.011913399062215514
Epoch: 45 Idx: 0 Loss: 0.006237772107648214
Epoch: 45 Idx: 5000 Loss: 0.014973271960467553
Epoch: 46 Idx: 0 Loss: 0.008056986247834401
Epoch: 46 Idx: 5000 Loss: 0.01422658516139632
Epoch: 47 Idx: 0 Loss: 0.016908146738720865
Epoch: 47 Idx: 5000 Loss: 0.03694411621486238
Epoch: 48 Idx: 0 Loss: 0.007140184300498645
Epoch: 48 Idx: 5000 Loss: 0.021509778700751117
Epoch: 49 Idx: 0 Loss: 0.020427218289319156
Epoch: 49 Idx: 5000 Loss: 0.010843732599922462
Len (direct inputs):  1737
Inputs len 1372 12 2352
Len (direct inputs):  992
Starting sliding window evaluation...
Step 4/7
Val onto:  [('ekaw', 'sigkdd')] test_onto:  [('cmt', 'conference')]
Training size: 111356 Testing size: 4145
Epoch: 0 Idx: 0 Loss: 0.13640416953746198
Epoch: 0 Idx: 5000 Loss: 0.02526761239780668
Epoch: 1 Idx: 0 Loss: 0.010009904090828492
Epoch: 1 Idx: 5000 Loss: 0.008555333826370715
Epoch: 2 Idx: 0 Loss: 0.02931920556094921
Epoch: 2 Idx: 5000 Loss: 0.014348069660117148
Epoch: 3 Idx: 0 Loss: 0.012489830617709065
Epoch: 3 Idx: 5000 Loss: 0.033679348933748854
Epoch: 4 Idx: 0 Loss: 0.035675325551626
Epoch: 4 Idx: 5000 Loss: 0.010934295782132378
Epoch: 5 Idx: 0 Loss: 0.017505603363913116
Epoch: 5 Idx: 5000 Loss: 0.015682653464864173
Epoch: 6 Idx: 0 Loss: 0.010511930965059988
Epoch: 6 Idx: 5000 Loss: 0.00985986520325774
Epoch: 7 Idx: 0 Loss: 0.01316038381225798
Epoch: 7 Idx: 5000 Loss: 0.032693549994664915
Epoch: 8 Idx: 0 Loss: 0.008603355635672955
Epoch: 8 Idx: 5000 Loss: 0.03314275052712916
Epoch: 9 Idx: 0 Loss: 0.011969700787089777
Epoch: 9 Idx: 5000 Loss: 0.01654332089446995
Epoch: 10 Idx: 0 Loss: 0.0257794950128257
Epoch: 10 Idx: 5000 Loss: 0.010146793860160092
Epoch: 11 Idx: 0 Loss: 0.013337704493710113
Epoch: 11 Idx: 5000 Loss: 0.009049800968642836
Epoch: 12 Idx: 0 Loss: 0.008914246433475107
Epoch: 12 Idx: 5000 Loss: 0.0073059669782301944
Epoch: 13 Idx: 0 Loss: 0.014957611079320924
Epoch: 13 Idx: 5000 Loss: 0.006671826446594237
Epoch: 14 Idx: 0 Loss: 0.022958425570176966
Epoch: 14 Idx: 5000 Loss: 0.01600711444517259
Epoch: 15 Idx: 0 Loss: 0.013612703157660482
Epoch: 15 Idx: 5000 Loss: 0.04414648512781159
Epoch: 16 Idx: 0 Loss: 0.01814590756568692
Epoch: 16 Idx: 5000 Loss: 0.011925227703303305
Epoch: 17 Idx: 0 Loss: 0.009130276499520525
Epoch: 17 Idx: 5000 Loss: 0.0173864978423786
Epoch: 18 Idx: 0 Loss: 0.011542837722481752
Epoch: 18 Idx: 5000 Loss: 0.018140062780062187
Epoch: 19 Idx: 0 Loss: 0.02142189443108209
Epoch: 19 Idx: 5000 Loss: 0.026548380183900336
Epoch: 20 Idx: 0 Loss: 0.03907432412512087
Epoch: 20 Idx: 5000 Loss: 0.019662746665756126
Epoch: 21 Idx: 0 Loss: 0.018454591868460893
Epoch: 21 Idx: 5000 Loss: 0.010615328891194278
Epoch: 22 Idx: 0 Loss: 0.019210752224433893
Epoch: 22 Idx: 5000 Loss: 0.015317605432215362
Epoch: 23 Idx: 0 Loss: 0.012575102906585586
Epoch: 23 Idx: 5000 Loss: 0.010910147367567253
Epoch: 24 Idx: 0 Loss: 0.014856548355292723
Epoch: 24 Idx: 5000 Loss: 0.009365085784634459
Epoch: 25 Idx: 0 Loss: 0.005174695222278562
Epoch: 25 Idx: 5000 Loss: 0.007150924317369619
Epoch: 26 Idx: 0 Loss: 0.007484573996764924
Epoch: 26 Idx: 5000 Loss: 0.033190739556281115
Epoch: 27 Idx: 0 Loss: 0.014844512539001513
Epoch: 27 Idx: 5000 Loss: 0.014200723383534892
Epoch: 28 Idx: 0 Loss: 0.0224072048391259
Epoch: 28 Idx: 5000 Loss: 0.010630718256886048
Epoch: 29 Idx: 0 Loss: 0.006895877833735301
Epoch: 29 Idx: 5000 Loss: 0.01914562994386131
Epoch: 30 Idx: 0 Loss: 0.008941483310738688
Epoch: 30 Idx: 5000 Loss: 0.016620325665210413
Epoch: 31 Idx: 0 Loss: 0.03199483963954858
Epoch: 31 Idx: 5000 Loss: 0.024892480425975985
Epoch: 32 Idx: 0 Loss: 0.009705492209309656
Epoch: 32 Idx: 5000 Loss: 0.023999408845645265
Epoch: 33 Idx: 0 Loss: 0.010193862289301919
Epoch: 33 Idx: 5000 Loss: 0.025972142663477905
Epoch: 34 Idx: 0 Loss: 0.021516720623330456
Epoch: 34 Idx: 5000 Loss: 0.020464933878699774
Epoch: 35 Idx: 0 Loss: 0.01782505766721155
Epoch: 35 Idx: 5000 Loss: 0.027200458905170637
Epoch: 36 Idx: 0 Loss: 0.014070672195959057
Epoch: 36 Idx: 5000 Loss: 0.015547919620140672
Epoch: 37 Idx: 0 Loss: 0.009869527859860922
Epoch: 37 Idx: 5000 Loss: 0.02022037224449837
Epoch: 38 Idx: 0 Loss: 0.016588253340999504
Epoch: 38 Idx: 5000 Loss: 0.016450201365380692
Epoch: 39 Idx: 0 Loss: 0.008164224597066286
Epoch: 39 Idx: 5000 Loss: 0.00986163762198658
Epoch: 40 Idx: 0 Loss: 0.03345429088940372
Epoch: 40 Idx: 5000 Loss: 0.008972408752441742
Epoch: 41 Idx: 0 Loss: 0.011378386682711081
Epoch: 41 Idx: 5000 Loss: 0.011154582991027043
Epoch: 42 Idx: 0 Loss: 0.024268724299334042
Epoch: 42 Idx: 5000 Loss: 0.010536467616824762
Epoch: 43 Idx: 0 Loss: 0.005878585452861593
Epoch: 43 Idx: 5000 Loss: 0.02412870139842277
Epoch: 44 Idx: 0 Loss: 0.029861200603939188
Epoch: 44 Idx: 5000 Loss: 0.01656692022007296
Epoch: 45 Idx: 0 Loss: 0.028258821983779332
Epoch: 45 Idx: 5000 Loss: 0.010534283395155358
Epoch: 46 Idx: 0 Loss: 0.014506711199533145
Epoch: 46 Idx: 5000 Loss: 0.01021618881205718
Epoch: 47 Idx: 0 Loss: 0.03089096360554947
Epoch: 47 Idx: 5000 Loss: 0.008478714846371159
Epoch: 48 Idx: 0 Loss: 0.020594046470903585
Epoch: 48 Idx: 5000 Loss: 0.008919509142626444
Epoch: 49 Idx: 0 Loss: 0.0076202729135508365
Epoch: 49 Idx: 5000 Loss: 0.013818126622993376
Len (direct inputs):  561
Inputs len 1568 15 4130
Len (direct inputs):  2577
Starting sliding window evaluation...
Step 6/7
Val onto:  [('edas', 'sigkdd')] test_onto:  [('conference', 'edas')]
Training size: 106045 Testing size: 7817
Epoch: 0 Idx: 0 Loss: 0.2210121741385917
Epoch: 0 Idx: 5000 Loss: 0.012924203029323771
Epoch: 1 Idx: 0 Loss: 0.02250187146105463
Epoch: 1 Idx: 5000 Loss: 0.02554010100891447
Epoch: 2 Idx: 0 Loss: 0.005178078772682319
Epoch: 2 Idx: 5000 Loss: 0.017529607776125215
Epoch: 3 Idx: 0 Loss: 0.0220542892564095
Epoch: 3 Idx: 5000 Loss: 0.006120080424237539
Epoch: 4 Idx: 0 Loss: 0.0244264068128694
Epoch: 4 Idx: 5000 Loss: 0.01754820099476237
Epoch: 5 Idx: 0 Loss: 0.007367740692542829
Epoch: 5 Idx: 5000 Loss: 0.013017344485851014
Epoch: 6 Idx: 0 Loss: 0.013421636804746916
Epoch: 6 Idx: 5000 Loss: 0.015756453106516553
Epoch: 7 Idx: 0 Loss: 0.051490762111161194
Epoch: 7 Idx: 5000 Loss: 0.009190861337009791
Epoch: 8 Idx: 0 Loss: 0.02576728156190655
Epoch: 8 Idx: 5000 Loss: 0.02872697358659833
Epoch: 9 Idx: 0 Loss: 0.015551562747477179
Epoch: 9 Idx: 5000 Loss: 0.01615161407726541
Epoch: 10 Idx: 0 Loss: 0.004726623960174784
Epoch: 10 Idx: 5000 Loss: 0.009985105580132762
Epoch: 11 Idx: 0 Loss: 0.0237311334775831
Epoch: 11 Idx: 5000 Loss: 0.015356863392091602
Epoch: 12 Idx: 0 Loss: 0.003848633930087072
Epoch: 12 Idx: 5000 Loss: 0.017367548723856183
Epoch: 13 Idx: 0 Loss: 0.03302157810393581
Epoch: 13 Idx: 5000 Loss: 0.020239180743252635
Epoch: 14 Idx: 0 Loss: 0.007013600174017944
Epoch: 14 Idx: 5000 Loss: 0.0056354802784480094
Epoch: 15 Idx: 0 Loss: 0.00682603473342017
Epoch: 15 Idx: 5000 Loss: 0.014284652848273387
Epoch: 16 Idx: 0 Loss: 0.008571816075956062
Epoch: 16 Idx: 5000 Loss: 0.030292745734023678
Epoch: 17 Idx: 0 Loss: 0.01529701825154718
Epoch: 17 Idx: 5000 Loss: 0.014100970678478712
Epoch: 18 Idx: 0 Loss: 0.014482909643147055
Epoch: 18 Idx: 5000 Loss: 0.0187758382656787
Epoch: 19 Idx: 0 Loss: 0.008188551788827273
Epoch: 19 Idx: 5000 Loss: 0.05347059597469054
Epoch: 20 Idx: 0 Loss: 0.019265412643642826
Epoch: 20 Idx: 5000 Loss: 0.00728111586417781
Epoch: 21 Idx: 0 Loss: 0.007752776756298694
Epoch: 21 Idx: 5000 Loss: 0.01921803712240317
Epoch: 22 Idx: 0 Loss: 0.013768010220030793
Epoch: 22 Idx: 5000 Loss: 0.05810736556532844
Epoch: 23 Idx: 0 Loss: 0.015369656160039349
Epoch: 23 Idx: 5000 Loss: 0.01696241867384829
Epoch: 24 Idx: 0 Loss: 0.006516736238635424
Epoch: 24 Idx: 5000 Loss: 0.0066540959532931715
Epoch: 25 Idx: 0 Loss: 0.019903934714071717
Epoch: 25 Idx: 5000 Loss: 0.016086212921703705
Epoch: 26 Idx: 0 Loss: 0.011976366238738207
Epoch: 26 Idx: 5000 Loss: 0.013801432896512466
Epoch: 27 Idx: 0 Loss: 0.016137042310077593
Epoch: 27 Idx: 5000 Loss: 0.026071184797714128
Epoch: 28 Idx: 0 Loss: 0.014407926879651312
Epoch: 28 Idx: 5000 Loss: 0.014131908107631137
Epoch: 29 Idx: 0 Loss: 0.01070010796747731
Epoch: 29 Idx: 5000 Loss: 0.020492711369808278
Epoch: 30 Idx: 0 Loss: 0.006424596158729888
Epoch: 30 Idx: 5000 Loss: 0.012043788108822558
Epoch: 31 Idx: 0 Loss: 0.013326618138081316
Epoch: 31 Idx: 5000 Loss: 0.04424028971456696
Epoch: 32 Idx: 0 Loss: 0.023374391837745263
Epoch: 32 Idx: 5000 Loss: 0.012563131320188207
Epoch: 33 Idx: 0 Loss: 0.010538866201238992
Epoch: 33 Idx: 5000 Loss: 0.02105140552413925
Epoch: 34 Idx: 0 Loss: 0.006697887286832633
Epoch: 34 Idx: 5000 Loss: 0.02172912670498685
Epoch: 35 Idx: 0 Loss: 0.00851776758310063
Epoch: 35 Idx: 5000 Loss: 0.016310481394633383
Epoch: 36 Idx: 0 Loss: 0.014663101250892416
Epoch: 36 Idx: 5000 Loss: 0.0037636455291382978
Epoch: 37 Idx: 0 Loss: 0.02402815207294447
Epoch: 37 Idx: 5000 Loss: 0.009334833260244839
Epoch: 38 Idx: 0 Loss: 0.030033235304693255
Epoch: 38 Idx: 5000 Loss: 0.009192840222869873
Epoch: 39 Idx: 0 Loss: 0.006736923187064248
Epoch: 39 Idx: 5000 Loss: 0.018133137583347503
Epoch: 40 Idx: 0 Loss: 0.0193624804498504
Epoch: 40 Idx: 5000 Loss: 0.016039931671877162
Epoch: 41 Idx: 0 Loss: 0.01773354442479386
Epoch: 41 Idx: 5000 Loss: 0.031171173112900925
Epoch: 42 Idx: 0 Loss: 0.006260885303165597
Epoch: 42 Idx: 5000 Loss: 0.007651904488680676
Epoch: 43 Idx: 0 Loss: 0.008301689942435766
Epoch: 43 Idx: 5000 Loss: 0.025224766050021663
Epoch: 44 Idx: 0 Loss: 0.011865600048801647
Epoch: 44 Idx: 5000 Loss: 0.02873298382352862
Epoch: 45 Idx: 0 Loss: 0.01804231153944487
Epoch: 45 Idx: 5000 Loss: 0.013392870694998654
Epoch: 46 Idx: 0 Loss: 0.02738930258317862
Epoch: 46 Idx: 5000 Loss: 0.012603891841839136
Epoch: 47 Idx: 0 Loss: 0.012537024643687455
Epoch: 47 Idx: 5000 Loss: 0.015872657094761138
Epoch: 48 Idx: 0 Loss: 0.022232118031766085
Epoch: 48 Idx: 5000 Loss: 0.0123128649835501
Epoch: 49 Idx: 0 Loss: 0.015481814440698235
Epoch: 49 Idx: 5000 Loss: 0.015164402131741417
Len (direct inputs):  877
Inputs len 5600 17 7800
Len (direct inputs):  2217
Starting sliding window evaluation...
Step 8/7
Val onto:  [('cmt', 'iasted')] test_onto:  [('confof', 'sigkdd')]
Training size: 111351 Testing size: 2336
Epoch: 0 Idx: 0 Loss: 0.22194838903041259
Epoch: 0 Idx: 5000 Loss: 0.013077662735374241
Epoch: 1 Idx: 0 Loss: 0.014742902605773897
Epoch: 1 Idx: 5000 Loss: 0.011773442780922926
Epoch: 2 Idx: 0 Loss: 0.007194903586384548
Epoch: 2 Idx: 5000 Loss: 0.025833351145690398
Epoch: 3 Idx: 0 Loss: 0.011575071070707937
Epoch: 3 Idx: 5000 Loss: 0.021970184984487343
Epoch: 4 Idx: 0 Loss: 0.023055121290515623
Epoch: 4 Idx: 5000 Loss: 0.016432851934786535
Epoch: 5 Idx: 0 Loss: 0.006587464996775708
Epoch: 5 Idx: 5000 Loss: 0.02799006515299588
Epoch: 6 Idx: 0 Loss: 0.012670151973958517
Epoch: 6 Idx: 5000 Loss: 0.03490954393592027
Epoch: 7 Idx: 0 Loss: 0.011087565579863718
Epoch: 7 Idx: 5000 Loss: 0.01346474979126228
Epoch: 8 Idx: 0 Loss: 0.02096349960444891
Epoch: 8 Idx: 5000 Loss: 0.013244542890800421
Epoch: 9 Idx: 0 Loss: 0.012432314342435809
Epoch: 9 Idx: 5000 Loss: 0.022217447565531425
Epoch: 10 Idx: 0 Loss: 0.00934190487502574
Epoch: 10 Idx: 5000 Loss: 0.009090954395169666
Epoch: 11 Idx: 0 Loss: 0.005378735637084627
Epoch: 11 Idx: 5000 Loss: 0.015202090212770462
Epoch: 12 Idx: 0 Loss: 0.010049843074070839
Epoch: 12 Idx: 5000 Loss: 0.02782973201778296
Epoch: 13 Idx: 0 Loss: 0.020959605812631947
Epoch: 13 Idx: 5000 Loss: 0.020550578257837418
Epoch: 14 Idx: 0 Loss: 0.027366506215760464
Epoch: 14 Idx: 5000 Loss: 0.047439979695051905
Epoch: 15 Idx: 0 Loss: 0.012640536386679184
Epoch: 15 Idx: 5000 Loss: 0.02544754720549217
Epoch: 16 Idx: 0 Loss: 0.008900815517937914
Epoch: 16 Idx: 5000 Loss: 0.019807967172256757
Epoch: 17 Idx: 0 Loss: 0.006475160087660986
Epoch: 17 Idx: 5000 Loss: 0.01647180344236031
Epoch: 18 Idx: 0 Loss: 0.01340599998434601
Epoch: 18 Idx: 5000 Loss: 0.005207102802073438
Epoch: 19 Idx: 0 Loss: 0.01270426837507698
Epoch: 19 Idx: 5000 Loss: 0.015331362682620742
Epoch: 20 Idx: 0 Loss: 0.014094917203476488
Epoch: 20 Idx: 5000 Loss: 0.03261329769079263
Epoch: 21 Idx: 0 Loss: 0.048271685066188535
Epoch: 21 Idx: 5000 Loss: 0.015524678915849577
Epoch: 22 Idx: 0 Loss: 0.03014014546618661
Epoch: 22 Idx: 5000 Loss: 0.02962275710121605
Epoch: 23 Idx: 0 Loss: 0.032126123546246985
Epoch: 23 Idx: 5000 Loss: 0.027864428762082318
Epoch: 24 Idx: 0 Loss: 0.010306573428432105
Epoch: 24 Idx: 5000 Loss: 0.007185154664553854
Epoch: 25 Idx: 0 Loss: 0.018751311612956525
Epoch: 25 Idx: 5000 Loss: 0.037503982580045926
Epoch: 26 Idx: 0 Loss: 0.02079621264111254
Epoch: 26 Idx: 5000 Loss: 0.04085585767450109
Epoch: 27 Idx: 0 Loss: 0.008499437382902956
Epoch: 27 Idx: 5000 Loss: 0.0072496092489597215
Epoch: 28 Idx: 0 Loss: 0.014267143337246376
Epoch: 28 Idx: 5000 Loss: 0.011278746907986663
Epoch: 29 Idx: 0 Loss: 0.012561974754111326
Epoch: 29 Idx: 5000 Loss: 0.006001325621331962
Epoch: 30 Idx: 0 Loss: 0.018335050814900375
Epoch: 30 Idx: 5000 Loss: 0.03798366746949222
Epoch: 31 Idx: 0 Loss: 0.011021681223335626
Epoch: 31 Idx: 5000 Loss: 0.01946728706114787
Epoch: 32 Idx: 0 Loss: 0.02434634422347201
Epoch: 32 Idx: 5000 Loss: 0.00797034336224287
Epoch: 33 Idx: 0 Loss: 0.0040167161100695525
Epoch: 33 Idx: 5000 Loss: 0.01186922322394752
Epoch: 34 Idx: 0 Loss: 0.021519495740962867
Epoch: 34 Idx: 5000 Loss: 0.0218955241893014
Epoch: 35 Idx: 0 Loss: 0.011622771545999347
Epoch: 35 Idx: 5000 Loss: 0.006632332385373328
Epoch: 36 Idx: 0 Loss: 0.01667263351871959
Epoch: 36 Idx: 5000 Loss: 0.01829719829945978
Epoch: 37 Idx: 0 Loss: 0.015907236590017693
Epoch: 37 Idx: 5000 Loss: 0.01245446829560741
Epoch: 38 Idx: 0 Loss: 0.039022509562997645
Epoch: 38 Idx: 5000 Loss: 0.01338774977667587
Epoch: 39 Idx: 0 Loss: 0.013585516551996978
Epoch: 39 Idx: 5000 Loss: 0.021811838481342632
Epoch: 40 Idx: 0 Loss: 0.01192893261451578
Epoch: 40 Idx: 5000 Loss: 0.018873761494031
Epoch: 41 Idx: 0 Loss: 0.017467799545984677
Epoch: 41 Idx: 5000 Loss: 0.012801670029676693
Epoch: 42 Idx: 0 Loss: 0.011396260685487285
Epoch: 42 Idx: 5000 Loss: 0.009836227456456584
Epoch: 43 Idx: 0 Loss: 0.013073655874858101
Epoch: 43 Idx: 5000 Loss: 0.007947267841707871
Epoch: 44 Idx: 0 Loss: 0.0198020454437708
Epoch: 44 Idx: 5000 Loss: 0.005537144051859089
Epoch: 45 Idx: 0 Loss: 0.008950904004196507
Epoch: 45 Idx: 5000 Loss: 0.008962312848779222
Epoch: 46 Idx: 0 Loss: 0.019226420547949187
Epoch: 46 Idx: 5000 Loss: 0.006067493923817021
Epoch: 47 Idx: 0 Loss: 0.02849173599354863
Epoch: 47 Idx: 5000 Loss: 0.008351387702868297
Epoch: 48 Idx: 0 Loss: 0.02909588210653065
Epoch: 48 Idx: 5000 Loss: 0.007467163414862176
Epoch: 49 Idx: 0 Loss: 0.030578715911323252
Epoch: 49 Idx: 5000 Loss: 0.018054150391194866
Len (direct inputs):  2088
Inputs len 1862 7 2329
Len (direct inputs):  474
Starting sliding window evaluation...
Step 10/7
Val onto:  [('cmt', 'ekaw')] test_onto:  [('ekaw', 'iasted')]
Training size: 104431 Testing size: 11474
Epoch: 0 Idx: 0 Loss: 0.20243301802227853
Epoch: 0 Idx: 5000 Loss: 0.01129545839360269
Epoch: 1 Idx: 0 Loss: 0.02274284040917548
Epoch: 1 Idx: 5000 Loss: 0.014464729136866042
Epoch: 2 Idx: 0 Loss: 0.007005914424331516
Epoch: 2 Idx: 5000 Loss: 0.013354892161266136
Epoch: 3 Idx: 0 Loss: 0.01590632943828685
Epoch: 3 Idx: 5000 Loss: 0.018597363108254418
Epoch: 4 Idx: 0 Loss: 0.021965460323109278
Epoch: 4 Idx: 5000 Loss: 0.035567840113993505
Epoch: 5 Idx: 0 Loss: 0.006420582165422192
Epoch: 5 Idx: 5000 Loss: 0.0341323492814746
Epoch: 6 Idx: 0 Loss: 0.015213915065746963
Epoch: 6 Idx: 5000 Loss: 0.012896029930241222
Epoch: 7 Idx: 0 Loss: 0.00714497966896365
Epoch: 7 Idx: 5000 Loss: 0.01698864737259345
Epoch: 8 Idx: 0 Loss: 0.00631700840272058
Epoch: 8 Idx: 5000 Loss: 0.04194573234575914
Epoch: 9 Idx: 0 Loss: 0.00832938181253667
Epoch: 9 Idx: 5000 Loss: 0.031309422368393805
Epoch: 10 Idx: 0 Loss: 0.006944759547545379
Epoch: 10 Idx: 5000 Loss: 0.030419445828538554
Epoch: 11 Idx: 0 Loss: 0.019142221126722512
Epoch: 11 Idx: 5000 Loss: 0.012872933515396133
Epoch: 12 Idx: 0 Loss: 0.0074501058309654365
Epoch: 12 Idx: 5000 Loss: 0.009333389957042783
Epoch: 13 Idx: 0 Loss: 0.010972121845154533
Epoch: 13 Idx: 5000 Loss: 0.014790177119098386
Epoch: 14 Idx: 0 Loss: 0.009298000440129076
Epoch: 14 Idx: 5000 Loss: 0.021041305088378127
Epoch: 15 Idx: 0 Loss: 0.015132375471396613
Epoch: 15 Idx: 5000 Loss: 0.029318285366499223
Epoch: 16 Idx: 0 Loss: 0.02231338131030231
Epoch: 16 Idx: 5000 Loss: 0.008856036387208669
Epoch: 17 Idx: 0 Loss: 0.021362053227867595
Epoch: 17 Idx: 5000 Loss: 0.03859742746267129
Epoch: 18 Idx: 0 Loss: 0.01817761169705511
Epoch: 18 Idx: 5000 Loss: 0.013415659642932256
Epoch: 19 Idx: 0 Loss: 0.013780107444183714
Epoch: 19 Idx: 5000 Loss: 0.02405260927757042
Epoch: 20 Idx: 0 Loss: 0.014296448284895102
Epoch: 20 Idx: 5000 Loss: 0.010766764263208904
Epoch: 21 Idx: 0 Loss: 0.04510932289473719
Epoch: 21 Idx: 5000 Loss: 0.01666327134884432
Epoch: 22 Idx: 0 Loss: 0.013108756332332456
Epoch: 22 Idx: 5000 Loss: 0.02226405848636247
Epoch: 23 Idx: 0 Loss: 0.007116230071344355
Epoch: 23 Idx: 5000 Loss: 0.023119985961410117
Epoch: 24 Idx: 0 Loss: 0.018987838083861563
Epoch: 24 Idx: 5000 Loss: 0.033836969559811136
Epoch: 25 Idx: 0 Loss: 0.008270289445816101
Epoch: 25 Idx: 5000 Loss: 0.011153898680000813
Epoch: 26 Idx: 0 Loss: 0.0053779753658676445
Epoch: 26 Idx: 5000 Loss: 0.0119724252247002
Epoch: 27 Idx: 0 Loss: 0.016116465305478278
Epoch: 27 Idx: 5000 Loss: 0.009191942037625146
Epoch: 28 Idx: 0 Loss: 0.01411543654737395
Epoch: 28 Idx: 5000 Loss: 0.01135575655876131
Epoch: 29 Idx: 0 Loss: 0.008618906170836927
Epoch: 29 Idx: 5000 Loss: 0.022906064956685217
Epoch: 30 Idx: 0 Loss: 0.013003641534681094
Epoch: 30 Idx: 5000 Loss: 0.014177567071181726
Epoch: 31 Idx: 0 Loss: 0.010927848410250726
Epoch: 31 Idx: 5000 Loss: 0.010188306175092279
Epoch: 32 Idx: 0 Loss: 0.007049672280338826
Epoch: 32 Idx: 5000 Loss: 0.03834125023166982
Epoch: 33 Idx: 0 Loss: 0.007563420959704994
Epoch: 33 Idx: 5000 Loss: 0.02176900798456101
Epoch: 34 Idx: 0 Loss: 0.030901276125381533
Epoch: 34 Idx: 5000 Loss: 0.0077527651981368195
Epoch: 35 Idx: 0 Loss: 0.009324634154407823
Epoch: 35 Idx: 5000 Loss: 0.009342928418316144
Epoch: 36 Idx: 0 Loss: 0.01607983718292149
Epoch: 36 Idx: 5000 Loss: 0.008412192812769507
Epoch: 37 Idx: 0 Loss: 0.006505500630331206
Epoch: 37 Idx: 5000 Loss: 0.010453705343079503
Epoch: 38 Idx: 0 Loss: 0.011919309768163702
Epoch: 38 Idx: 5000 Loss: 0.010098169110068762
Epoch: 39 Idx: 0 Loss: 0.006825743492255561
Epoch: 39 Idx: 5000 Loss: 0.018288794500735412
Epoch: 40 Idx: 0 Loss: 0.01351225768182447
Epoch: 40 Idx: 5000 Loss: 0.006193569507791163
Epoch: 41 Idx: 0 Loss: 0.008765437519644275
Epoch: 41 Idx: 5000 Loss: 0.018782755969434538
Epoch: 42 Idx: 0 Loss: 0.0112678798160022
Epoch: 42 Idx: 5000 Loss: 0.018805918090897024
Epoch: 43 Idx: 0 Loss: 0.008479501855950086
Epoch: 43 Idx: 5000 Loss: 0.009355005490155251
Epoch: 44 Idx: 0 Loss: 0.0329310379802752
Epoch: 44 Idx: 5000 Loss: 0.02013890967255497
Epoch: 45 Idx: 0 Loss: 0.012312772616520671
Epoch: 45 Idx: 5000 Loss: 0.008061621148235
Epoch: 46 Idx: 0 Loss: 0.018511392328498147
Epoch: 46 Idx: 5000 Loss: 0.01941294914116157
Epoch: 47 Idx: 0 Loss: 0.01980075613639112
Epoch: 47 Idx: 5000 Loss: 0.01019181734847354
Epoch: 48 Idx: 0 Loss: 0.0045855526057770385
Epoch: 48 Idx: 5000 Loss: 0.03312567725014536
Epoch: 49 Idx: 0 Loss: 0.016067302807660222
Epoch: 49 Idx: 5000 Loss: 0.022607721068977706
Len (direct inputs):  1690
Inputs len 10074 10 11464
Len (direct inputs):  1400
Starting sliding window evaluation...
Step 12/7
Val onto:  [('conference', 'iasted')] test_onto:  [('confof', 'edas')]
Training size: 104813 Testing size: 4764
Epoch: 0 Idx: 0 Loss: 0.21499911639525923
Epoch: 0 Idx: 5000 Loss: 0.011967666924844954
Epoch: 1 Idx: 0 Loss: 0.022524713516031955
Epoch: 1 Idx: 5000 Loss: 0.022023638903010838
Epoch: 2 Idx: 0 Loss: 0.00944378715293806
Epoch: 2 Idx: 5000 Loss: 0.015707396442462343
Epoch: 3 Idx: 0 Loss: 0.009827903748612697
Epoch: 3 Idx: 5000 Loss: 0.010079511447674619
Epoch: 4 Idx: 0 Loss: 0.006341880931508606
Epoch: 4 Idx: 5000 Loss: 0.017342678108670583
Epoch: 5 Idx: 0 Loss: 0.024393279246533117
Epoch: 5 Idx: 5000 Loss: 0.01170197641907236
Epoch: 6 Idx: 0 Loss: 0.014376617422306266
Epoch: 6 Idx: 5000 Loss: 0.00660497562635942
Epoch: 7 Idx: 0 Loss: 0.02314586316559585
Epoch: 7 Idx: 5000 Loss: 0.021586093688764848
Epoch: 8 Idx: 0 Loss: 0.02263149051975636
Epoch: 8 Idx: 5000 Loss: 0.007937288983570396
Epoch: 9 Idx: 0 Loss: 0.010066502019432556
Traceback (most recent call last):
  File "main.py", line 517, in <module>
    loss.backward()
  File "/u/harshitk/miniconda3/envs/py37/lib/python3.7/site-packages/torch/tensor.py", line 185, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/u/harshitk/miniconda3/envs/py37/lib/python3.7/site-packages/torch/autograd/__init__.py", line 127, in backward
    allow_unreachable=True)  # allow_unreachable flag
KeyboardInterrupt

------------------------------------------------------------
Sender: LSF System <rer@dccxc228>
Subject: Job 4066871: <python main.py 6 2 False False> in cluster <dcc> Exited

Job <python main.py 6 2 False False> was submitted from host <dccxl004> by user <harshitk> in cluster <dcc> at Tue Sep 15 15:48:39 2020
Job was executed on host(s) <dccxc228>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep 15 15:49:35 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/VeeAlign/src> was used as the working directory.
Started at Tue Sep 15 15:49:35 2020
Terminated at Wed Sep 16 04:38:39 2020
Results reported at Wed Sep 16 04:38:39 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python main.py 6 2 False False
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   46076.52 sec.
    Max Memory :                                 2892 MB
    Average Memory :                             2747.44 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40525.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                13
    Run time :                                   46144 sec.
    Turnaround time :                            46200 sec.

The output (if any) is above this job summary.

