2020-09-15 15:49:28.791555: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-09-15 15:49:38.341312: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1
2020-09-15 15:49:38.470235: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:1b:00.0 name: Tesla K80 computeCapability: 3.7
coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s
2020-09-15 15:49:38.470321: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-09-15 15:49:38.472683: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
2020-09-15 15:49:38.492135: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2020-09-15 15:49:38.526552: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2020-09-15 15:49:38.568893: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2020-09-15 15:49:38.591117: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10
2020-09-15 15:49:38.591681: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudnn.so.7'; dlerror: libcudnn.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/share/lsf-9.1.3/10.1/linux3.10-glibc2.17-x86_64/lib:=/opt/share/gcc-4.9.2_rhel6/x86_64/lib/:/opt/share/gcc-4.9.2_rhel6/x86_64/lib64:/opt/share/Python-3.6.2/x86_64/lib:=/opt/share/gcc-5.4.0/x86_64/lib/:/opt/share/gcc-5.4.0/x86_64/lib64:/opt/share/isl-0.17/x86_64/lib/:/opt/share/protobuf-3.1.0/x86_64/lib/:/opt/share/leveldb-1.19/x86_64/lib/:/opt/share/boost-1.62.0/x86_64/lib/:/opt/share/torch-7/x86_64/install/lib:/opt/share/Python-2.7.12/x86_64/lib:/opt/share/Python-3.5.2/x86_64/lib:/opt/share/cuDNN-v5.1-8.0/cuda/lib64:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/opt/share/cuda-8.0/
2020-09-15 15:49:38.591704: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1753] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2020-09-15 15:49:38.592125: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-09-15 15:49:38.628624: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2600005000 Hz
2020-09-15 15:49:38.628898: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x56113389d970 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-09-15 15:49:38.628918: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-09-15 15:49:38.631826: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-09-15 15:49:38.631861: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      
Prefix path:  /dccstor/cogfin/arvind/da/VeeAlign/
Ontologies being aligned are:  [('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/confOf.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/conference.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/conference.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/conference.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/conference.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/confOf.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/conference.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/confOf.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/confOf.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/confOf.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl')]
Total number of extracted unique classes and properties from entire RA set:  829
Constructing abbrevation resolution dict....
Results after abbreviation resolution:  {'PC': 'Program Committee', 'OC': 'Organizing Committee'}
Resolving abbreviations...
Number of entities: 119639
Starting sliding window evaluation...
Step 0/7
Val onto:  [('confof', 'ekaw')] test_onto:  [('conference', 'sigkdd')]
Training size: 112565 Testing size: 3871
Epoch: 0 Idx: 0 Loss: 0.19911351092696883
Epoch: 0 Idx: 5000 Loss: 0.00979275438246981
Epoch: 1 Idx: 0 Loss: 0.01224392475432115
Epoch: 1 Idx: 5000 Loss: 0.019860462006246322
Epoch: 2 Idx: 0 Loss: 0.003901354361882669
Epoch: 2 Idx: 5000 Loss: 0.009042793400595443
Epoch: 3 Idx: 0 Loss: 0.023606934656856527
Epoch: 3 Idx: 5000 Loss: 0.027262394516147745
Epoch: 4 Idx: 0 Loss: 0.011944123086703033
Epoch: 4 Idx: 5000 Loss: 0.011200963257174679
Epoch: 5 Idx: 0 Loss: 0.011580205463567
Epoch: 5 Idx: 5000 Loss: 0.014133881381674429
Epoch: 6 Idx: 0 Loss: 0.024877232742815378
Epoch: 6 Idx: 5000 Loss: 0.01420782901862015
Epoch: 7 Idx: 0 Loss: 0.013771859226421256
Epoch: 7 Idx: 5000 Loss: 0.022252390008669198
Epoch: 8 Idx: 0 Loss: 0.014169491744299632
Epoch: 8 Idx: 5000 Loss: 0.006744445331853799
Epoch: 9 Idx: 0 Loss: 0.02082467329484471
Epoch: 9 Idx: 5000 Loss: 0.013675944318198699
Epoch: 10 Idx: 0 Loss: 0.024123778861539203
Epoch: 10 Idx: 5000 Loss: 0.038466369862698094
Epoch: 11 Idx: 0 Loss: 0.013418269480465532
Epoch: 11 Idx: 5000 Loss: 0.015364363490034836
Epoch: 12 Idx: 0 Loss: 0.005407951664837103
Epoch: 12 Idx: 5000 Loss: 0.015422002050600072
Epoch: 13 Idx: 0 Loss: 0.012265154444516386
Epoch: 13 Idx: 5000 Loss: 0.023067123438070068
Epoch: 14 Idx: 0 Loss: 0.012370971275244952
Epoch: 14 Idx: 5000 Loss: 0.025742959871271346
Epoch: 15 Idx: 0 Loss: 0.014754690162828408
Epoch: 15 Idx: 5000 Loss: 0.017004820887128902
Epoch: 16 Idx: 0 Loss: 0.0223717635336558
Epoch: 16 Idx: 5000 Loss: 0.02210585600979169
Epoch: 17 Idx: 0 Loss: 0.021416533692567852
Epoch: 17 Idx: 5000 Loss: 0.013709253150691329
Epoch: 18 Idx: 0 Loss: 0.018453706934599527
Epoch: 18 Idx: 5000 Loss: 0.016461802588344218
Epoch: 19 Idx: 0 Loss: 0.019990743790367763
Epoch: 19 Idx: 5000 Loss: 0.01276154953133055
Epoch: 20 Idx: 0 Loss: 0.013617760757105236
Epoch: 20 Idx: 5000 Loss: 0.017063364573690967
Epoch: 21 Idx: 0 Loss: 0.009625993522271867
Epoch: 21 Idx: 5000 Loss: 0.016082625950897034
Epoch: 22 Idx: 0 Loss: 0.012762132748030725
Epoch: 22 Idx: 5000 Loss: 0.020706693034078214
Epoch: 23 Idx: 0 Loss: 0.01054984550589447
Epoch: 23 Idx: 5000 Loss: 0.01588523010003879
Epoch: 24 Idx: 0 Loss: 0.01763590839183885
Epoch: 24 Idx: 5000 Loss: 0.013268217123556431
Epoch: 25 Idx: 0 Loss: 0.015189654846893339
Epoch: 25 Idx: 5000 Loss: 0.013708928464705238
Epoch: 26 Idx: 0 Loss: 0.014223091829421153
Epoch: 26 Idx: 5000 Loss: 0.0138629229327965
Epoch: 27 Idx: 0 Loss: 0.008174599360821991
Epoch: 27 Idx: 5000 Loss: 0.020997965918554462
Epoch: 28 Idx: 0 Loss: 0.016352509779511665
Epoch: 28 Idx: 5000 Loss: 0.012877483736054515
Epoch: 29 Idx: 0 Loss: 0.016463341264950245
Epoch: 29 Idx: 5000 Loss: 0.00880670885344634
Epoch: 30 Idx: 0 Loss: 0.019633115681992327
Epoch: 30 Idx: 5000 Loss: 0.023614301289789547
Epoch: 31 Idx: 0 Loss: 0.02392276357608655
Epoch: 31 Idx: 5000 Loss: 0.014405273716395722
Epoch: 32 Idx: 0 Loss: 0.023502773849927043
Epoch: 32 Idx: 5000 Loss: 0.016338825341985823
Epoch: 33 Idx: 0 Loss: 0.008428298231386563
Epoch: 33 Idx: 5000 Loss: 0.024215883449887433
Epoch: 34 Idx: 0 Loss: 0.0425848777130073
Epoch: 34 Idx: 5000 Loss: 0.010565035018309302
Epoch: 35 Idx: 0 Loss: 0.006720824714850533
Epoch: 35 Idx: 5000 Loss: 0.0186980163492213
Epoch: 36 Idx: 0 Loss: 0.015089741338084488
Epoch: 36 Idx: 5000 Loss: 0.009751623413673001
Epoch: 37 Idx: 0 Loss: 0.008348496133249818
Epoch: 37 Idx: 5000 Loss: 0.03005306871510304
Epoch: 38 Idx: 0 Loss: 0.008801285518407179
Epoch: 38 Idx: 5000 Loss: 0.024353436684207894
Epoch: 39 Idx: 0 Loss: 0.046594778895172044
Epoch: 39 Idx: 5000 Loss: 0.053326641972455784
Epoch: 40 Idx: 0 Loss: 0.00966587577387791
Epoch: 40 Idx: 5000 Loss: 0.016043972895262822
Epoch: 41 Idx: 0 Loss: 0.018271060180925905
Epoch: 41 Idx: 5000 Loss: 0.006657889406316116
Epoch: 42 Idx: 0 Loss: 0.012967946369487487
Epoch: 42 Idx: 5000 Loss: 0.0051798126394209205
Epoch: 43 Idx: 0 Loss: 0.013064751529622562
Epoch: 43 Idx: 5000 Loss: 0.014787282399706173
Epoch: 44 Idx: 0 Loss: 0.008876753166286827
Epoch: 44 Idx: 5000 Loss: 0.01956999134663144
Epoch: 45 Idx: 0 Loss: 0.012913613188710725
Epoch: 45 Idx: 5000 Loss: 0.009398704327441968
Epoch: 46 Idx: 0 Loss: 0.006995161560154985
Epoch: 46 Idx: 5000 Loss: 0.0281810466701285
Epoch: 47 Idx: 0 Loss: 0.005912503234223799
Epoch: 47 Idx: 5000 Loss: 0.01273335297146231
Epoch: 48 Idx: 0 Loss: 0.009657950917804574
Epoch: 48 Idx: 5000 Loss: 0.016574277710796843
Epoch: 49 Idx: 0 Loss: 0.011341275343721292
Epoch: 49 Idx: 5000 Loss: 0.04308287045922476
Len (direct inputs):  429
Inputs len 2744 15 3856
Len (direct inputs):  1127
Starting sliding window evaluation...
Step 2/7
Val onto:  [('conference', 'ekaw')] test_onto:  [('cmt', 'sigkdd')]
Training size: 111450 Testing size: 2364
Epoch: 0 Idx: 0 Loss: 0.14670070914272493
Epoch: 0 Idx: 5000 Loss: 0.018918348759343007
Epoch: 1 Idx: 0 Loss: 0.014537571270602708
Epoch: 1 Idx: 5000 Loss: 0.026090345557060603
Epoch: 2 Idx: 0 Loss: 0.01687460474695103
Epoch: 2 Idx: 5000 Loss: 0.008174397817985742
Epoch: 3 Idx: 0 Loss: 0.011254376628283332
Epoch: 3 Idx: 5000 Loss: 0.019714544022615573
Epoch: 4 Idx: 0 Loss: 0.01030382621750384
Epoch: 4 Idx: 5000 Loss: 0.013609923729871328
Epoch: 5 Idx: 0 Loss: 0.012302980801734555
Epoch: 5 Idx: 5000 Loss: 0.007180796437658045
Epoch: 6 Idx: 0 Loss: 0.02439498232679031
Epoch: 6 Idx: 5000 Loss: 0.017763406857428582
Epoch: 7 Idx: 0 Loss: 0.01907847066342456
Epoch: 7 Idx: 5000 Loss: 0.019015736937446764
Epoch: 8 Idx: 0 Loss: 0.02048396180491895
Epoch: 8 Idx: 5000 Loss: 0.015424880825846542
Epoch: 9 Idx: 0 Loss: 0.0175075513608275
Epoch: 9 Idx: 5000 Loss: 0.023891221520323552
Epoch: 10 Idx: 0 Loss: 0.014562647231124454
Epoch: 10 Idx: 5000 Loss: 0.0275791661469571
Epoch: 11 Idx: 0 Loss: 0.0362446845463863
Epoch: 11 Idx: 5000 Loss: 0.008006324462661677
Epoch: 12 Idx: 0 Loss: 0.01803058100671053
Epoch: 12 Idx: 5000 Loss: 0.02342656945053487
Epoch: 13 Idx: 0 Loss: 0.031794891827494655
Epoch: 13 Idx: 5000 Loss: 0.01286582978175932
Epoch: 14 Idx: 0 Loss: 0.009758736646600855
Epoch: 14 Idx: 5000 Loss: 0.016565071724107483
Epoch: 15 Idx: 0 Loss: 0.015295552247962474
Epoch: 15 Idx: 5000 Loss: 0.020675090506376698
Epoch: 16 Idx: 0 Loss: 0.019371451515589833
Epoch: 16 Idx: 5000 Loss: 0.009601113717581097
Epoch: 17 Idx: 0 Loss: 0.02026384716105997
Epoch: 17 Idx: 5000 Loss: 0.00880948196723778
Epoch: 18 Idx: 0 Loss: 0.010144481822092554
Epoch: 18 Idx: 5000 Loss: 0.01127807162267894
Epoch: 19 Idx: 0 Loss: 0.021983947997035777
Epoch: 19 Idx: 5000 Loss: 0.014304939496725962
Epoch: 20 Idx: 0 Loss: 0.01658721086260105
Epoch: 20 Idx: 5000 Loss: 0.019868090598364252
Epoch: 21 Idx: 0 Loss: 0.00553962207615799
Epoch: 21 Idx: 5000 Loss: 0.023246384407775933
Epoch: 22 Idx: 0 Loss: 0.018304426028486767
Epoch: 22 Idx: 5000 Loss: 0.007375682451637954
Epoch: 23 Idx: 0 Loss: 0.01258992968195034
Epoch: 23 Idx: 5000 Loss: 0.015774215129268958
Epoch: 24 Idx: 0 Loss: 0.009483633268488582
Epoch: 24 Idx: 5000 Loss: 0.013078337701470407
Epoch: 25 Idx: 0 Loss: 0.03245662832631096
Epoch: 25 Idx: 5000 Loss: 0.03619109399375064
Epoch: 26 Idx: 0 Loss: 0.01966783192449109
Epoch: 26 Idx: 5000 Loss: 0.020866249997465385
Epoch: 27 Idx: 0 Loss: 0.031224698327321346
Epoch: 27 Idx: 5000 Loss: 0.013470146852115342
Epoch: 28 Idx: 0 Loss: 0.0182052852737224
Epoch: 28 Idx: 5000 Loss: 0.028328498097098913
Epoch: 29 Idx: 0 Loss: 0.020327947252584477
Epoch: 29 Idx: 5000 Loss: 0.0462399509667732
Epoch: 30 Idx: 0 Loss: 0.01761732969510096
Epoch: 30 Idx: 5000 Loss: 0.021738077706997552
Epoch: 31 Idx: 0 Loss: 0.024626175609778794
Epoch: 31 Idx: 5000 Loss: 0.013436894423520133
Epoch: 32 Idx: 0 Loss: 0.012970735357711732
Epoch: 32 Idx: 5000 Loss: 0.012735954224740435
Epoch: 33 Idx: 0 Loss: 0.006809291346788435
Epoch: 33 Idx: 5000 Loss: 0.022892957768807395
Epoch: 34 Idx: 0 Loss: 0.013277099331510538
Epoch: 34 Idx: 5000 Loss: 0.01840128387141187
Epoch: 35 Idx: 0 Loss: 0.007317931534959054
Epoch: 35 Idx: 5000 Loss: 0.01250916719993143
Epoch: 36 Idx: 0 Loss: 0.007481322909076594
Epoch: 36 Idx: 5000 Loss: 0.015949971367080158
Epoch: 37 Idx: 0 Loss: 0.015345586307449133
Epoch: 37 Idx: 5000 Loss: 0.01213899255967239
Epoch: 38 Idx: 0 Loss: 0.012061257684113756
Epoch: 38 Idx: 5000 Loss: 0.006697271643401884
Epoch: 39 Idx: 0 Loss: 0.014949658464890162
Epoch: 39 Idx: 5000 Loss: 0.01876584421691986
Epoch: 40 Idx: 0 Loss: 0.01202435348965949
Epoch: 40 Idx: 5000 Loss: 0.03338145026437954
Epoch: 41 Idx: 0 Loss: 0.01095823209075693
Epoch: 41 Idx: 5000 Loss: 0.023160327849582548
Epoch: 42 Idx: 0 Loss: 0.01713011124596692
Epoch: 42 Idx: 5000 Loss: 0.05203147956226233
Epoch: 43 Idx: 0 Loss: 0.01466306575320363
Epoch: 43 Idx: 5000 Loss: 0.01251953277424487
Epoch: 44 Idx: 0 Loss: 0.012197703655857543
Epoch: 44 Idx: 5000 Loss: 0.024469745785658147
Epoch: 45 Idx: 0 Loss: 0.007701744409742261
Epoch: 45 Idx: 5000 Loss: 0.010779162245989958
Epoch: 46 Idx: 0 Loss: 0.007713693707204106
Epoch: 46 Idx: 5000 Loss: 0.03676473298726052
Epoch: 47 Idx: 0 Loss: 0.013209265574967495
Epoch: 47 Idx: 5000 Loss: 0.010169624991739788
Epoch: 48 Idx: 0 Loss: 0.025652172909592402
Epoch: 48 Idx: 5000 Loss: 0.01849188088333066
Epoch: 49 Idx: 0 Loss: 0.009934017511699431
Epoch: 49 Idx: 5000 Loss: 0.017081168700185445
Len (direct inputs):  1737
Inputs len 1372 12 2352
Len (direct inputs):  992
Starting sliding window evaluation...
Step 4/7
Val onto:  [('ekaw', 'sigkdd')] test_onto:  [('cmt', 'conference')]
Training size: 111356 Testing size: 4145
Epoch: 0 Idx: 0 Loss: 0.14650850895613537
Epoch: 0 Idx: 5000 Loss: 0.018804128368646852
Epoch: 1 Idx: 0 Loss: 0.013004696846721352
Epoch: 1 Idx: 5000 Loss: 0.006447750157412766
Epoch: 2 Idx: 0 Loss: 0.0186071069128642
Epoch: 2 Idx: 5000 Loss: 0.011205890620182067
Epoch: 3 Idx: 0 Loss: 0.008778762624424482
Epoch: 3 Idx: 5000 Loss: 0.010000347964724193
Epoch: 4 Idx: 0 Loss: 0.027385553595374007
Epoch: 4 Idx: 5000 Loss: 0.014355100759405763
Epoch: 5 Idx: 0 Loss: 0.01587589946086354
Epoch: 5 Idx: 5000 Loss: 0.019438686330971924
Epoch: 6 Idx: 0 Loss: 0.013211705151912598
Epoch: 6 Idx: 5000 Loss: 0.016156426786589073
Epoch: 7 Idx: 0 Loss: 0.030007021798839996
Epoch: 7 Idx: 5000 Loss: 0.019456217339237282
Epoch: 8 Idx: 0 Loss: 0.009357702227114649
Epoch: 8 Idx: 5000 Loss: 0.019787258931829838
Epoch: 9 Idx: 0 Loss: 0.013893997107001512
Epoch: 9 Idx: 5000 Loss: 0.008805022753861459
Epoch: 10 Idx: 0 Loss: 0.02078905066314861
Epoch: 10 Idx: 5000 Loss: 0.00879028433971106
Epoch: 11 Idx: 0 Loss: 0.014974422556551239
Epoch: 11 Idx: 5000 Loss: 0.005496612880440037
Epoch: 12 Idx: 0 Loss: 0.01206678706876071
Epoch: 12 Idx: 5000 Loss: 0.011397817665888913
Epoch: 13 Idx: 0 Loss: 0.01978467151126445
Epoch: 13 Idx: 5000 Loss: 0.006464292230749572
Epoch: 14 Idx: 0 Loss: 0.04479454840876999
Epoch: 14 Idx: 5000 Loss: 0.014026490995860487
Epoch: 15 Idx: 0 Loss: 0.009490706945097087
Epoch: 15 Idx: 5000 Loss: 0.02798687198214533
Epoch: 16 Idx: 0 Loss: 0.0190756051129216
Epoch: 16 Idx: 5000 Loss: 0.015786902941132414
Epoch: 17 Idx: 0 Loss: 0.018234045997379333
Epoch: 17 Idx: 5000 Loss: 0.021785704264945038
Epoch: 18 Idx: 0 Loss: 0.010128910240588321
Epoch: 18 Idx: 5000 Loss: 0.03159458768708337
Epoch: 19 Idx: 0 Loss: 0.026729167948820406
Epoch: 19 Idx: 5000 Loss: 0.007504658343642251
Epoch: 20 Idx: 0 Loss: 0.006987788111328589
Epoch: 20 Idx: 5000 Loss: 0.015920433209672524
Epoch: 21 Idx: 0 Loss: 0.02190168994643451
Epoch: 21 Idx: 5000 Loss: 0.02324994077853281
Epoch: 22 Idx: 0 Loss: 0.017058818162587825
Epoch: 22 Idx: 5000 Loss: 0.01862937331066133
Epoch: 23 Idx: 0 Loss: 0.018890512314878472
Epoch: 23 Idx: 5000 Loss: 0.009777770729913843
Epoch: 24 Idx: 0 Loss: 0.022487564047745864
Epoch: 24 Idx: 5000 Loss: 0.010218939581932525
Epoch: 25 Idx: 0 Loss: 0.0058192934386540615
Epoch: 25 Idx: 5000 Loss: 0.03407323290982702
Epoch: 26 Idx: 0 Loss: 0.018119013132198953
Epoch: 26 Idx: 5000 Loss: 0.011726836249682637
Epoch: 27 Idx: 0 Loss: 0.01510156616004415
Epoch: 27 Idx: 5000 Loss: 0.005429473512531716
Epoch: 28 Idx: 0 Loss: 0.017197373352364265
Epoch: 28 Idx: 5000 Loss: 0.027255395536505845
Epoch: 29 Idx: 0 Loss: 0.024497554811305958
Epoch: 29 Idx: 5000 Loss: 0.011891977001309296
Epoch: 30 Idx: 0 Loss: 0.010395529434229113
Epoch: 30 Idx: 5000 Loss: 0.009041375950484015
Epoch: 31 Idx: 0 Loss: 0.016671677141898666
Epoch: 31 Idx: 5000 Loss: 0.01873401035905153
Epoch: 32 Idx: 0 Loss: 0.008807205739421155
Epoch: 32 Idx: 5000 Loss: 0.009130777797025678
Epoch: 33 Idx: 0 Loss: 0.005237722111705116
Epoch: 33 Idx: 5000 Loss: 0.007888264266672065
Epoch: 34 Idx: 0 Loss: 0.016964609028670204
Epoch: 34 Idx: 5000 Loss: 0.007221016654591886
Epoch: 35 Idx: 0 Loss: 0.017804306960934844
Epoch: 35 Idx: 5000 Loss: 0.006971027849649578
Epoch: 36 Idx: 0 Loss: 0.012382885480807657
Epoch: 36 Idx: 5000 Loss: 0.04103934287307526
Epoch: 37 Idx: 0 Loss: 0.009383831785204678
Epoch: 37 Idx: 5000 Loss: 0.010538393278855815
Epoch: 38 Idx: 0 Loss: 0.00803568868728978
Epoch: 38 Idx: 5000 Loss: 0.011178045231404032
Epoch: 39 Idx: 0 Loss: 0.033961310331860714
Epoch: 39 Idx: 5000 Loss: 0.01252346592394305
Epoch: 40 Idx: 0 Loss: 0.03305590148314999
Epoch: 40 Idx: 5000 Loss: 0.025867219560177873
Epoch: 41 Idx: 0 Loss: 0.01504371314266991
Epoch: 41 Idx: 5000 Loss: 0.013867637897122718
Epoch: 42 Idx: 0 Loss: 0.029591292278562442
Epoch: 42 Idx: 5000 Loss: 0.016289734663978782
Epoch: 43 Idx: 0 Loss: 0.022814015686857675
Traceback (most recent call last):
  File "main.py", line 514, in <module>
    outputs = model(node_elems, inp_elems)
  File "/u/harshitk/miniconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "main.py", line 312, in forward
    feature_emb = self.name_embedding(features[i]) #  dim: (2, batch_size, max_types, max_paths, max_pathlen, 512)
  File "/u/harshitk/miniconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/u/harshitk/miniconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/sparse.py", line 126, in forward
    self.norm_type, self.scale_grad_by_freq, self.sparse)
  File "/u/harshitk/miniconda3/envs/py37/lib/python3.7/site-packages/torch/nn/functional.py", line 1814, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
KeyboardInterrupt

------------------------------------------------------------
Sender: LSF System <rer@dccxc268>
Subject: Job 4066860: <python main.py 5 12 False True> in cluster <dcc> Exited

Job <python main.py 5 12 False True> was submitted from host <dccxl004> by user <harshitk> in cluster <dcc> at Tue Sep 15 15:48:38 2020
Job was executed on host(s) <dccxc268>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep 15 15:49:23 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/VeeAlign/src> was used as the working directory.
Started at Tue Sep 15 15:49:23 2020
Terminated at Wed Sep 16 04:38:42 2020
Results reported at Wed Sep 16 04:38:42 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python main.py 5 12 False True
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   46068.35 sec.
    Max Memory :                                 2943 MB
    Average Memory :                             2742.71 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40474.00 MB
    Max Swap :                                   2 MB
    Max Processes :                              3
    Max Threads :                                13
    Run time :                                   46185 sec.
    Turnaround time :                            46204 sec.

The output (if any) is above this job summary.

