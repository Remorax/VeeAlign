2020-09-15 15:48:43.653917: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-09-15 15:48:50.740938: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1
2020-09-15 15:48:50.856159: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:1e:00.0 name: Tesla K80 computeCapability: 3.7
coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s
2020-09-15 15:48:50.856236: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-09-15 15:48:50.858586: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
2020-09-15 15:48:50.878028: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2020-09-15 15:48:50.914110: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2020-09-15 15:48:50.957552: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2020-09-15 15:48:50.980352: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10
2020-09-15 15:48:50.980961: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudnn.so.7'; dlerror: libcudnn.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/share/lsf-9.1.3/10.1/linux3.10-glibc2.17-x86_64/lib:=/opt/share/gcc-4.9.2_rhel6/x86_64/lib/:/opt/share/gcc-4.9.2_rhel6/x86_64/lib64:/opt/share/Python-3.6.2/x86_64/lib:=/opt/share/gcc-5.4.0/x86_64/lib/:/opt/share/gcc-5.4.0/x86_64/lib64:/opt/share/isl-0.17/x86_64/lib/:/opt/share/protobuf-3.1.0/x86_64/lib/:/opt/share/leveldb-1.19/x86_64/lib/:/opt/share/boost-1.62.0/x86_64/lib/:/opt/share/torch-7/x86_64/install/lib:/opt/share/Python-2.7.12/x86_64/lib:/opt/share/Python-3.5.2/x86_64/lib:/opt/share/cuDNN-v5.1-8.0/cuda/lib64:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/opt/share/cuda-8.0/
2020-09-15 15:48:50.980986: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1753] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2020-09-15 15:48:50.981506: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-09-15 15:48:51.027693: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2600145000 Hz
2020-09-15 15:48:51.028072: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55684032e520 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-09-15 15:48:51.028096: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-09-15 15:48:51.031054: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-09-15 15:48:51.031091: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      
Prefix path:  /dccstor/cogfin/arvind/da/VeeAlign/
Ontologies being aligned are:  [('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/confOf.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/conference.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/conference.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/conference.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/conference.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/confOf.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/conference.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/confOf.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/confOf.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/confOf.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl')]
Total number of extracted unique classes and properties from entire RA set:  829
Constructing abbrevation resolution dict....
Results after abbreviation resolution:  {'PC': 'Program Committee', 'OC': 'Organizing Committee'}
Resolving abbreviations...
Number of entities: 119639
Starting sliding window evaluation...
Step 0/7
Val onto:  [('confof', 'ekaw')] test_onto:  [('conference', 'sigkdd')]
Training size: 112565 Testing size: 3871
Epoch: 0 Idx: 0 Loss: 0.18754966528252107
Epoch: 0 Idx: 5000 Loss: 0.012287303019320888
Epoch: 1 Idx: 0 Loss: 0.015520842121057806
Epoch: 1 Idx: 5000 Loss: 0.011699347942011705
Epoch: 2 Idx: 0 Loss: 0.011993069525815673
Epoch: 2 Idx: 5000 Loss: 0.008253190086752557
Epoch: 3 Idx: 0 Loss: 0.028629001910411067
Epoch: 3 Idx: 5000 Loss: 0.01765654411779729
Epoch: 4 Idx: 0 Loss: 0.005329113194903536
Epoch: 4 Idx: 5000 Loss: 0.00923959322743705
Epoch: 5 Idx: 0 Loss: 0.011731087203221004
Epoch: 5 Idx: 5000 Loss: 0.007213816650702912
Epoch: 6 Idx: 0 Loss: 0.018207874742497947
Epoch: 6 Idx: 5000 Loss: 0.0090756073118886
Epoch: 7 Idx: 0 Loss: 0.016448522676057956
Epoch: 7 Idx: 5000 Loss: 0.029535810324932618
Epoch: 8 Idx: 0 Loss: 0.02056351557357793
Epoch: 8 Idx: 5000 Loss: 0.005637319396509591
Epoch: 9 Idx: 0 Loss: 0.014883980809544697
Epoch: 9 Idx: 5000 Loss: 0.010441625523933332
Epoch: 10 Idx: 0 Loss: 0.01092630098884386
Epoch: 10 Idx: 5000 Loss: 0.015221735328313814
Epoch: 11 Idx: 0 Loss: 0.011584201066458135
Epoch: 11 Idx: 5000 Loss: 0.019001863892910607
Epoch: 12 Idx: 0 Loss: 0.004969158561487463
Epoch: 12 Idx: 5000 Loss: 0.031103872756676886
Epoch: 13 Idx: 0 Loss: 0.0033705152034797683
Epoch: 13 Idx: 5000 Loss: 0.009280973259006136
Epoch: 14 Idx: 0 Loss: 0.04849597212699151
Epoch: 14 Idx: 5000 Loss: 0.013849141915572693
Epoch: 15 Idx: 0 Loss: 0.012789266273372237
Epoch: 15 Idx: 5000 Loss: 0.029541745498797418
Epoch: 16 Idx: 0 Loss: 0.007713454296428213
Epoch: 16 Idx: 5000 Loss: 0.012586923726865625
Epoch: 17 Idx: 0 Loss: 0.05036434350046514
Epoch: 17 Idx: 5000 Loss: 0.021222200350043272
Epoch: 18 Idx: 0 Loss: 0.017860263404375742
Epoch: 18 Idx: 5000 Loss: 0.04044333308024778
Epoch: 19 Idx: 0 Loss: 0.019683733429239066
Epoch: 19 Idx: 5000 Loss: 0.015417609704408093
Epoch: 20 Idx: 0 Loss: 0.014334420791534639
Epoch: 20 Idx: 5000 Loss: 0.011411209860760737
Epoch: 21 Idx: 0 Loss: 0.029150352731500923
Epoch: 21 Idx: 5000 Loss: 0.02246908535561673
Epoch: 22 Idx: 0 Loss: 0.007980997041212815
Epoch: 22 Idx: 5000 Loss: 0.012360750090153114
Epoch: 23 Idx: 0 Loss: 0.010045522248649172
Epoch: 23 Idx: 5000 Loss: 0.012511302404861195
Epoch: 24 Idx: 0 Loss: 0.023605220911566957
Epoch: 24 Idx: 5000 Loss: 0.010063333990135181
Epoch: 25 Idx: 0 Loss: 0.015791637890244046
Epoch: 25 Idx: 5000 Loss: 0.024564237708541967
Epoch: 26 Idx: 0 Loss: 0.031942788512213445
Epoch: 26 Idx: 5000 Loss: 0.029384054533025672
Epoch: 27 Idx: 0 Loss: 0.02680382336633026
Epoch: 27 Idx: 5000 Loss: 0.009419545995463914
Epoch: 28 Idx: 0 Loss: 0.025936307872555294
Epoch: 28 Idx: 5000 Loss: 0.015939434021722314
Epoch: 29 Idx: 0 Loss: 0.013527229518324106
Epoch: 29 Idx: 5000 Loss: 0.021077132188321916
Epoch: 30 Idx: 0 Loss: 0.013451983373421031
Epoch: 30 Idx: 5000 Loss: 0.010485838716942622
Epoch: 31 Idx: 0 Loss: 0.009911144492355499
Epoch: 31 Idx: 5000 Loss: 0.04092707854138153
Epoch: 32 Idx: 0 Loss: 0.03319187451667115
Epoch: 32 Idx: 5000 Loss: 0.01713302352609192
Epoch: 33 Idx: 0 Loss: 0.008833375352275361
Epoch: 33 Idx: 5000 Loss: 0.024971450064087422
Epoch: 34 Idx: 0 Loss: 0.014677217462868243
Epoch: 34 Idx: 5000 Loss: 0.04019890362473115
Epoch: 35 Idx: 0 Loss: 0.014105833968098603
Epoch: 35 Idx: 5000 Loss: 0.0259063594279463
Epoch: 36 Idx: 0 Loss: 0.012689183863257068
Epoch: 36 Idx: 5000 Loss: 0.017952029149119177
Epoch: 37 Idx: 0 Loss: 0.022440802560381844
Epoch: 37 Idx: 5000 Loss: 0.00977625117550578
Epoch: 38 Idx: 0 Loss: 0.024925705840292574
Epoch: 38 Idx: 5000 Loss: 0.011079388632492105
Epoch: 39 Idx: 0 Loss: 0.014893185214321383
Epoch: 39 Idx: 5000 Loss: 0.02290127562936663
Epoch: 40 Idx: 0 Loss: 0.05512346622931391
Epoch: 40 Idx: 5000 Loss: 0.015338643811606961
Epoch: 41 Idx: 0 Loss: 0.011411610154954899
Epoch: 41 Idx: 5000 Loss: 0.014275824924096018
Epoch: 42 Idx: 0 Loss: 0.011991385919170096
Epoch: 42 Idx: 5000 Loss: 0.02843984083838704
Epoch: 43 Idx: 0 Loss: 0.017751891774530668
Epoch: 43 Idx: 5000 Loss: 0.033696706867165103
Epoch: 44 Idx: 0 Loss: 0.015635727687164872
Epoch: 44 Idx: 5000 Loss: 0.010512856269559251
Epoch: 45 Idx: 0 Loss: 0.0072895378520717174
Epoch: 45 Idx: 5000 Loss: 0.010982648285443443
Epoch: 46 Idx: 0 Loss: 0.029414158174918232
Epoch: 46 Idx: 5000 Loss: 0.009217631026214888
Epoch: 47 Idx: 0 Loss: 0.021176498952899952
Epoch: 47 Idx: 5000 Loss: 0.011071035181093968
Epoch: 48 Idx: 0 Loss: 0.0359847166123049
Epoch: 48 Idx: 5000 Loss: 0.020695193067279447
Epoch: 49 Idx: 0 Loss: 0.014869528367375812
Epoch: 49 Idx: 5000 Loss: 0.023817719451783098
Len (direct inputs):  429
Inputs len 2744 15 3856
Len (direct inputs):  1127
Starting sliding window evaluation...
Step 2/7
Val onto:  [('conference', 'ekaw')] test_onto:  [('cmt', 'sigkdd')]
Training size: 111450 Testing size: 2364
Epoch: 0 Idx: 0 Loss: 0.14879860300197553
Epoch: 0 Idx: 5000 Loss: 0.015750545201918893
Epoch: 1 Idx: 0 Loss: 0.026280126333609058
Epoch: 1 Idx: 5000 Loss: 0.012658856014160154
Epoch: 2 Idx: 0 Loss: 0.009835391811260313
Epoch: 2 Idx: 5000 Loss: 0.011995134185908059
Epoch: 3 Idx: 0 Loss: 0.013425101084931862
Epoch: 3 Idx: 5000 Loss: 0.01723659691405276
Epoch: 4 Idx: 0 Loss: 0.02041220040544436
Epoch: 4 Idx: 5000 Loss: 0.018364249330040033
Epoch: 5 Idx: 0 Loss: 0.019752003679381295
Epoch: 5 Idx: 5000 Loss: 0.023234072760834182
Epoch: 6 Idx: 0 Loss: 0.010068519344574329
Epoch: 6 Idx: 5000 Loss: 0.020087811750541362
Epoch: 7 Idx: 0 Loss: 0.012772295167941886
Epoch: 7 Idx: 5000 Loss: 0.027075353008643752
Epoch: 8 Idx: 0 Loss: 0.009145515977078682
Epoch: 8 Idx: 5000 Loss: 0.008407976585288354
Epoch: 9 Idx: 0 Loss: 0.01190683845076156
Epoch: 9 Idx: 5000 Loss: 0.012059953906717604
Epoch: 10 Idx: 0 Loss: 0.011620864385606697
Epoch: 10 Idx: 5000 Loss: 0.03578196634350808
Epoch: 11 Idx: 0 Loss: 0.01154622054643953
Epoch: 11 Idx: 5000 Loss: 0.008783346702290553
Epoch: 12 Idx: 0 Loss: 0.03983618527694855
Epoch: 12 Idx: 5000 Loss: 0.06424543233633175
Epoch: 13 Idx: 0 Loss: 0.01008913700787029
Epoch: 13 Idx: 5000 Loss: 0.008217472529473033
Epoch: 14 Idx: 0 Loss: 0.00624193560481407
Epoch: 14 Idx: 5000 Loss: 0.010700196638129805
Epoch: 15 Idx: 0 Loss: 0.013151935740204665
Epoch: 15 Idx: 5000 Loss: 0.012762133930162943
Epoch: 16 Idx: 0 Loss: 0.03551468298583997
Epoch: 16 Idx: 5000 Loss: 0.012331741035749577
Epoch: 17 Idx: 0 Loss: 0.00787236327408303
Epoch: 17 Idx: 5000 Loss: 0.009921335962723436
Epoch: 18 Idx: 0 Loss: 0.01357057891700578
Epoch: 18 Idx: 5000 Loss: 0.01740338074915755
Epoch: 19 Idx: 0 Loss: 0.03247287279030194
Epoch: 19 Idx: 5000 Loss: 0.009459378962411258
Epoch: 20 Idx: 0 Loss: 0.011891348115656186
Epoch: 20 Idx: 5000 Loss: 0.006221657035885008
Epoch: 21 Idx: 0 Loss: 0.016169702238086444
Epoch: 21 Idx: 5000 Loss: 0.044852608541500645
Epoch: 22 Idx: 0 Loss: 0.013229296278865434
Epoch: 22 Idx: 5000 Loss: 0.047638140600574995
Epoch: 23 Idx: 0 Loss: 0.03079327888191334
Epoch: 23 Idx: 5000 Loss: 0.030009805614035483
Epoch: 24 Idx: 0 Loss: 0.014023345882413232
Epoch: 24 Idx: 5000 Loss: 0.028484922674200334
Epoch: 25 Idx: 0 Loss: 0.006348168968129145
Epoch: 25 Idx: 5000 Loss: 0.02674479648481908
Epoch: 26 Idx: 0 Loss: 0.010209893329190057
Epoch: 26 Idx: 5000 Loss: 0.007968896724214329
Epoch: 27 Idx: 0 Loss: 0.0250765874875189
Epoch: 27 Idx: 5000 Loss: 0.013673239197005456
Epoch: 28 Idx: 0 Loss: 0.01111122546905189
Epoch: 28 Idx: 5000 Loss: 0.005137755104022419
Epoch: 29 Idx: 0 Loss: 0.023887826409761198
Epoch: 29 Idx: 5000 Loss: 0.011429680442912296
Epoch: 30 Idx: 0 Loss: 0.014710471110495971
Epoch: 30 Idx: 5000 Loss: 0.01609354050143107
Epoch: 31 Idx: 0 Loss: 0.034311607464308586
Epoch: 31 Idx: 5000 Loss: 0.018278072122820125
Epoch: 32 Idx: 0 Loss: 0.012289750187133072
Epoch: 32 Idx: 5000 Loss: 0.009816001358220222
Epoch: 33 Idx: 0 Loss: 0.013854640706955065
Epoch: 33 Idx: 5000 Loss: 0.006828687757348487
Epoch: 34 Idx: 0 Loss: 0.013297872018291763
Epoch: 34 Idx: 5000 Loss: 0.012284376034856605
Epoch: 35 Idx: 0 Loss: 0.023162138535988572
Epoch: 35 Idx: 5000 Loss: 0.03468227149943297
Epoch: 36 Idx: 0 Loss: 0.024624644108857743
Epoch: 36 Idx: 5000 Loss: 0.03244804481373069
Epoch: 37 Idx: 0 Loss: 0.008684851912435416
Epoch: 37 Idx: 5000 Loss: 0.027472173530954616
Epoch: 38 Idx: 0 Loss: 0.009319837006558841
Epoch: 38 Idx: 5000 Loss: 0.010134738586846676
Epoch: 39 Idx: 0 Loss: 0.0298597720759601
Epoch: 39 Idx: 5000 Loss: 0.00742747848391993
Epoch: 40 Idx: 0 Loss: 0.006928105324669795
Epoch: 40 Idx: 5000 Loss: 0.01727877975577138
Epoch: 41 Idx: 0 Loss: 0.03576818605271373
Epoch: 41 Idx: 5000 Loss: 0.005770451769639105
Epoch: 42 Idx: 0 Loss: 0.010887144190732877
Epoch: 42 Idx: 5000 Loss: 0.00944435129421979
Epoch: 43 Idx: 0 Loss: 0.008162513741086817
Epoch: 43 Idx: 5000 Loss: 0.0066381650733942825
Epoch: 44 Idx: 0 Loss: 0.010147823986488412
Epoch: 44 Idx: 5000 Loss: 0.00953471669295058
Epoch: 45 Idx: 0 Loss: 0.01675163214684828
Epoch: 45 Idx: 5000 Loss: 0.013028415487200663
Epoch: 46 Idx: 0 Loss: 0.010550525351735849
Epoch: 46 Idx: 5000 Loss: 0.01952420066664589
Epoch: 47 Idx: 0 Loss: 0.016087995884290197
Epoch: 47 Idx: 5000 Loss: 0.013153529450622602
Epoch: 48 Idx: 0 Loss: 0.011099428726806897
Epoch: 48 Idx: 5000 Loss: 0.014773139586314637
Epoch: 49 Idx: 0 Loss: 0.010293897025621432
Epoch: 49 Idx: 5000 Loss: 0.01726160440809275
Len (direct inputs):  1737
Inputs len 1372 12 2352
Len (direct inputs):  992
Starting sliding window evaluation...
Step 4/7
Val onto:  [('ekaw', 'sigkdd')] test_onto:  [('cmt', 'conference')]
Training size: 111356 Testing size: 4145
Epoch: 0 Idx: 0 Loss: 0.14393199861571143
Epoch: 0 Idx: 5000 Loss: 0.011240792610106074
Epoch: 1 Idx: 0 Loss: 0.007062533435466629
Epoch: 1 Idx: 5000 Loss: 0.005814763057312644
Epoch: 2 Idx: 0 Loss: 0.017745144688809986
Epoch: 2 Idx: 5000 Loss: 0.020268324521236874
Epoch: 3 Idx: 0 Loss: 0.026024590094539325
Epoch: 3 Idx: 5000 Loss: 0.022179122407829426
Epoch: 4 Idx: 0 Loss: 0.027410284428630305
Epoch: 4 Idx: 5000 Loss: 0.02114785866892641
Epoch: 5 Idx: 0 Loss: 0.019877050113196716
Epoch: 5 Idx: 5000 Loss: 0.011751809095886037
Epoch: 6 Idx: 0 Loss: 0.008085965372514594
Epoch: 6 Idx: 5000 Loss: 0.015115362141840479
Epoch: 7 Idx: 0 Loss: 0.024888866394448282
Epoch: 7 Idx: 5000 Loss: 0.033798594111020694
Epoch: 8 Idx: 0 Loss: 0.00816792204118387
Epoch: 8 Idx: 5000 Loss: 0.019040995557322006
Epoch: 9 Idx: 0 Loss: 0.03257394651358287
Epoch: 9 Idx: 5000 Loss: 0.007802642741150901
Epoch: 10 Idx: 0 Loss: 0.019011698073732517
Epoch: 10 Idx: 5000 Loss: 0.008861938226710253
Epoch: 11 Idx: 0 Loss: 0.014067768676021778
Epoch: 11 Idx: 5000 Loss: 0.01695841881868807
Epoch: 12 Idx: 0 Loss: 0.026569334647837683
Epoch: 12 Idx: 5000 Loss: 0.028296030232284306
Epoch: 13 Idx: 0 Loss: 0.00871385498962635
Epoch: 13 Idx: 5000 Loss: 0.008779762793357416
Epoch: 14 Idx: 0 Loss: 0.019321649664143088
Epoch: 14 Idx: 5000 Loss: 0.007936852800465686
Epoch: 15 Idx: 0 Loss: 0.009577392732464888
Epoch: 15 Idx: 5000 Loss: 0.008935207648538116
Epoch: 16 Idx: 0 Loss: 0.01052900030968498
Epoch: 16 Idx: 5000 Loss: 0.014907215876107651
Epoch: 17 Idx: 0 Loss: 0.016794340462880024
Epoch: 17 Idx: 5000 Loss: 0.013329679990658394
Epoch: 18 Idx: 0 Loss: 0.009317413130016928
Epoch: 18 Idx: 5000 Loss: 0.011071304225578104
Epoch: 19 Idx: 0 Loss: 0.01304924071999096
Epoch: 19 Idx: 5000 Loss: 0.007925643680411324
Epoch: 20 Idx: 0 Loss: 0.009719514405862342
Epoch: 20 Idx: 5000 Loss: 0.01005432075623788
Epoch: 21 Idx: 0 Loss: 0.011595047234577736
Epoch: 21 Idx: 5000 Loss: 0.03718577852865612
Epoch: 22 Idx: 0 Loss: 0.02748369872098646
Epoch: 22 Idx: 5000 Loss: 0.02479588313199082
Epoch: 23 Idx: 0 Loss: 0.010442720966576917
Epoch: 23 Idx: 5000 Loss: 0.023811588246057898
Epoch: 24 Idx: 0 Loss: 0.019805887187427568
Epoch: 24 Idx: 5000 Loss: 0.014504405833847305
Epoch: 25 Idx: 0 Loss: 0.02148825823884853
Epoch: 25 Idx: 5000 Loss: 0.028378471945914995
Epoch: 26 Idx: 0 Loss: 0.008542909921194903
Epoch: 26 Idx: 5000 Loss: 0.024998374981848405
Epoch: 27 Idx: 0 Loss: 0.017060734454392275
Epoch: 27 Idx: 5000 Loss: 0.03397527365599819
Epoch: 28 Idx: 0 Loss: 0.02614836467319612
Epoch: 28 Idx: 5000 Loss: 0.013802582345904103
Epoch: 29 Idx: 0 Loss: 0.00938773042501617
Epoch: 29 Idx: 5000 Loss: 0.01944245270519673
Epoch: 30 Idx: 0 Loss: 0.020659918480448437
Epoch: 30 Idx: 5000 Loss: 0.05106190558958097
Epoch: 31 Idx: 0 Loss: 0.034258497683404186
Epoch: 31 Idx: 5000 Loss: 0.01312545472528152
Epoch: 32 Idx: 0 Loss: 0.0211722070535048
Epoch: 32 Idx: 5000 Loss: 0.018247836075917963
Epoch: 33 Idx: 0 Loss: 0.01072342709548084
Epoch: 33 Idx: 5000 Loss: 0.010656906065223638
Epoch: 34 Idx: 0 Loss: 0.013933632737997786
Epoch: 34 Idx: 5000 Loss: 0.00869446597783366
Epoch: 35 Idx: 0 Loss: 0.012168513424859036
Epoch: 35 Idx: 5000 Loss: 0.037867037278444794
Epoch: 36 Idx: 0 Loss: 0.011349600761973333
Epoch: 36 Idx: 5000 Loss: 0.008294356813032281
Epoch: 37 Idx: 0 Loss: 0.008226376397692092
Epoch: 37 Idx: 5000 Loss: 0.02303121463740624
Epoch: 38 Idx: 0 Loss: 0.019756092309925442
Epoch: 38 Idx: 5000 Loss: 0.022756784137739117
Epoch: 39 Idx: 0 Loss: 0.01076732052057694
Epoch: 39 Idx: 5000 Loss: 0.00713537840589466
Epoch: 40 Idx: 0 Loss: 0.03871764896165465
Epoch: 40 Idx: 5000 Loss: 0.01683504231714642
Epoch: 41 Idx: 0 Loss: 0.02392412085408779
Epoch: 41 Idx: 5000 Loss: 0.01350504906199683
Epoch: 42 Idx: 0 Loss: 0.009116488649419371
Epoch: 42 Idx: 5000 Loss: 0.013192440220753244
Epoch: 43 Idx: 0 Loss: 0.005054925240412331
Epoch: 43 Idx: 5000 Loss: 0.013268488888273977
Epoch: 44 Idx: 0 Loss: 0.01147212419395281
Epoch: 44 Idx: 5000 Loss: 0.015389496134917156
Epoch: 45 Idx: 0 Loss: 0.029161663375934004
Epoch: 45 Idx: 5000 Loss: 0.052041096287224906
Epoch: 46 Idx: 0 Loss: 0.006849964369980429
Epoch: 46 Idx: 5000 Loss: 0.019855560511986992
Epoch: 47 Idx: 0 Loss: 0.01848101528661554
Epoch: 47 Idx: 5000 Loss: 0.01212592786246809
Epoch: 48 Idx: 0 Loss: 0.014233671000453947
Epoch: 48 Idx: 5000 Loss: 0.011960600870631749
Epoch: 49 Idx: 0 Loss: 0.014555851712968844
Epoch: 49 Idx: 5000 Loss: 0.011756013208765476
Len (direct inputs):  561
Inputs len 1568 15 4130
Len (direct inputs):  2577
Starting sliding window evaluation...
Step 6/7
Val onto:  [('edas', 'sigkdd')] test_onto:  [('conference', 'edas')]
Training size: 106045 Testing size: 7817
Epoch: 0 Idx: 0 Loss: 0.21237116486456814
Epoch: 0 Idx: 5000 Loss: 0.013559553992051502
Epoch: 1 Idx: 0 Loss: 0.00823986860412086
Epoch: 1 Idx: 5000 Loss: 0.021965523781000418
Epoch: 2 Idx: 0 Loss: 0.01212403983500362
Epoch: 2 Idx: 5000 Loss: 0.013637244445361758
Epoch: 3 Idx: 0 Loss: 0.02862754254376641
Epoch: 3 Idx: 5000 Loss: 0.023198158041690186
Epoch: 4 Idx: 0 Loss: 0.04049172095672864
Epoch: 4 Idx: 5000 Loss: 0.017755877612910653
Epoch: 5 Idx: 0 Loss: 0.008679275108232496
Epoch: 5 Idx: 5000 Loss: 0.017742408207852424
Epoch: 6 Idx: 0 Loss: 0.018090244349530986
Epoch: 6 Idx: 5000 Loss: 0.0070079309787331005
Epoch: 7 Idx: 0 Loss: 0.007260942333098439
Epoch: 7 Idx: 5000 Loss: 0.02514661788044438
Epoch: 8 Idx: 0 Loss: 0.026427128538350508
Epoch: 8 Idx: 5000 Loss: 0.0073576795235052814
Epoch: 9 Idx: 0 Loss: 0.006551219802238611
Epoch: 9 Idx: 5000 Loss: 0.012129598358299612
Epoch: 10 Idx: 0 Loss: 0.010895813055512536
Epoch: 10 Idx: 5000 Loss: 0.020193122726131354
Epoch: 11 Idx: 0 Loss: 0.008300115505526092
Epoch: 11 Idx: 5000 Loss: 0.011786076523272454
Epoch: 12 Idx: 0 Loss: 0.013136984902730704
Epoch: 12 Idx: 5000 Loss: 0.03629694884042624
Epoch: 13 Idx: 0 Loss: 0.01779011597760337
Epoch: 13 Idx: 5000 Loss: 0.016304484103574483
Epoch: 14 Idx: 0 Loss: 0.032058034541486154
Epoch: 14 Idx: 5000 Loss: 0.009141292422991866
Epoch: 15 Idx: 0 Loss: 0.024178721402584948
Epoch: 15 Idx: 5000 Loss: 0.008283277374140436
Epoch: 16 Idx: 0 Loss: 0.01583453937353789
Epoch: 16 Idx: 5000 Loss: 0.02255138522706425
Epoch: 17 Idx: 0 Loss: 0.008273601365375027
Epoch: 17 Idx: 5000 Loss: 0.011892430214985341
Epoch: 18 Idx: 0 Loss: 0.017752676977911254
Epoch: 18 Idx: 5000 Loss: 0.03340219385147653
Epoch: 19 Idx: 0 Loss: 0.007003722070084423
Epoch: 19 Idx: 5000 Loss: 0.011222850277727318
Epoch: 20 Idx: 0 Loss: 0.023167386200855913
Epoch: 20 Idx: 5000 Loss: 0.016287065257544635
Epoch: 21 Idx: 0 Loss: 0.010551538346787253
Epoch: 21 Idx: 5000 Loss: 0.01118493221943171
Epoch: 22 Idx: 0 Loss: 0.012962734612204586
Epoch: 22 Idx: 5000 Loss: 0.02104715659687905
Epoch: 23 Idx: 0 Loss: 0.01698335785295567
Epoch: 23 Idx: 5000 Loss: 0.029967724073785118
Epoch: 24 Idx: 0 Loss: 0.005097037295017926
Epoch: 24 Idx: 5000 Loss: 0.007470291738187568
Epoch: 25 Idx: 0 Loss: 0.010618775352873474
Epoch: 25 Idx: 5000 Loss: 0.019564768513619206
Epoch: 26 Idx: 0 Loss: 0.02001186107247758
Epoch: 26 Idx: 5000 Loss: 0.014402018933248814
Epoch: 27 Idx: 0 Loss: 0.009248154573764829
Epoch: 27 Idx: 5000 Loss: 0.0062194358947956834
Epoch: 28 Idx: 0 Loss: 0.011019208605286854
Epoch: 28 Idx: 5000 Loss: 0.012643887558019914
Epoch: 29 Idx: 0 Loss: 0.0034965730945225
Epoch: 29 Idx: 5000 Loss: 0.03395653742329946
Epoch: 30 Idx: 0 Loss: 0.011979169378737203
Epoch: 30 Idx: 5000 Loss: 0.0088785517290969
Epoch: 31 Idx: 0 Loss: 0.015270699377081576
Epoch: 31 Idx: 5000 Loss: 0.007374318642598563
Epoch: 32 Idx: 0 Loss: 0.021843215057598876
Epoch: 32 Idx: 5000 Loss: 0.02808899877996572
Epoch: 33 Idx: 0 Loss: 0.04478915781061372
Epoch: 33 Idx: 5000 Loss: 0.009726348759428934
Epoch: 34 Idx: 0 Loss: 0.024521809594447358
Epoch: 34 Idx: 5000 Loss: 0.027797208463672222
Epoch: 35 Idx: 0 Loss: 0.036341068653020954
Epoch: 35 Idx: 5000 Loss: 0.01235892125278636
Epoch: 36 Idx: 0 Loss: 0.027535579859382625
Epoch: 36 Idx: 5000 Loss: 0.010125582866403924
Epoch: 37 Idx: 0 Loss: 0.035582669904425684
Epoch: 37 Idx: 5000 Loss: 0.02593885681536511
Epoch: 38 Idx: 0 Loss: 0.026621276161613323
Traceback (most recent call last):
  File "main.py", line 514, in <module>
    outputs = model(node_elems, inp_elems)
  File "/u/harshitk/miniconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "main.py", line 357, in forward
    output_node_emb = self.output(contextual_node_emb)
  File "/u/harshitk/miniconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/u/harshitk/miniconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/linear.py", line 91, in forward
    return F.linear(input, self.weight, self.bias)
  File "/u/harshitk/miniconda3/envs/py37/lib/python3.7/site-packages/torch/nn/functional.py", line 1674, in linear
    ret = torch.addmm(bias, input, weight.t())
KeyboardInterrupt

------------------------------------------------------------
Sender: LSF System <rer@dccxc273>
Subject: Job 4066833: <python main.py 4 12 False False> in cluster <dcc> Exited

Job <python main.py 4 12 False False> was submitted from host <dccxl004> by user <harshitk> in cluster <dcc> at Tue Sep 15 15:48:37 2020
Job was executed on host(s) <dccxc273>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep 15 15:48:38 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/VeeAlign/src> was used as the working directory.
Started at Tue Sep 15 15:48:38 2020
Terminated at Wed Sep 16 04:38:41 2020
Results reported at Wed Sep 16 04:38:41 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python main.py 4 12 False False
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   44714.70 sec.
    Max Memory :                                 2950 MB
    Average Memory :                             2740.93 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40467.00 MB
    Max Swap :                                   2 MB
    Max Processes :                              3
    Max Threads :                                13
    Run time :                                   46223 sec.
    Turnaround time :                            46204 sec.

The output (if any) is above this job summary.

