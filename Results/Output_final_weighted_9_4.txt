2020-09-15 15:48:43.711006: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-09-15 15:48:51.051164: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1
2020-09-15 15:48:51.167386: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:1b:00.0 name: Tesla K80 computeCapability: 3.7
coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s
2020-09-15 15:48:51.167483: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-09-15 15:48:51.169902: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
2020-09-15 15:48:51.190975: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2020-09-15 15:48:51.226747: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2020-09-15 15:48:51.268957: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2020-09-15 15:48:51.292663: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10
2020-09-15 15:48:51.293246: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudnn.so.7'; dlerror: libcudnn.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/share/lsf-9.1.3/10.1/linux3.10-glibc2.17-x86_64/lib:=/opt/share/gcc-4.9.2_rhel6/x86_64/lib/:/opt/share/gcc-4.9.2_rhel6/x86_64/lib64:/opt/share/Python-3.6.2/x86_64/lib:=/opt/share/gcc-5.4.0/x86_64/lib/:/opt/share/gcc-5.4.0/x86_64/lib64:/opt/share/isl-0.17/x86_64/lib/:/opt/share/protobuf-3.1.0/x86_64/lib/:/opt/share/leveldb-1.19/x86_64/lib/:/opt/share/boost-1.62.0/x86_64/lib/:/opt/share/torch-7/x86_64/install/lib:/opt/share/Python-2.7.12/x86_64/lib:/opt/share/Python-3.5.2/x86_64/lib:/opt/share/cuDNN-v5.1-8.0/cuda/lib64:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/opt/share/cuda-8.0/
2020-09-15 15:48:51.293278: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1753] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2020-09-15 15:48:51.293780: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-09-15 15:48:51.335510: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2599850000 Hz
2020-09-15 15:48:51.335809: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x56045c5d1f10 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-09-15 15:48:51.335832: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-09-15 15:48:51.339015: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-09-15 15:48:51.339037: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      
Prefix path:  /dccstor/cogfin/arvind/da/VeeAlign/
Ontologies being aligned are:  [('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/confOf.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/conference.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/conference.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/conference.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/conference.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/confOf.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/conference.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/confOf.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/confOf.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/confOf.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl')]
Total number of extracted unique classes and properties from entire RA set:  829
Constructing abbrevation resolution dict....
Results after abbreviation resolution:  {'PC': 'Program Committee', 'OC': 'Organizing Committee'}
Resolving abbreviations...
Number of entities: 119639
Starting sliding window evaluation...
Step 0/7
Val onto:  [('confof', 'ekaw')] test_onto:  [('conference', 'sigkdd')]
Training size: 112565 Testing size: 3871
Epoch: 0 Idx: 0 Loss: 0.19616194024145994
Epoch: 0 Idx: 5000 Loss: 0.025477302856024862
Epoch: 1 Idx: 0 Loss: 0.01676439069681963
Epoch: 1 Idx: 5000 Loss: 0.010645739285851481
Epoch: 2 Idx: 0 Loss: 0.012175581883805204
Epoch: 2 Idx: 5000 Loss: 0.014673528737328046
Epoch: 3 Idx: 0 Loss: 0.013183096041800471
Epoch: 3 Idx: 5000 Loss: 0.010725542396568057
Epoch: 4 Idx: 0 Loss: 0.00727450890092554
Epoch: 4 Idx: 5000 Loss: 0.01850846109309349
Epoch: 5 Idx: 0 Loss: 0.007421741699032911
Epoch: 5 Idx: 5000 Loss: 0.016152010990040704
Epoch: 6 Idx: 0 Loss: 0.025545271943590207
Epoch: 6 Idx: 5000 Loss: 0.03505965580326006
Epoch: 7 Idx: 0 Loss: 0.010854443795513712
Epoch: 7 Idx: 5000 Loss: 0.032300198306930705
Epoch: 8 Idx: 0 Loss: 0.015908257819193364
Epoch: 8 Idx: 5000 Loss: 0.010985702613060212
Epoch: 9 Idx: 0 Loss: 0.01466862034833679
Epoch: 9 Idx: 5000 Loss: 0.006370511647372148
Epoch: 10 Idx: 0 Loss: 0.01727742972667977
Epoch: 10 Idx: 5000 Loss: 0.011999287113477638
Epoch: 11 Idx: 0 Loss: 0.019554751914088748
Epoch: 11 Idx: 5000 Loss: 0.014672639393476059
Epoch: 12 Idx: 0 Loss: 0.004754283531656161
Epoch: 12 Idx: 5000 Loss: 0.01174225211122951
Epoch: 13 Idx: 0 Loss: 0.03657730658083322
Epoch: 13 Idx: 5000 Loss: 0.005320911042982609
Epoch: 14 Idx: 0 Loss: 0.0163652335494692
Epoch: 14 Idx: 5000 Loss: 0.030766799245066437
Epoch: 15 Idx: 0 Loss: 0.021134010306187653
Epoch: 15 Idx: 5000 Loss: 0.03348239463076179
Epoch: 16 Idx: 0 Loss: 0.013273117641383971
Epoch: 16 Idx: 5000 Loss: 0.01657947911026015
Epoch: 17 Idx: 0 Loss: 0.01094871856435669
Epoch: 17 Idx: 5000 Loss: 0.01513381053205156
Epoch: 18 Idx: 0 Loss: 0.027045102352664625
Epoch: 18 Idx: 5000 Loss: 0.01064304407125464
Epoch: 19 Idx: 0 Loss: 0.010811023382343372
Epoch: 19 Idx: 5000 Loss: 0.00969667712411949
Epoch: 20 Idx: 0 Loss: 0.005556673167757052
Epoch: 20 Idx: 5000 Loss: 0.010823810429780274
Epoch: 21 Idx: 0 Loss: 0.019863587862095484
Epoch: 21 Idx: 5000 Loss: 0.012558751269676335
Epoch: 22 Idx: 0 Loss: 0.01741024315136927
Epoch: 22 Idx: 5000 Loss: 0.026062493634862252
Epoch: 23 Idx: 0 Loss: 0.01928818003438453
Epoch: 23 Idx: 5000 Loss: 0.02253718814301623
Epoch: 24 Idx: 0 Loss: 0.028449792406069725
Epoch: 24 Idx: 5000 Loss: 0.011072894788554069
Epoch: 25 Idx: 0 Loss: 0.013944144061222439
Epoch: 25 Idx: 5000 Loss: 0.03462414223621597
Epoch: 26 Idx: 0 Loss: 0.015799655077780448
Epoch: 26 Idx: 5000 Loss: 0.02098834809756639
Epoch: 27 Idx: 0 Loss: 0.027822581675547405
Epoch: 27 Idx: 5000 Loss: 0.012491580813075451
Epoch: 28 Idx: 0 Loss: 0.02326958511336194
Epoch: 28 Idx: 5000 Loss: 0.0206502376415134
Epoch: 29 Idx: 0 Loss: 0.009779836556348733
Epoch: 29 Idx: 5000 Loss: 0.011747611537784064
Epoch: 30 Idx: 0 Loss: 0.015958607716783674
Epoch: 30 Idx: 5000 Loss: 0.0070982638618028605
Epoch: 31 Idx: 0 Loss: 0.0170406878161092
Epoch: 31 Idx: 5000 Loss: 0.013821595187394429
Epoch: 32 Idx: 0 Loss: 0.030666386750232342
Epoch: 32 Idx: 5000 Loss: 0.008077980690581538
Epoch: 33 Idx: 0 Loss: 0.019747782120086935
Epoch: 33 Idx: 5000 Loss: 0.023553574577794265
Epoch: 34 Idx: 0 Loss: 0.010354725649459924
Epoch: 34 Idx: 5000 Loss: 0.028318202323172607
Epoch: 35 Idx: 0 Loss: 0.015964988797589195
Epoch: 35 Idx: 5000 Loss: 0.009622149011430884
Epoch: 36 Idx: 0 Loss: 0.010296107214457086
Epoch: 36 Idx: 5000 Loss: 0.012975754533149367
Epoch: 37 Idx: 0 Loss: 0.011797276499984698
Epoch: 37 Idx: 5000 Loss: 0.043471893872685734
Epoch: 38 Idx: 0 Loss: 0.014877801254228834
Epoch: 38 Idx: 5000 Loss: 0.02041628437129299
Epoch: 39 Idx: 0 Loss: 0.013196941021083045
Epoch: 39 Idx: 5000 Loss: 0.021322081445239446
Epoch: 40 Idx: 0 Loss: 0.00749475117819373
Epoch: 40 Idx: 5000 Loss: 0.007338707544672426
Epoch: 41 Idx: 0 Loss: 0.015122826689519989
Epoch: 41 Idx: 5000 Loss: 0.024433618706398014
Epoch: 42 Idx: 0 Loss: 0.024800409256991528
Epoch: 42 Idx: 5000 Loss: 0.012574757947474091
Epoch: 43 Idx: 0 Loss: 0.018858522674853082
Epoch: 43 Idx: 5000 Loss: 0.029066458108463512
Epoch: 44 Idx: 0 Loss: 0.01544958449653825
Epoch: 44 Idx: 5000 Loss: 0.014552465599941751
Epoch: 45 Idx: 0 Loss: 0.01722642702473559
Epoch: 45 Idx: 5000 Loss: 0.019630620401494943
Epoch: 46 Idx: 0 Loss: 0.006399573242919471
Epoch: 46 Idx: 5000 Loss: 0.017986869917272476
Epoch: 47 Idx: 0 Loss: 0.009431407025064242
Epoch: 47 Idx: 5000 Loss: 0.00745207228180426
Epoch: 48 Idx: 0 Loss: 0.01690933211107707
Epoch: 48 Idx: 5000 Loss: 0.017391933518851915
Epoch: 49 Idx: 0 Loss: 0.005517475454096507
Epoch: 49 Idx: 5000 Loss: 0.009221558645610342
Len (direct inputs):  429
Inputs len 2744 15 3856
Len (direct inputs):  1127
Starting sliding window evaluation...
Step 2/7
Val onto:  [('conference', 'ekaw')] test_onto:  [('cmt', 'sigkdd')]
Training size: 111450 Testing size: 2364
Epoch: 0 Idx: 0 Loss: 0.13812691972117336
Epoch: 0 Idx: 5000 Loss: 0.020335518308931447
Epoch: 1 Idx: 0 Loss: 0.0065698780073751035
Epoch: 1 Idx: 5000 Loss: 0.014707835000531667
Epoch: 2 Idx: 0 Loss: 0.025743595849345515
Epoch: 2 Idx: 5000 Loss: 0.014131030994906844
Epoch: 3 Idx: 0 Loss: 0.02094735211360415
Epoch: 3 Idx: 5000 Loss: 0.01688596924222696
Epoch: 4 Idx: 0 Loss: 0.011371538735190062
Epoch: 4 Idx: 5000 Loss: 0.014725367073344967
Epoch: 5 Idx: 0 Loss: 0.01921572688201147
Epoch: 5 Idx: 5000 Loss: 0.011492085385668463
Epoch: 6 Idx: 0 Loss: 0.020719623948248823
Epoch: 6 Idx: 5000 Loss: 0.012446848882153746
Epoch: 7 Idx: 0 Loss: 0.013235464597803023
Epoch: 7 Idx: 5000 Loss: 0.035328016604811864
Epoch: 8 Idx: 0 Loss: 0.010767432277964041
Epoch: 8 Idx: 5000 Loss: 0.0264093122714567
Epoch: 9 Idx: 0 Loss: 0.0192833384156785
Epoch: 9 Idx: 5000 Loss: 0.011329678958745846
Epoch: 10 Idx: 0 Loss: 0.02022193618444358
Epoch: 10 Idx: 5000 Loss: 0.02237000131742329
Epoch: 11 Idx: 0 Loss: 0.025491507399752214
Epoch: 11 Idx: 5000 Loss: 0.009772055775392297
Epoch: 12 Idx: 0 Loss: 0.022384406207205262
Epoch: 12 Idx: 5000 Loss: 0.01481193442660789
Epoch: 13 Idx: 0 Loss: 0.009807028662945317
Epoch: 13 Idx: 5000 Loss: 0.025466202305959026
Epoch: 14 Idx: 0 Loss: 0.014585991899480743
Epoch: 14 Idx: 5000 Loss: 0.02484487416968129
Epoch: 15 Idx: 0 Loss: 0.007630978427080912
Epoch: 15 Idx: 5000 Loss: 0.03109893053503733
Epoch: 16 Idx: 0 Loss: 0.012848758847483547
Epoch: 16 Idx: 5000 Loss: 0.012556517138074593
Epoch: 17 Idx: 0 Loss: 0.009192080331572958
Epoch: 17 Idx: 5000 Loss: 0.023016293493950173
Epoch: 18 Idx: 0 Loss: 0.01801230612104763
Epoch: 18 Idx: 5000 Loss: 0.021368487870388253
Epoch: 19 Idx: 0 Loss: 0.025145716345736386
Epoch: 19 Idx: 5000 Loss: 0.013781751685145246
Epoch: 20 Idx: 0 Loss: 0.0065552930424740315
Epoch: 20 Idx: 5000 Loss: 0.01571014129847999
Epoch: 21 Idx: 0 Loss: 0.02141226908621703
Epoch: 21 Idx: 5000 Loss: 0.02300664285876393
Epoch: 22 Idx: 0 Loss: 0.018671864194501866
Epoch: 22 Idx: 5000 Loss: 0.008458694221051378
Epoch: 23 Idx: 0 Loss: 0.013898800414448588
Epoch: 23 Idx: 5000 Loss: 0.028658399234419398
Epoch: 24 Idx: 0 Loss: 0.019613890656317298
Epoch: 24 Idx: 5000 Loss: 0.011502139654513759
Epoch: 25 Idx: 0 Loss: 0.02260756686482057
Epoch: 25 Idx: 5000 Loss: 0.019050523127102325
Epoch: 26 Idx: 0 Loss: 0.009882110843067435
Epoch: 26 Idx: 5000 Loss: 0.011058209478454609
Epoch: 27 Idx: 0 Loss: 0.0074124909258314765
Epoch: 27 Idx: 5000 Loss: 0.010695460140854674
Epoch: 28 Idx: 0 Loss: 0.01819608415312625
Epoch: 28 Idx: 5000 Loss: 0.019528925578851584
Epoch: 29 Idx: 0 Loss: 0.018685199086018567
Epoch: 29 Idx: 5000 Loss: 0.03478177973232946
Epoch: 30 Idx: 0 Loss: 0.01693828218141559
Epoch: 30 Idx: 5000 Loss: 0.02088687434002172
Epoch: 31 Idx: 0 Loss: 0.010466270161319454
Epoch: 31 Idx: 5000 Loss: 0.008294590557922324
Epoch: 32 Idx: 0 Loss: 0.007019447572530086
Epoch: 32 Idx: 5000 Loss: 0.011411227029366812
Epoch: 33 Idx: 0 Loss: 0.010246079293860158
Epoch: 33 Idx: 5000 Loss: 0.014413704654845424
Epoch: 34 Idx: 0 Loss: 0.01403041610744837
Epoch: 34 Idx: 5000 Loss: 0.009883696521962693
Epoch: 35 Idx: 0 Loss: 0.007267908697730883
Epoch: 35 Idx: 5000 Loss: 0.007482022142219101
Epoch: 36 Idx: 0 Loss: 0.03904775157283176
Epoch: 36 Idx: 5000 Loss: 0.014641462373856383
Epoch: 37 Idx: 0 Loss: 0.007786492183267558
Epoch: 37 Idx: 5000 Loss: 0.05094653416855332
Epoch: 38 Idx: 0 Loss: 0.010726222930216227
Epoch: 38 Idx: 5000 Loss: 0.011713880751928682
Epoch: 39 Idx: 0 Loss: 0.010486284805268425
Epoch: 39 Idx: 5000 Loss: 0.018251560082068956
Epoch: 40 Idx: 0 Loss: 0.011992938957726004
Epoch: 40 Idx: 5000 Loss: 0.015251658366831343
Epoch: 41 Idx: 0 Loss: 0.01976254894766307
Epoch: 41 Idx: 5000 Loss: 0.010828329234227881
Epoch: 42 Idx: 0 Loss: 0.006979589714268595
Epoch: 42 Idx: 5000 Loss: 0.016325577602380497
Epoch: 43 Idx: 0 Loss: 0.01785584528577069
Epoch: 43 Idx: 5000 Loss: 0.00760260799063762
Epoch: 44 Idx: 0 Loss: 0.014004073719963475
Epoch: 44 Idx: 5000 Loss: 0.026822989160083237
Epoch: 45 Idx: 0 Loss: 0.013153078191453239
Epoch: 45 Idx: 5000 Loss: 0.009314487168729717
Epoch: 46 Idx: 0 Loss: 0.02132563203991214
Epoch: 46 Idx: 5000 Loss: 0.007653407423531579
Epoch: 47 Idx: 0 Loss: 0.014105623077542253
Epoch: 47 Idx: 5000 Loss: 0.014756466885896033
Epoch: 48 Idx: 0 Loss: 0.007488736411012123
Epoch: 48 Idx: 5000 Loss: 0.008990171945176373
Epoch: 49 Idx: 0 Loss: 0.013541523332345384
Epoch: 49 Idx: 5000 Loss: 0.012684939618693186
Len (direct inputs):  1737
Inputs len 1372 12 2352
Len (direct inputs):  992
Starting sliding window evaluation...
Step 4/7
Val onto:  [('ekaw', 'sigkdd')] test_onto:  [('cmt', 'conference')]
Training size: 111356 Testing size: 4145
Epoch: 0 Idx: 0 Loss: 0.14221188188940978
Epoch: 0 Idx: 5000 Loss: 0.035502741275227526
Epoch: 1 Idx: 0 Loss: 0.01828957263619876
Epoch: 1 Idx: 5000 Loss: 0.029530443916781873
Epoch: 2 Idx: 0 Loss: 0.019534469147024693
Epoch: 2 Idx: 5000 Loss: 0.015627796635510328
Epoch: 3 Idx: 0 Loss: 0.022970714995026863
Epoch: 3 Idx: 5000 Loss: 0.01217032303680605
Epoch: 4 Idx: 0 Loss: 0.02830971078052247
Epoch: 4 Idx: 5000 Loss: 0.017869242911253386
Epoch: 5 Idx: 0 Loss: 0.009714659868080535
Epoch: 5 Idx: 5000 Loss: 0.017215876703140363
Epoch: 6 Idx: 0 Loss: 0.026385648215910985
Epoch: 6 Idx: 5000 Loss: 0.008078211819916891
Epoch: 7 Idx: 0 Loss: 0.011358362059781538
Epoch: 7 Idx: 5000 Loss: 0.016033641776241035
Epoch: 8 Idx: 0 Loss: 0.00668338024875915
Epoch: 8 Idx: 5000 Loss: 0.018906205606121405
Epoch: 9 Idx: 0 Loss: 0.010060746531158831
Epoch: 9 Idx: 5000 Loss: 0.008246959793353416
Epoch: 10 Idx: 0 Loss: 0.008929348094971016
Epoch: 10 Idx: 5000 Loss: 0.029457486375903667
Epoch: 11 Idx: 0 Loss: 0.020436641323025702
Epoch: 11 Idx: 5000 Loss: 0.013831479344612353
Epoch: 12 Idx: 0 Loss: 0.01509658630057626
Epoch: 12 Idx: 5000 Loss: 0.017182739607140465
Epoch: 13 Idx: 0 Loss: 0.014397197267489486
Epoch: 13 Idx: 5000 Loss: 0.007431369584912689
Epoch: 14 Idx: 0 Loss: 0.04539249938111555
Epoch: 14 Idx: 5000 Loss: 0.04273857363341192
Epoch: 15 Idx: 0 Loss: 0.004647063748145036
Epoch: 15 Idx: 5000 Loss: 0.02002929006675701
Epoch: 16 Idx: 0 Loss: 0.008598046683542061
Epoch: 16 Idx: 5000 Loss: 0.022712665424901114
Epoch: 17 Idx: 0 Loss: 0.010405178238596983
Epoch: 17 Idx: 5000 Loss: 0.016241138979708646
Epoch: 18 Idx: 0 Loss: 0.02016645148777753
Epoch: 18 Idx: 5000 Loss: 0.007184219094089957
Epoch: 19 Idx: 0 Loss: 0.012903560627055891
Epoch: 19 Idx: 5000 Loss: 0.015467996045257927
Epoch: 20 Idx: 0 Loss: 0.010885299042818816
Epoch: 20 Idx: 5000 Loss: 0.009239496607732463
Epoch: 21 Idx: 0 Loss: 0.007081785779048345
Epoch: 21 Idx: 5000 Loss: 0.007911612515966306
Epoch: 22 Idx: 0 Loss: 0.020366558608876956
Epoch: 22 Idx: 5000 Loss: 0.016501886646251563
Epoch: 23 Idx: 0 Loss: 0.013423019050053931
Epoch: 23 Idx: 5000 Loss: 0.019212591905230662
Epoch: 24 Idx: 0 Loss: 0.00911914274157058
Epoch: 24 Idx: 5000 Loss: 0.019708125176230537
Epoch: 25 Idx: 0 Loss: 0.02110060695752763
Epoch: 25 Idx: 5000 Loss: 0.015701737549540403
Epoch: 26 Idx: 0 Loss: 0.008178131691691468
Epoch: 26 Idx: 5000 Loss: 0.00555534493839532
Epoch: 27 Idx: 0 Loss: 0.022209059251468703
Epoch: 27 Idx: 5000 Loss: 0.00968405237984476
Epoch: 28 Idx: 0 Loss: 0.013806991464603785
Epoch: 28 Idx: 5000 Loss: 0.01189947420931611
Epoch: 29 Idx: 0 Loss: 0.017135069142775274
Epoch: 29 Idx: 5000 Loss: 0.022150951699232247
Epoch: 30 Idx: 0 Loss: 0.010118702484142605
Epoch: 30 Idx: 5000 Loss: 0.01048902932197277
Epoch: 31 Idx: 0 Loss: 0.027005926272720856
Epoch: 31 Idx: 5000 Loss: 0.02191833755273253
Epoch: 32 Idx: 0 Loss: 0.012766141591904591
Epoch: 32 Idx: 5000 Loss: 0.018503954441812822
Epoch: 33 Idx: 0 Loss: 0.0037988012302594644
Epoch: 33 Idx: 5000 Loss: 0.0108738748144206
Epoch: 34 Idx: 0 Loss: 0.012347743341784151
Epoch: 34 Idx: 5000 Loss: 0.01016743995945808
Epoch: 35 Idx: 0 Loss: 0.019833156520161956
Epoch: 35 Idx: 5000 Loss: 0.016293248858766367
Epoch: 36 Idx: 0 Loss: 0.005731994754245493
Epoch: 36 Idx: 5000 Loss: 0.009738461138393316
Epoch: 37 Idx: 0 Loss: 0.020064479051849462
Epoch: 37 Idx: 5000 Loss: 0.014309934597707937
Epoch: 38 Idx: 0 Loss: 0.010357569634119703
Epoch: 38 Idx: 5000 Loss: 0.014777138131361292
Epoch: 39 Idx: 0 Loss: 0.012616527242420834
Epoch: 39 Idx: 5000 Loss: 0.023589029888532304
Epoch: 40 Idx: 0 Loss: 0.011907177233758108
Epoch: 40 Idx: 5000 Loss: 0.0091179680475206
Epoch: 41 Idx: 0 Loss: 0.0046570091081386085
Epoch: 41 Idx: 5000 Loss: 0.013280250673516332
Epoch: 42 Idx: 0 Loss: 0.022298880599922025
Epoch: 42 Idx: 5000 Loss: 0.011818159566693185
Epoch: 43 Idx: 0 Loss: 0.011372927770195733
Epoch: 43 Idx: 5000 Loss: 0.013257891025843117
Epoch: 44 Idx: 0 Loss: 0.020553021355407233
Epoch: 44 Idx: 5000 Loss: 0.013664105657015535
Epoch: 45 Idx: 0 Loss: 0.015838234065349205
Epoch: 45 Idx: 5000 Loss: 0.010077901059137844
Epoch: 46 Idx: 0 Loss: 0.019883421984276353
Epoch: 46 Idx: 5000 Loss: 0.015315539757261236
Epoch: 47 Idx: 0 Loss: 0.012332705936627681
Epoch: 47 Idx: 5000 Loss: 0.009460235889701064
Epoch: 48 Idx: 0 Loss: 0.03788104906041807
Epoch: 48 Idx: 5000 Loss: 0.00916154267511878
Epoch: 49 Idx: 0 Loss: 0.01702123402441285
Epoch: 49 Idx: 5000 Loss: 0.0022734725135104966
Len (direct inputs):  561
Inputs len 1568 15 4130
Len (direct inputs):  2577
Starting sliding window evaluation...
Step 6/7
Val onto:  [('edas', 'sigkdd')] test_onto:  [('conference', 'edas')]
Training size: 106045 Testing size: 7817
Epoch: 0 Idx: 0 Loss: 0.21441950352095787
Epoch: 0 Idx: 5000 Loss: 0.033021581093684776
Epoch: 1 Idx: 0 Loss: 0.01712011550218839
Epoch: 1 Idx: 5000 Loss: 0.015255940256448358
Epoch: 2 Idx: 0 Loss: 0.011992022596159859
Epoch: 2 Idx: 5000 Loss: 0.017562525137329695
Epoch: 3 Idx: 0 Loss: 0.014002435927848297
Epoch: 3 Idx: 5000 Loss: 0.03188123622240768
Epoch: 4 Idx: 0 Loss: 0.018357474245308335
Epoch: 4 Idx: 5000 Loss: 0.02149189567878322
Epoch: 5 Idx: 0 Loss: 0.020115360281276307
Epoch: 5 Idx: 5000 Loss: 0.015851015252355816
Epoch: 6 Idx: 0 Loss: 0.012388180123538664
Epoch: 6 Idx: 5000 Loss: 0.02674925969386374
Epoch: 7 Idx: 0 Loss: 0.021770408762388515
Epoch: 7 Idx: 5000 Loss: 0.015172173993076476
Epoch: 8 Idx: 0 Loss: 0.007370512174387039
Epoch: 8 Idx: 5000 Loss: 0.00658298931502511
Epoch: 9 Idx: 0 Loss: 0.013831143691003658
Epoch: 9 Idx: 5000 Loss: 0.011746748192152743
Epoch: 10 Idx: 0 Loss: 0.005906570503482574
Epoch: 10 Idx: 5000 Loss: 0.01288334656841114
Epoch: 11 Idx: 0 Loss: 0.011501091467428084
Epoch: 11 Idx: 5000 Loss: 0.041626389841780936
Epoch: 12 Idx: 0 Loss: 0.012248792772727418
Epoch: 12 Idx: 5000 Loss: 0.02239701008819832
Epoch: 13 Idx: 0 Loss: 0.0187009555832932
Epoch: 13 Idx: 5000 Loss: 0.018032567526888327
Epoch: 14 Idx: 0 Loss: 0.007431326523855602
Epoch: 14 Idx: 5000 Loss: 0.019282465198524168
Epoch: 15 Idx: 0 Loss: 0.02658912714641805
Epoch: 15 Idx: 5000 Loss: 0.009785705863595694
Epoch: 16 Idx: 0 Loss: 0.013887022474535967
Epoch: 16 Idx: 5000 Loss: 0.006993112580018925
Epoch: 17 Idx: 0 Loss: 0.016710073516114483
Epoch: 17 Idx: 5000 Loss: 0.021148938759264803
Epoch: 18 Idx: 0 Loss: 0.0072288817123245955
Epoch: 18 Idx: 5000 Loss: 0.027026824025304295
Epoch: 19 Idx: 0 Loss: 0.015206131766166694
Epoch: 19 Idx: 5000 Loss: 0.011231258184165895
Epoch: 20 Idx: 0 Loss: 0.011852702414074014
Epoch: 20 Idx: 5000 Loss: 0.00871824291956215
Epoch: 21 Idx: 0 Loss: 0.009701447323498126
Epoch: 21 Idx: 5000 Loss: 0.024202066948813817
Epoch: 22 Idx: 0 Loss: 0.02339604264322539
Epoch: 22 Idx: 5000 Loss: 0.03235853401283111
Epoch: 23 Idx: 0 Loss: 0.02106878960342783
Epoch: 23 Idx: 5000 Loss: 0.023454188508164178
Epoch: 24 Idx: 0 Loss: 0.010335290655425497
Epoch: 24 Idx: 5000 Loss: 0.017165238626304494
Epoch: 25 Idx: 0 Loss: 0.019906690833291142
Epoch: 25 Idx: 5000 Loss: 0.01431746585796998
Epoch: 26 Idx: 0 Loss: 0.01658867443717616
Epoch: 26 Idx: 5000 Loss: 0.01775631643594208
Epoch: 27 Idx: 0 Loss: 0.028355899981996315
Epoch: 27 Idx: 5000 Loss: 0.015004220335300428
Epoch: 28 Idx: 0 Loss: 0.012540797302516
Epoch: 28 Idx: 5000 Loss: 0.03898812992301259
Epoch: 29 Idx: 0 Loss: 0.005507794851113088
Epoch: 29 Idx: 5000 Loss: 0.014209867368154895
Epoch: 30 Idx: 0 Loss: 0.007146062919054559
Epoch: 30 Idx: 5000 Loss: 0.014874552377271921
Epoch: 31 Idx: 0 Loss: 0.015340234669286981
Epoch: 31 Idx: 5000 Loss: 0.032444135496437675
Epoch: 32 Idx: 0 Loss: 0.041208275239612194
Epoch: 32 Idx: 5000 Loss: 0.020330699246198528
Epoch: 33 Idx: 0 Loss: 0.02264818428348913
Epoch: 33 Idx: 5000 Loss: 0.01659373789911578
Epoch: 34 Idx: 0 Loss: 0.01106950791789701
Epoch: 34 Idx: 5000 Loss: 0.041306169449592145
Epoch: 35 Idx: 0 Loss: 0.018852737233871514
Epoch: 35 Idx: 5000 Loss: 0.025746828194932642
Epoch: 36 Idx: 0 Loss: 0.02226155163378332
Epoch: 36 Idx: 5000 Loss: 0.008404582908406647
Epoch: 37 Idx: 0 Loss: 0.015574406955102614
Epoch: 37 Idx: 5000 Loss: 0.016505124180789123
Epoch: 38 Idx: 0 Loss: 0.015588691122720199
Epoch: 38 Idx: 5000 Loss: 0.02070749167611093
Epoch: 39 Idx: 0 Loss: 0.013721741997110543
Epoch: 39 Idx: 5000 Loss: 0.03031564109722729
Epoch: 40 Idx: 0 Loss: 0.01759950386795243
Epoch: 40 Idx: 5000 Loss: 0.028767002703703443
Epoch: 41 Idx: 0 Loss: 0.010612013598731509
Epoch: 41 Idx: 5000 Loss: 0.015399192262984227
Epoch: 42 Idx: 0 Loss: 0.006384992942821839
Epoch: 42 Idx: 5000 Loss: 0.03209207536412046
Epoch: 43 Idx: 0 Loss: 0.012558160511576273
Epoch: 43 Idx: 5000 Loss: 0.05133085240518978
Epoch: 44 Idx: 0 Loss: 0.02940469364006321
Epoch: 44 Idx: 5000 Loss: 0.011739034291236792
Epoch: 45 Idx: 0 Loss: 0.021459616170853943
Epoch: 45 Idx: 5000 Loss: 0.028670193123321443
Epoch: 46 Idx: 0 Loss: 0.028325434009721304
Traceback (most recent call last):
  File "main.py", line 518, in <module>
    optimizer.step()
  File "/u/harshitk/miniconda3/envs/py37/lib/python3.7/site-packages/torch/autograd/grad_mode.py", line 15, in decorate_context
    return func(*args, **kwargs)
  File "/u/harshitk/miniconda3/envs/py37/lib/python3.7/site-packages/torch/optim/adam.py", line 100, in step
    exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)
KeyboardInterrupt

------------------------------------------------------------
Sender: LSF System <rer@dccxc215>
Subject: Job 4066829: <python main.py 4 9 False True> in cluster <dcc> Exited

Job <python main.py 4 9 False True> was submitted from host <dccxl004> by user <harshitk> in cluster <dcc> at Tue Sep 15 15:48:37 2020
Job was executed on host(s) <dccxc215>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep 15 15:48:38 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/VeeAlign/src> was used as the working directory.
Started at Tue Sep 15 15:48:38 2020
Terminated at Wed Sep 16 04:38:42 2020
Results reported at Wed Sep 16 04:38:42 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python main.py 4 9 False True
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   46129.86 sec.
    Max Memory :                                 2916 MB
    Average Memory :                             2732.26 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40501.00 MB
    Max Swap :                                   2 MB
    Max Processes :                              3
    Max Threads :                                13
    Run time :                                   46224 sec.
    Turnaround time :                            46205 sec.

The output (if any) is above this job summary.

