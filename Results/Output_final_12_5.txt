2020-09-15 15:48:44.646262: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-09-15 15:48:51.781191: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1
2020-09-15 15:48:51.893133: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:1b:00.0 name: Tesla K80 computeCapability: 3.7
coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s
2020-09-15 15:48:51.893217: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-09-15 15:48:51.895470: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
2020-09-15 15:48:51.956409: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2020-09-15 15:48:52.076433: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2020-09-15 15:48:52.123488: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2020-09-15 15:48:52.170056: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10
2020-09-15 15:48:52.170582: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudnn.so.7'; dlerror: libcudnn.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/share/lsf-9.1.3/10.1/linux3.10-glibc2.17-x86_64/lib:=/opt/share/gcc-4.9.2_rhel6/x86_64/lib/:/opt/share/gcc-4.9.2_rhel6/x86_64/lib64:/opt/share/Python-3.6.2/x86_64/lib:=/opt/share/gcc-5.4.0/x86_64/lib/:/opt/share/gcc-5.4.0/x86_64/lib64:/opt/share/isl-0.17/x86_64/lib/:/opt/share/protobuf-3.1.0/x86_64/lib/:/opt/share/leveldb-1.19/x86_64/lib/:/opt/share/boost-1.62.0/x86_64/lib/:/opt/share/torch-7/x86_64/install/lib:/opt/share/Python-2.7.12/x86_64/lib:/opt/share/Python-3.5.2/x86_64/lib:/opt/share/cuDNN-v5.1-8.0/cuda/lib64:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/opt/share/cuda-8.0/
2020-09-15 15:48:52.170607: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1753] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2020-09-15 15:48:52.171096: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-09-15 15:48:52.207648: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2600135000 Hz
2020-09-15 15:48:52.207923: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55d900d5d750 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-09-15 15:48:52.207947: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-09-15 15:48:52.211116: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-09-15 15:48:52.211152: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      
Prefix path:  /dccstor/cogfin/arvind/da/VeeAlign/
Ontologies being aligned are:  [('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/confOf.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/conference.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/conference.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/conference.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/conference.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/confOf.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/conference.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/confOf.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/confOf.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/confOf.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl')]
Total number of extracted unique classes and properties from entire RA set:  829
Constructing abbrevation resolution dict....
Results after abbreviation resolution:  {'PC': 'Program Committee', 'OC': 'Organizing Committee'}
Resolving abbreviations...
Number of entities: 119639
Starting sliding window evaluation...
Step 0/7
Val onto:  [('confof', 'ekaw')] test_onto:  [('conference', 'sigkdd')]
Training size: 112565 Testing size: 3871
Epoch: 0 Idx: 0 Loss: 0.18981385443522242
Epoch: 0 Idx: 5000 Loss: 0.006747135980118914
Epoch: 1 Idx: 0 Loss: 0.01064607441054587
Epoch: 1 Idx: 5000 Loss: 0.01661137922102123
Epoch: 2 Idx: 0 Loss: 0.005420845310645906
Epoch: 2 Idx: 5000 Loss: 0.024209592038372245
Epoch: 3 Idx: 0 Loss: 0.039114601549059005
Epoch: 3 Idx: 5000 Loss: 0.03722088508030944
Epoch: 4 Idx: 0 Loss: 0.024060680848861277
Epoch: 4 Idx: 5000 Loss: 0.009110609908416525
Epoch: 5 Idx: 0 Loss: 0.03723230082403981
Epoch: 5 Idx: 5000 Loss: 0.011793211863365592
Epoch: 6 Idx: 0 Loss: 0.012286783730551896
Epoch: 6 Idx: 5000 Loss: 0.02042990898976097
Epoch: 7 Idx: 0 Loss: 0.03186029874600101
Epoch: 7 Idx: 5000 Loss: 0.006946542034009579
Epoch: 8 Idx: 0 Loss: 0.031764344433711275
Epoch: 8 Idx: 5000 Loss: 0.011578585830398955
Epoch: 9 Idx: 0 Loss: 0.014481568057949644
Epoch: 9 Idx: 5000 Loss: 0.015919376115078807
Epoch: 10 Idx: 0 Loss: 0.011075956402437075
Epoch: 10 Idx: 5000 Loss: 0.012079120454861958
Epoch: 11 Idx: 0 Loss: 0.023134856071527064
Epoch: 11 Idx: 5000 Loss: 0.01286891836985108
Epoch: 12 Idx: 0 Loss: 0.0053791637884263895
Epoch: 12 Idx: 5000 Loss: 0.029513591670737308
Epoch: 13 Idx: 0 Loss: 0.00918748946389019
Epoch: 13 Idx: 5000 Loss: 0.0074796851698302885
Epoch: 14 Idx: 0 Loss: 0.015317638655444592
Epoch: 14 Idx: 5000 Loss: 0.008564154453849322
Epoch: 15 Idx: 0 Loss: 0.02019818053143831
Epoch: 15 Idx: 5000 Loss: 0.009215743188865574
Epoch: 16 Idx: 0 Loss: 0.03491128349445056
Epoch: 16 Idx: 5000 Loss: 0.022088239084319894
Epoch: 17 Idx: 0 Loss: 0.02178616124898086
Epoch: 17 Idx: 5000 Loss: 0.013726218845526043
Epoch: 18 Idx: 0 Loss: 0.03522981660713066
Epoch: 18 Idx: 5000 Loss: 0.01459039562157636
Epoch: 19 Idx: 0 Loss: 0.047147828813679744
Epoch: 19 Idx: 5000 Loss: 0.04833384715882988
Epoch: 20 Idx: 0 Loss: 0.004623011163103066
Epoch: 20 Idx: 5000 Loss: 0.012606766737912277
Epoch: 21 Idx: 0 Loss: 0.008617544056009731
Epoch: 21 Idx: 5000 Loss: 0.012386445421472079
Epoch: 22 Idx: 0 Loss: 0.012034590094232559
Epoch: 22 Idx: 5000 Loss: 0.01716934602024319
Epoch: 23 Idx: 0 Loss: 0.008842298980503307
Epoch: 23 Idx: 5000 Loss: 0.012449159995073043
Epoch: 24 Idx: 0 Loss: 0.011039045642135595
Epoch: 24 Idx: 5000 Loss: 0.017705198845820228
Epoch: 25 Idx: 0 Loss: 0.010871605239800759
Epoch: 25 Idx: 5000 Loss: 0.023763843315914853
Epoch: 26 Idx: 0 Loss: 0.010024763686996523
Epoch: 26 Idx: 5000 Loss: 0.013332586042369317
Epoch: 27 Idx: 0 Loss: 0.006151298512136177
Epoch: 27 Idx: 5000 Loss: 0.018714515706270536
Epoch: 28 Idx: 0 Loss: 0.03435255500599437
Epoch: 28 Idx: 5000 Loss: 0.033939073490473634
Epoch: 29 Idx: 0 Loss: 0.009265324961816156
Epoch: 29 Idx: 5000 Loss: 0.01177716687939022
Epoch: 30 Idx: 0 Loss: 0.01177627765166126
Epoch: 30 Idx: 5000 Loss: 0.0036285485196646153
Epoch: 31 Idx: 0 Loss: 0.029391490497043433
Epoch: 31 Idx: 5000 Loss: 0.005545187555791711
Epoch: 32 Idx: 0 Loss: 0.0238344783907151
Epoch: 32 Idx: 5000 Loss: 0.01295272361314688
Epoch: 33 Idx: 0 Loss: 0.015134456987629167
Epoch: 33 Idx: 5000 Loss: 0.013217513345663835
Epoch: 34 Idx: 0 Loss: 0.02832152906763164
Epoch: 34 Idx: 5000 Loss: 0.013176518475868282
Epoch: 35 Idx: 0 Loss: 0.016680672677938616
Epoch: 35 Idx: 5000 Loss: 0.01231260616474406
Epoch: 36 Idx: 0 Loss: 0.026676061964759953
Epoch: 36 Idx: 5000 Loss: 0.025710928509308374
Epoch: 37 Idx: 0 Loss: 0.013628039029666793
Epoch: 37 Idx: 5000 Loss: 0.013806227471904714
Epoch: 38 Idx: 0 Loss: 0.01568702333923208
Epoch: 38 Idx: 5000 Loss: 0.02336264804764807
Epoch: 39 Idx: 0 Loss: 0.013387798657877703
Epoch: 39 Idx: 5000 Loss: 0.01821609564711194
Epoch: 40 Idx: 0 Loss: 0.011328788462740758
Epoch: 40 Idx: 5000 Loss: 0.027307852982347544
Epoch: 41 Idx: 0 Loss: 0.013310829613686386
Epoch: 41 Idx: 5000 Loss: 0.0187869154656151
Epoch: 42 Idx: 0 Loss: 0.008377614381198703
Epoch: 42 Idx: 5000 Loss: 0.011534959731457316
Epoch: 43 Idx: 0 Loss: 0.014122269094368381
Epoch: 43 Idx: 5000 Loss: 0.04929513255313856
Epoch: 44 Idx: 0 Loss: 0.016977196131527685
Epoch: 44 Idx: 5000 Loss: 0.013639003267937645
Epoch: 45 Idx: 0 Loss: 0.011742923926420883
Epoch: 45 Idx: 5000 Loss: 0.01300263251061489
Epoch: 46 Idx: 0 Loss: 0.027119451003113214
Epoch: 46 Idx: 5000 Loss: 0.031551841455800325
Epoch: 47 Idx: 0 Loss: 0.007965454311827744
Epoch: 47 Idx: 5000 Loss: 0.022580859418052997
Epoch: 48 Idx: 0 Loss: 0.024800560641889326
Epoch: 48 Idx: 5000 Loss: 0.014349125819813704
Epoch: 49 Idx: 0 Loss: 0.018258020120388112
Epoch: 49 Idx: 5000 Loss: 0.01969864498194667
Len (direct inputs):  429
Inputs len 2744 15 3856
Len (direct inputs):  1127
Starting sliding window evaluation...
Step 2/7
Val onto:  [('conference', 'ekaw')] test_onto:  [('cmt', 'sigkdd')]
Training size: 111450 Testing size: 2364
Epoch: 0 Idx: 0 Loss: 0.15267345883926217
Epoch: 0 Idx: 5000 Loss: 0.027220093116006292
Epoch: 1 Idx: 0 Loss: 0.013832532599251202
Epoch: 1 Idx: 5000 Loss: 0.01045061053768365
Epoch: 2 Idx: 0 Loss: 0.012290793898375042
Epoch: 2 Idx: 5000 Loss: 0.00971666968319299
Epoch: 3 Idx: 0 Loss: 0.02248685655565869
Epoch: 3 Idx: 5000 Loss: 0.008968453489115508
Epoch: 4 Idx: 0 Loss: 0.01494155094336614
Epoch: 4 Idx: 5000 Loss: 0.011603587389575766
Epoch: 5 Idx: 0 Loss: 0.018598426976813014
Epoch: 5 Idx: 5000 Loss: 0.034061864696719915
Epoch: 6 Idx: 0 Loss: 0.014047154257226677
Epoch: 6 Idx: 5000 Loss: 0.0565006845601734
Epoch: 7 Idx: 0 Loss: 0.009783599668787692
Epoch: 7 Idx: 5000 Loss: 0.00929485780365552
Epoch: 8 Idx: 0 Loss: 0.0074532727431186555
Epoch: 8 Idx: 5000 Loss: 0.016347198601771695
Epoch: 9 Idx: 0 Loss: 0.020774948932185136
Epoch: 9 Idx: 5000 Loss: 0.02633082296470069
Epoch: 10 Idx: 0 Loss: 0.006392488400838
Epoch: 10 Idx: 5000 Loss: 0.010689392737444382
Epoch: 11 Idx: 0 Loss: 0.009636768641697682
Epoch: 11 Idx: 5000 Loss: 0.008879094408080665
Epoch: 12 Idx: 0 Loss: 0.006403922357449842
Epoch: 12 Idx: 5000 Loss: 0.010544289239090203
Epoch: 13 Idx: 0 Loss: 0.010309625866210488
Epoch: 13 Idx: 5000 Loss: 0.0124557301109261
Epoch: 14 Idx: 0 Loss: 0.014480348383791013
Epoch: 14 Idx: 5000 Loss: 0.03502285855441321
Epoch: 15 Idx: 0 Loss: 0.01131144847120094
Epoch: 15 Idx: 5000 Loss: 0.01810585723923799
Epoch: 16 Idx: 0 Loss: 0.027746005025219993
Epoch: 16 Idx: 5000 Loss: 0.016797646569375314
Epoch: 17 Idx: 0 Loss: 0.03085753841752304
Epoch: 17 Idx: 5000 Loss: 0.01884212048402533
Epoch: 18 Idx: 0 Loss: 0.010618336554585703
Epoch: 18 Idx: 5000 Loss: 0.02230320491760753
Epoch: 19 Idx: 0 Loss: 0.01726631093417733
Epoch: 19 Idx: 5000 Loss: 0.013431521252241216
Epoch: 20 Idx: 0 Loss: 0.010993520302430768
Epoch: 20 Idx: 5000 Loss: 0.012625079577739996
Epoch: 21 Idx: 0 Loss: 0.015245656824104172
Epoch: 21 Idx: 5000 Loss: 0.029632679866080217
Epoch: 22 Idx: 0 Loss: 0.010379483772554909
Epoch: 22 Idx: 5000 Loss: 0.009814321643350069
Epoch: 23 Idx: 0 Loss: 0.008236412462873618
Epoch: 23 Idx: 5000 Loss: 0.00836779825463443
Epoch: 24 Idx: 0 Loss: 0.010105848797785724
Epoch: 24 Idx: 5000 Loss: 0.014172294056831463
Epoch: 25 Idx: 0 Loss: 0.014335390587686516
Epoch: 25 Idx: 5000 Loss: 0.021820286339062293
Epoch: 26 Idx: 0 Loss: 0.01619736631478941
Epoch: 26 Idx: 5000 Loss: 0.015479372341238131
Epoch: 27 Idx: 0 Loss: 0.0271828967239462
Epoch: 27 Idx: 5000 Loss: 0.016243643090721023
Epoch: 28 Idx: 0 Loss: 0.023947423679332162
Epoch: 28 Idx: 5000 Loss: 0.008940823141248262
Epoch: 29 Idx: 0 Loss: 0.031264217629998724
Epoch: 29 Idx: 5000 Loss: 0.016993885850952524
Epoch: 30 Idx: 0 Loss: 0.011181017529688909
Epoch: 30 Idx: 5000 Loss: 0.023048700445799687
Epoch: 31 Idx: 0 Loss: 0.02099764133102066
Epoch: 31 Idx: 5000 Loss: 0.015793460851867343
Epoch: 32 Idx: 0 Loss: 0.014030319808001367
Epoch: 32 Idx: 5000 Loss: 0.021355263066045273
Epoch: 33 Idx: 0 Loss: 0.010049615427684594
Epoch: 33 Idx: 5000 Loss: 0.008121341923415312
Epoch: 34 Idx: 0 Loss: 0.012561258545828949
Epoch: 34 Idx: 5000 Loss: 0.01199405543031073
Epoch: 35 Idx: 0 Loss: 0.023559574485020898
Epoch: 35 Idx: 5000 Loss: 0.013405530361485701
Epoch: 36 Idx: 0 Loss: 0.03801520085447563
Epoch: 36 Idx: 5000 Loss: 0.009165738906263033
Epoch: 37 Idx: 0 Loss: 0.007932397399561981
Epoch: 37 Idx: 5000 Loss: 0.012032980128366708
Epoch: 38 Idx: 0 Loss: 0.006709664891036804
Epoch: 38 Idx: 5000 Loss: 0.009057041511346396
Epoch: 39 Idx: 0 Loss: 0.009259545156752372
Epoch: 39 Idx: 5000 Loss: 0.013902712041707128
Epoch: 40 Idx: 0 Loss: 0.01573545161246919
Epoch: 40 Idx: 5000 Loss: 0.015872422767610892
Epoch: 41 Idx: 0 Loss: 0.013638900382020777
Epoch: 41 Idx: 5000 Loss: 0.008856353329803616
Epoch: 42 Idx: 0 Loss: 0.013205577434871415
Epoch: 42 Idx: 5000 Loss: 0.027955672470334838
Epoch: 43 Idx: 0 Loss: 0.008893377166083315
Epoch: 43 Idx: 5000 Loss: 0.01272908106310815
Epoch: 44 Idx: 0 Loss: 0.025744075504880996
Epoch: 44 Idx: 5000 Loss: 0.009978835512634213
Epoch: 45 Idx: 0 Loss: 0.007322037540425183
Epoch: 45 Idx: 5000 Loss: 0.011264621130500672
Epoch: 46 Idx: 0 Loss: 0.009663235882305814
Epoch: 46 Idx: 5000 Loss: 0.015205730475681868
Epoch: 47 Idx: 0 Loss: 0.006221394577346234
Epoch: 47 Idx: 5000 Loss: 0.014830134491225572
Epoch: 48 Idx: 0 Loss: 0.012676824986521748
Epoch: 48 Idx: 5000 Loss: 0.012029524592276826
Epoch: 49 Idx: 0 Loss: 0.007919645491771245
Epoch: 49 Idx: 5000 Loss: 0.011435438903978325
Len (direct inputs):  1737
Inputs len 1372 12 2352
Len (direct inputs):  992
Starting sliding window evaluation...
Step 4/7
Val onto:  [('ekaw', 'sigkdd')] test_onto:  [('cmt', 'conference')]
Training size: 111356 Testing size: 4145
Epoch: 0 Idx: 0 Loss: 0.13538954477776569
Epoch: 0 Idx: 5000 Loss: 0.010961063313925209
Epoch: 1 Idx: 0 Loss: 0.021203803167789338
Epoch: 1 Idx: 5000 Loss: 0.008513301664855453
Epoch: 2 Idx: 0 Loss: 0.0146168546514474
Epoch: 2 Idx: 5000 Loss: 0.03476314723315292
Epoch: 3 Idx: 0 Loss: 0.04680336227284892
Epoch: 3 Idx: 5000 Loss: 0.03363171249772348
Epoch: 4 Idx: 0 Loss: 0.03287824603428938
Epoch: 4 Idx: 5000 Loss: 0.01753457769707445
Epoch: 5 Idx: 0 Loss: 0.01675577612562397
Epoch: 5 Idx: 5000 Loss: 0.01726102080845747
Epoch: 6 Idx: 0 Loss: 0.01819084627372935
Epoch: 6 Idx: 5000 Loss: 0.012683129217140167
Epoch: 7 Idx: 0 Loss: 0.016725877239967046
Epoch: 7 Idx: 5000 Loss: 0.03263657139277775
Epoch: 8 Idx: 0 Loss: 0.010493951856876542
Epoch: 8 Idx: 5000 Loss: 0.015252735505639744
Epoch: 9 Idx: 0 Loss: 0.023588588384272316
Epoch: 9 Idx: 5000 Loss: 0.009665798497340812
Epoch: 10 Idx: 0 Loss: 0.008914749356824764
Epoch: 10 Idx: 5000 Loss: 0.023906968273964074
Epoch: 11 Idx: 0 Loss: 0.007786977121460856
Epoch: 11 Idx: 5000 Loss: 0.007155256960159551
Epoch: 12 Idx: 0 Loss: 0.012651582050070156
Epoch: 12 Idx: 5000 Loss: 0.006500710673102228
Epoch: 13 Idx: 0 Loss: 0.030917580376712425
Epoch: 13 Idx: 5000 Loss: 0.011488879865335816
Epoch: 14 Idx: 0 Loss: 0.01284048471183587
Epoch: 14 Idx: 5000 Loss: 0.008541227681882058
Epoch: 15 Idx: 0 Loss: 0.012482915035487336
Epoch: 15 Idx: 5000 Loss: 0.021287321034370175
Epoch: 16 Idx: 0 Loss: 0.01106449260614232
Epoch: 16 Idx: 5000 Loss: 0.012608472148831549
Epoch: 17 Idx: 0 Loss: 0.02574329480193969
Epoch: 17 Idx: 5000 Loss: 0.03200691521574646
Epoch: 18 Idx: 0 Loss: 0.011456618218278303
Epoch: 18 Idx: 5000 Loss: 0.01971667021429811
Epoch: 19 Idx: 0 Loss: 0.040585103543630645
Epoch: 19 Idx: 5000 Loss: 0.006974758605383375
Epoch: 20 Idx: 0 Loss: 0.016781763505602655
Epoch: 20 Idx: 5000 Loss: 0.018508108875938165
Epoch: 21 Idx: 0 Loss: 0.015249916300138954
Epoch: 21 Idx: 5000 Loss: 0.026509982804023773
Epoch: 22 Idx: 0 Loss: 0.03639493822715368
Epoch: 22 Idx: 5000 Loss: 0.011026311215583333
Epoch: 23 Idx: 0 Loss: 0.02081410962143702
Epoch: 23 Idx: 5000 Loss: 0.013339512596035781
Epoch: 24 Idx: 0 Loss: 0.018120271118014716
Epoch: 24 Idx: 5000 Loss: 0.015330555397259237
Epoch: 25 Idx: 0 Loss: 0.010224236470586675
Epoch: 25 Idx: 5000 Loss: 0.01960673168243041
Epoch: 26 Idx: 0 Loss: 0.012538982960445817
Epoch: 26 Idx: 5000 Loss: 0.012227845581665264
Epoch: 27 Idx: 0 Loss: 0.009235128889252773
Epoch: 27 Idx: 5000 Loss: 0.02373693050810828
Epoch: 28 Idx: 0 Loss: 0.013315381647523158
Epoch: 28 Idx: 5000 Loss: 0.010340647036423248
Epoch: 29 Idx: 0 Loss: 0.01601488911284381
Epoch: 29 Idx: 5000 Loss: 0.018268905215398524
Epoch: 30 Idx: 0 Loss: 0.009828756834921706
Epoch: 30 Idx: 5000 Loss: 0.014978839593610063
Epoch: 31 Idx: 0 Loss: 0.019815253359561674
Epoch: 31 Idx: 5000 Loss: 0.02429604773906767
Epoch: 32 Idx: 0 Loss: 0.009177472246302889
Epoch: 32 Idx: 5000 Loss: 0.017480175198803675
Epoch: 33 Idx: 0 Loss: 0.01293934433897795
Epoch: 33 Idx: 5000 Loss: 0.010113907821276961
Epoch: 34 Idx: 0 Loss: 0.011701197026929016
Epoch: 34 Idx: 5000 Loss: 0.00597338935468154
Epoch: 35 Idx: 0 Loss: 0.024671855268205298
Epoch: 35 Idx: 5000 Loss: 0.02095781778758892
Epoch: 36 Idx: 0 Loss: 0.013404457431960332
Epoch: 36 Idx: 5000 Loss: 0.013458725669333146
Epoch: 37 Idx: 0 Loss: 0.0118849731022116
Epoch: 37 Idx: 5000 Loss: 0.04027902209440227
Epoch: 38 Idx: 0 Loss: 0.011615576261629852
Epoch: 38 Idx: 5000 Loss: 0.010008532687239304
Epoch: 39 Idx: 0 Loss: 0.005402550876130091
Epoch: 39 Idx: 5000 Loss: 0.022612723220604595
Epoch: 40 Idx: 0 Loss: 0.016646630752994787
Epoch: 40 Idx: 5000 Loss: 0.01139645531392762
Epoch: 41 Idx: 0 Loss: 0.0216439133963604
Epoch: 41 Idx: 5000 Loss: 0.010225172898836286
Epoch: 42 Idx: 0 Loss: 0.015008026265528392
Epoch: 42 Idx: 5000 Loss: 0.01624290406038285
Epoch: 43 Idx: 0 Loss: 0.004169929626846456
Epoch: 43 Idx: 5000 Loss: 0.007768778523906366
Epoch: 44 Idx: 0 Loss: 0.03342478181691754
Epoch: 44 Idx: 5000 Loss: 0.00930034637546557
Epoch: 45 Idx: 0 Loss: 0.02056049527196745
Epoch: 45 Idx: 5000 Loss: 0.013521069284667094
Epoch: 46 Idx: 0 Loss: 0.018254230292983016
Epoch: 46 Idx: 5000 Loss: 0.020059087020415818
Epoch: 47 Idx: 0 Loss: 0.010675589245193944
Epoch: 47 Idx: 5000 Loss: 0.014861431633715994
Epoch: 48 Idx: 0 Loss: 0.026970354594747656
Epoch: 48 Idx: 5000 Loss: 0.010962237070496409
Epoch: 49 Idx: 0 Loss: 0.00799565587594446
Epoch: 49 Idx: 5000 Loss: 0.008783531692983616
Len (direct inputs):  561
Inputs len 1568 15 4130
Len (direct inputs):  2577
Starting sliding window evaluation...
Step 6/7
Val onto:  [('edas', 'sigkdd')] test_onto:  [('conference', 'edas')]
Training size: 106045 Testing size: 7817
Epoch: 0 Idx: 0 Loss: 0.25232543882226416
Epoch: 0 Idx: 5000 Loss: 0.013781529128065599
Epoch: 1 Idx: 0 Loss: 0.01496899813375573
Epoch: 1 Idx: 5000 Loss: 0.007964580556033692
Epoch: 2 Idx: 0 Loss: 0.013346601046753788
Epoch: 2 Idx: 5000 Loss: 0.0175268804918164
Epoch: 3 Idx: 0 Loss: 0.006259711251329278
Epoch: 3 Idx: 5000 Loss: 0.01547202840147184
Epoch: 4 Idx: 0 Loss: 0.008786526137834282
Epoch: 4 Idx: 5000 Loss: 0.009620603863610515
Epoch: 5 Idx: 0 Loss: 0.01024652169681806
Epoch: 5 Idx: 5000 Loss: 0.01087776446857051
Epoch: 6 Idx: 0 Loss: 0.017650268444002908
Epoch: 6 Idx: 5000 Loss: 0.030259063566038486
Epoch: 7 Idx: 0 Loss: 0.011947319510744457
Epoch: 7 Idx: 5000 Loss: 0.010522416402733766
Epoch: 8 Idx: 0 Loss: 0.010571825778484272
Epoch: 8 Idx: 5000 Loss: 0.03495879172218233
Epoch: 9 Idx: 0 Loss: 0.011762712366481222
Epoch: 9 Idx: 5000 Loss: 0.008349843902544597
Epoch: 10 Idx: 0 Loss: 0.008197937954137228
Epoch: 10 Idx: 5000 Loss: 0.01112142820593292
Epoch: 11 Idx: 0 Loss: 0.006803296579732586
Epoch: 11 Idx: 5000 Loss: 0.020113511548410394
Epoch: 12 Idx: 0 Loss: 0.01240050269575446
Epoch: 12 Idx: 5000 Loss: 0.026668727767140264
Epoch: 13 Idx: 0 Loss: 0.009989246084813986
Epoch: 13 Idx: 5000 Loss: 0.025109243833699897
Epoch: 14 Idx: 0 Loss: 0.005345033765901829
Epoch: 14 Idx: 5000 Loss: 0.010432194967885524
Epoch: 15 Idx: 0 Loss: 0.01516704091490837
Epoch: 15 Idx: 5000 Loss: 0.005858743914493707
Epoch: 16 Idx: 0 Loss: 0.03385973936389436
Epoch: 16 Idx: 5000 Loss: 0.023467208125718373
Epoch: 17 Idx: 0 Loss: 0.01041867793413641
Epoch: 17 Idx: 5000 Loss: 0.012876209999619026
Epoch: 18 Idx: 0 Loss: 0.013119723140292146
Epoch: 18 Idx: 5000 Loss: 0.014611712080755245
Epoch: 19 Idx: 0 Loss: 0.005831555047137361
Epoch: 19 Idx: 5000 Loss: 0.029250217569642033
Epoch: 20 Idx: 0 Loss: 0.014583680553767575
Epoch: 20 Idx: 5000 Loss: 0.030900191145732735
Epoch: 21 Idx: 0 Loss: 0.024144545986995534
Traceback (most recent call last):
  File "main.py", line 514, in <module>
    outputs = model(node_elems, inp_elems)
  File "/u/harshitk/miniconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "main.py", line 312, in forward
    feature_emb = self.name_embedding(features[i]) #  dim: (2, batch_size, max_types, max_paths, max_pathlen, 512)
  File "/u/harshitk/miniconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/u/harshitk/miniconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/sparse.py", line 126, in forward
    self.norm_type, self.scale_grad_by_freq, self.sparse)
  File "/u/harshitk/miniconda3/envs/py37/lib/python3.7/site-packages/torch/nn/functional.py", line 1814, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
KeyboardInterrupt

------------------------------------------------------------
Sender: LSF System <rer@dccxc220>
Subject: Job 4066859: <python main.py 5 12 False False> in cluster <dcc> Exited

Job <python main.py 5 12 False False> was submitted from host <dccxl004> by user <harshitk> in cluster <dcc> at Tue Sep 15 15:48:38 2020
Job was executed on host(s) <dccxc220>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep 15 15:48:39 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/VeeAlign/src> was used as the working directory.
Started at Tue Sep 15 15:48:39 2020
Terminated at Wed Sep 16 04:38:39 2020
Results reported at Wed Sep 16 04:38:39 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python main.py 5 12 False False
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   46049.36 sec.
    Max Memory :                                 2927 MB
    Average Memory :                             2728.79 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40490.00 MB
    Max Swap :                                   2 MB
    Max Processes :                              3
    Max Threads :                                13
    Run time :                                   46200 sec.
    Turnaround time :                            46201 sec.

The output (if any) is above this job summary.

