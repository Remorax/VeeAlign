2020-09-15 15:48:45.306701: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-09-15 15:48:52.531797: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1
2020-09-15 15:48:52.652135: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:13:00.0 name: Tesla K80 computeCapability: 3.7
coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s
2020-09-15 15:48:52.652216: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-09-15 15:48:52.666847: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
2020-09-15 15:48:52.696423: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2020-09-15 15:48:52.732675: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2020-09-15 15:48:52.774115: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2020-09-15 15:48:52.804860: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10
2020-09-15 15:48:52.805396: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudnn.so.7'; dlerror: libcudnn.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/share/lsf-9.1.3/10.1/linux3.10-glibc2.17-x86_64/lib:=/opt/share/gcc-4.9.2_rhel6/x86_64/lib/:/opt/share/gcc-4.9.2_rhel6/x86_64/lib64:/opt/share/Python-3.6.2/x86_64/lib:=/opt/share/gcc-5.4.0/x86_64/lib/:/opt/share/gcc-5.4.0/x86_64/lib64:/opt/share/isl-0.17/x86_64/lib/:/opt/share/protobuf-3.1.0/x86_64/lib/:/opt/share/leveldb-1.19/x86_64/lib/:/opt/share/boost-1.62.0/x86_64/lib/:/opt/share/torch-7/x86_64/install/lib:/opt/share/Python-2.7.12/x86_64/lib:/opt/share/Python-3.5.2/x86_64/lib:/opt/share/cuDNN-v5.1-8.0/cuda/lib64:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/opt/share/cuda-8.0/
2020-09-15 15:48:52.805424: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1753] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2020-09-15 15:48:52.805930: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-09-15 15:48:52.842260: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2599955000 Hz
2020-09-15 15:48:52.842587: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55cd21a92d20 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-09-15 15:48:52.842611: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-09-15 15:48:52.845604: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-09-15 15:48:52.845643: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      
Prefix path:  /dccstor/cogfin/arvind/da/VeeAlign/
Ontologies being aligned are:  [('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/confOf.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/conference.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/conference.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/conference.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/conference.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/confOf.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/conference.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/confOf.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/confOf.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/confOf.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl')]
Total number of extracted unique classes and properties from entire RA set:  829
Constructing abbrevation resolution dict....
Results after abbreviation resolution:  {'PC': 'Program Committee', 'OC': 'Organizing Committee'}
Resolving abbreviations...
Number of entities: 119639
Starting sliding window evaluation...
Step 0/7
Val onto:  [('confof', 'ekaw')] test_onto:  [('conference', 'sigkdd')]
Training size: 112565 Testing size: 3871
Epoch: 0 Idx: 0 Loss: 0.18298038565423644
Epoch: 0 Idx: 5000 Loss: 0.031619996123211126
Epoch: 1 Idx: 0 Loss: 0.017157829188729917
Epoch: 1 Idx: 5000 Loss: 0.011148902820118704
Epoch: 2 Idx: 0 Loss: 0.013280598317650015
Epoch: 2 Idx: 5000 Loss: 0.017710157364448243
Epoch: 3 Idx: 0 Loss: 0.048119476645502854
Epoch: 3 Idx: 5000 Loss: 0.009116567482166764
Epoch: 4 Idx: 0 Loss: 0.004900109197885012
Epoch: 4 Idx: 5000 Loss: 0.022090457217490096
Epoch: 5 Idx: 0 Loss: 0.01101296705767382
Epoch: 5 Idx: 5000 Loss: 0.013277803174207037
Epoch: 6 Idx: 0 Loss: 0.030028984501344308
Epoch: 6 Idx: 5000 Loss: 0.013123292965572774
Epoch: 7 Idx: 0 Loss: 0.018831992973991914
Epoch: 7 Idx: 5000 Loss: 0.025709558687729585
Epoch: 8 Idx: 0 Loss: 0.020359649392951484
Epoch: 8 Idx: 5000 Loss: 0.010166941782473063
Epoch: 9 Idx: 0 Loss: 0.026423093026905146
Epoch: 9 Idx: 5000 Loss: 0.018464499136717143
Epoch: 10 Idx: 0 Loss: 0.011178765993676676
Epoch: 10 Idx: 5000 Loss: 0.010696952045588678
Epoch: 11 Idx: 0 Loss: 0.014729955169136359
Epoch: 11 Idx: 5000 Loss: 0.013143709103683909
Epoch: 12 Idx: 0 Loss: 0.007469985531076195
Epoch: 12 Idx: 5000 Loss: 0.02125866763216764
Epoch: 13 Idx: 0 Loss: 0.006214639701135382
Epoch: 13 Idx: 5000 Loss: 0.005028210879692155
Epoch: 14 Idx: 0 Loss: 0.035316313665445034
Epoch: 14 Idx: 5000 Loss: 0.010638563223480621
Epoch: 15 Idx: 0 Loss: 0.030327546805534813
Epoch: 15 Idx: 5000 Loss: 0.019203866808117434
Epoch: 16 Idx: 0 Loss: 0.016491798740965458
Epoch: 16 Idx: 5000 Loss: 0.01000768465497483
Epoch: 17 Idx: 0 Loss: 0.011263576727839807
Epoch: 17 Idx: 5000 Loss: 0.009000073241902775
Epoch: 18 Idx: 0 Loss: 0.025463780242689343
Epoch: 18 Idx: 5000 Loss: 0.005605819700942947
Epoch: 19 Idx: 0 Loss: 0.017739791205125997
Epoch: 19 Idx: 5000 Loss: 0.04023640440654203
Epoch: 20 Idx: 0 Loss: 0.006593104624846899
Epoch: 20 Idx: 5000 Loss: 0.01901804094698185
Epoch: 21 Idx: 0 Loss: 0.03031130811224165
Epoch: 21 Idx: 5000 Loss: 0.024572658818833203
Epoch: 22 Idx: 0 Loss: 0.017107413192336475
Epoch: 22 Idx: 5000 Loss: 0.014088106643883734
Epoch: 23 Idx: 0 Loss: 0.014025699888980322
Epoch: 23 Idx: 5000 Loss: 0.012776835228661602
Epoch: 24 Idx: 0 Loss: 0.02208557277923677
Epoch: 24 Idx: 5000 Loss: 0.010306903045338996
Epoch: 25 Idx: 0 Loss: 0.010651032810789215
Epoch: 25 Idx: 5000 Loss: 0.011700147438720474
Epoch: 26 Idx: 0 Loss: 0.029164725035019733
Epoch: 26 Idx: 5000 Loss: 0.009803334705385686
Epoch: 27 Idx: 0 Loss: 0.014159224195256897
Epoch: 27 Idx: 5000 Loss: 0.012608473593316856
Epoch: 28 Idx: 0 Loss: 0.027299934040599622
Epoch: 28 Idx: 5000 Loss: 0.010837536324117231
Epoch: 29 Idx: 0 Loss: 0.028594180769828666
Epoch: 29 Idx: 5000 Loss: 0.03299216285137247
Epoch: 30 Idx: 0 Loss: 0.028414201439836903
Epoch: 30 Idx: 5000 Loss: 0.011327587835260135
Epoch: 31 Idx: 0 Loss: 0.01978107114535682
Epoch: 31 Idx: 5000 Loss: 0.01855058755381133
Epoch: 32 Idx: 0 Loss: 0.01672466848688813
Epoch: 32 Idx: 5000 Loss: 0.017480209600664508
Epoch: 33 Idx: 0 Loss: 0.006518263549328671
Epoch: 33 Idx: 5000 Loss: 0.014594621793867538
Epoch: 34 Idx: 0 Loss: 0.014302240482541369
Epoch: 34 Idx: 5000 Loss: 0.04840032467920566
Epoch: 35 Idx: 0 Loss: 0.028295911303897397
Epoch: 35 Idx: 5000 Loss: 0.012380222520505841
Epoch: 36 Idx: 0 Loss: 0.011056677388259497
Epoch: 36 Idx: 5000 Loss: 0.015612551702900318
Epoch: 37 Idx: 0 Loss: 0.017899171719082074
Epoch: 37 Idx: 5000 Loss: 0.028311303618156432
Epoch: 38 Idx: 0 Loss: 0.015706802324024986
Epoch: 38 Idx: 5000 Loss: 0.034954443273753674
Epoch: 39 Idx: 0 Loss: 0.02776248767142133
Epoch: 39 Idx: 5000 Loss: 0.019613388901989656
Epoch: 40 Idx: 0 Loss: 0.013701532310050366
Epoch: 40 Idx: 5000 Loss: 0.009893121075637931
Epoch: 41 Idx: 0 Loss: 0.011398276638613139
Epoch: 41 Idx: 5000 Loss: 0.04923569392258308
Epoch: 42 Idx: 0 Loss: 0.01747842151361303
Epoch: 42 Idx: 5000 Loss: 0.00993684478282522
Epoch: 43 Idx: 0 Loss: 0.020001742502134198
Epoch: 43 Idx: 5000 Loss: 0.013164430386468605
Epoch: 44 Idx: 0 Loss: 0.012984337295334797
Epoch: 44 Idx: 5000 Loss: 0.012250525545172471
Epoch: 45 Idx: 0 Loss: 0.01834819301649302
Epoch: 45 Idx: 5000 Loss: 0.012049653407857666
Epoch: 46 Idx: 0 Loss: 0.013650551965505714
Epoch: 46 Idx: 5000 Loss: 0.010728416342150191
Epoch: 47 Idx: 0 Loss: 0.02112080842876031
Epoch: 47 Idx: 5000 Loss: 0.01183131832525998
Epoch: 48 Idx: 0 Loss: 0.007293384470030783
Epoch: 48 Idx: 5000 Loss: 0.014918141850648211
Epoch: 49 Idx: 0 Loss: 0.011430100148517287
Epoch: 49 Idx: 5000 Loss: 0.040421113502055586
Len (direct inputs):  429
Inputs len 2744 15 3856
Len (direct inputs):  1127
Starting sliding window evaluation...
Step 2/7
Val onto:  [('conference', 'ekaw')] test_onto:  [('cmt', 'sigkdd')]
Training size: 111450 Testing size: 2364
Epoch: 0 Idx: 0 Loss: 0.15172646240962692
Epoch: 0 Idx: 5000 Loss: 0.025289154374576937
Epoch: 1 Idx: 0 Loss: 0.026593529962547442
Epoch: 1 Idx: 5000 Loss: 0.01837955636179963
Epoch: 2 Idx: 0 Loss: 0.03853603722694926
Epoch: 2 Idx: 5000 Loss: 0.010284956588204164
Epoch: 3 Idx: 0 Loss: 0.03782614814572293
Epoch: 3 Idx: 5000 Loss: 0.012838133700075102
Epoch: 4 Idx: 0 Loss: 0.004133559033723109
Epoch: 4 Idx: 5000 Loss: 0.04397887123395839
Epoch: 5 Idx: 0 Loss: 0.01601166901445415
Epoch: 5 Idx: 5000 Loss: 0.008029251406203607
Epoch: 6 Idx: 0 Loss: 0.02015254139008551
Epoch: 6 Idx: 5000 Loss: 0.011808413169108693
Epoch: 7 Idx: 0 Loss: 0.005415991673724538
Epoch: 7 Idx: 5000 Loss: 0.011288834528442404
Epoch: 8 Idx: 0 Loss: 0.012623944337173908
Epoch: 8 Idx: 5000 Loss: 0.014156575131535204
Epoch: 9 Idx: 0 Loss: 0.012654972125011241
Epoch: 9 Idx: 5000 Loss: 0.010151537685226836
Epoch: 10 Idx: 0 Loss: 0.006857227343375904
Epoch: 10 Idx: 5000 Loss: 0.011768413655840325
Epoch: 11 Idx: 0 Loss: 0.026328083905942178
Epoch: 11 Idx: 5000 Loss: 0.008846501382148573
Epoch: 12 Idx: 0 Loss: 0.042092387609725536
Epoch: 12 Idx: 5000 Loss: 0.01256175344386368
Epoch: 13 Idx: 0 Loss: 0.0053270671827746465
Epoch: 13 Idx: 5000 Loss: 0.007600303565821779
Epoch: 14 Idx: 0 Loss: 0.044604686524198287
Epoch: 14 Idx: 5000 Loss: 0.01430327965042934
Epoch: 15 Idx: 0 Loss: 0.017091046497544445
Epoch: 15 Idx: 5000 Loss: 0.00641539391282486
Epoch: 16 Idx: 0 Loss: 0.025642876109497358
Epoch: 16 Idx: 5000 Loss: 0.029260817296636873
Epoch: 17 Idx: 0 Loss: 0.02945151194171614
Epoch: 17 Idx: 5000 Loss: 0.005595859967583328
Epoch: 18 Idx: 0 Loss: 0.04129072907964066
Epoch: 18 Idx: 5000 Loss: 0.020398006085742146
Epoch: 19 Idx: 0 Loss: 0.032634464994652154
Epoch: 19 Idx: 5000 Loss: 0.0130777406198691
Epoch: 20 Idx: 0 Loss: 0.01904724131395112
Epoch: 20 Idx: 5000 Loss: 0.012285844691676182
Epoch: 21 Idx: 0 Loss: 0.02127405872279348
Epoch: 21 Idx: 5000 Loss: 0.009799586838003214
Epoch: 22 Idx: 0 Loss: 0.009113382079322895
Epoch: 22 Idx: 5000 Loss: 0.015440421356343013
Epoch: 23 Idx: 0 Loss: 0.011726030803790408
Epoch: 23 Idx: 5000 Loss: 0.011201783010984834
Epoch: 24 Idx: 0 Loss: 0.006626718437790098
Epoch: 24 Idx: 5000 Loss: 0.014173221903038916
Epoch: 25 Idx: 0 Loss: 0.016723701967200858
Epoch: 25 Idx: 5000 Loss: 0.01519716548927718
Epoch: 26 Idx: 0 Loss: 0.006783887015398097
Epoch: 26 Idx: 5000 Loss: 0.024878361559634
Epoch: 27 Idx: 0 Loss: 0.012576248082699682
Epoch: 27 Idx: 5000 Loss: 0.011990857397553288
Epoch: 28 Idx: 0 Loss: 0.008188665999122798
Epoch: 28 Idx: 5000 Loss: 0.008314350541164896
Epoch: 29 Idx: 0 Loss: 0.012605047175548455
Epoch: 29 Idx: 5000 Loss: 0.018329799430566673
Epoch: 30 Idx: 0 Loss: 0.014428298926279315
Epoch: 30 Idx: 5000 Loss: 0.017133978652044954
Epoch: 31 Idx: 0 Loss: 0.011184336696432623
Epoch: 31 Idx: 5000 Loss: 0.018586204101230408
Epoch: 32 Idx: 0 Loss: 0.015375501400649244
Epoch: 32 Idx: 5000 Loss: 0.020734905216555105
Epoch: 33 Idx: 0 Loss: 0.009727305681925858
Epoch: 33 Idx: 5000 Loss: 0.009269404635234747
Epoch: 34 Idx: 0 Loss: 0.006954999356839266
Epoch: 34 Idx: 5000 Loss: 0.01775404228652147
Epoch: 35 Idx: 0 Loss: 0.010858018953763982
Epoch: 35 Idx: 5000 Loss: 0.019586139788842737
Epoch: 36 Idx: 0 Loss: 0.057876482116849134
Epoch: 36 Idx: 5000 Loss: 0.013757709576775351
Epoch: 37 Idx: 0 Loss: 0.010185326123998927
Epoch: 37 Idx: 5000 Loss: 0.0350252908932269
Epoch: 38 Idx: 0 Loss: 0.013979262510311242
Epoch: 38 Idx: 5000 Loss: 0.017210648292969873
Epoch: 39 Idx: 0 Loss: 0.020833880774996146
Epoch: 39 Idx: 5000 Loss: 0.01804904846840364
Epoch: 40 Idx: 0 Loss: 0.01013800708801729
Epoch: 40 Idx: 5000 Loss: 0.006156658281705286
Epoch: 41 Idx: 0 Loss: 0.02080322836417752
Epoch: 41 Idx: 5000 Loss: 0.014350016714452647
Epoch: 42 Idx: 0 Loss: 0.010232372691071974
Epoch: 42 Idx: 5000 Loss: 0.019051382837505546
Epoch: 43 Idx: 0 Loss: 0.009131366516284663
Epoch: 43 Idx: 5000 Loss: 0.008731263509633005
Epoch: 44 Idx: 0 Loss: 0.027611065823164083
Epoch: 44 Idx: 5000 Loss: 0.009955766794479917
Epoch: 45 Idx: 0 Loss: 0.010581017320794184
Epoch: 45 Idx: 5000 Loss: 0.004515849230238277
Epoch: 46 Idx: 0 Loss: 0.009909739512222187
Epoch: 46 Idx: 5000 Loss: 0.020505841315464908
Epoch: 47 Idx: 0 Loss: 0.008573129532170019
Epoch: 47 Idx: 5000 Loss: 0.010642893744769276
Epoch: 48 Idx: 0 Loss: 0.013054334094541669
Epoch: 48 Idx: 5000 Loss: 0.017455056205100232
Epoch: 49 Idx: 0 Loss: 0.029600443791385603
Epoch: 49 Idx: 5000 Loss: 0.0122178319544472
Len (direct inputs):  1737
Inputs len 1372 12 2352
Len (direct inputs):  992
Starting sliding window evaluation...
Step 4/7
Val onto:  [('ekaw', 'sigkdd')] test_onto:  [('cmt', 'conference')]
Training size: 111356 Testing size: 4145
Epoch: 0 Idx: 0 Loss: 0.16277644546903022
Epoch: 0 Idx: 5000 Loss: 0.015217298159632115
Epoch: 1 Idx: 0 Loss: 0.011343552132946796
Epoch: 1 Idx: 5000 Loss: 0.01623708084697402
Epoch: 2 Idx: 0 Loss: 0.017720779177693075
Epoch: 2 Idx: 5000 Loss: 0.024444171936747895
Epoch: 3 Idx: 0 Loss: 0.014430175961198898
Epoch: 3 Idx: 5000 Loss: 0.0112277837741939
Epoch: 4 Idx: 0 Loss: 0.033064284760466585
Epoch: 4 Idx: 5000 Loss: 0.018104851597176362
Epoch: 5 Idx: 0 Loss: 0.022061240999508444
Epoch: 5 Idx: 5000 Loss: 0.012608952845564816
Epoch: 6 Idx: 0 Loss: 0.009176052766335135
Epoch: 6 Idx: 5000 Loss: 0.03023974097889484
Epoch: 7 Idx: 0 Loss: 0.012409655928252637
Epoch: 7 Idx: 5000 Loss: 0.02321669319696116
Epoch: 8 Idx: 0 Loss: 0.013510226678062821
Epoch: 8 Idx: 5000 Loss: 0.011723576614722293
Epoch: 9 Idx: 0 Loss: 0.0477876974134018
Epoch: 9 Idx: 5000 Loss: 0.011061264005794547
Epoch: 10 Idx: 0 Loss: 0.019609310152252802
Epoch: 10 Idx: 5000 Loss: 0.010309369522547768
Epoch: 11 Idx: 0 Loss: 0.01360484896755945
Epoch: 11 Idx: 5000 Loss: 0.014381903957709843
Epoch: 12 Idx: 0 Loss: 0.02353360790234861
Epoch: 12 Idx: 5000 Loss: 0.015183769532131764
Epoch: 13 Idx: 0 Loss: 0.011390273916589085
Epoch: 13 Idx: 5000 Loss: 0.014354891457927128
Epoch: 14 Idx: 0 Loss: 0.029932808664793528
Epoch: 14 Idx: 5000 Loss: 0.00717287528162141
Epoch: 15 Idx: 0 Loss: 0.012049811099925482
Epoch: 15 Idx: 5000 Loss: 0.02776100257372873
Epoch: 16 Idx: 0 Loss: 0.01018412418122143
Epoch: 16 Idx: 5000 Loss: 0.017591038564387887
Epoch: 17 Idx: 0 Loss: 0.0080641451335621
Epoch: 17 Idx: 5000 Loss: 0.016706588540404577
Epoch: 18 Idx: 0 Loss: 0.010879963622750332
Epoch: 18 Idx: 5000 Loss: 0.018547337364268217
Epoch: 19 Idx: 0 Loss: 0.013911838260963679
Epoch: 19 Idx: 5000 Loss: 0.020772644081993363
Epoch: 20 Idx: 0 Loss: 0.006076302521297851
Epoch: 20 Idx: 5000 Loss: 0.021454235942742216
Epoch: 21 Idx: 0 Loss: 0.010853747292450547
Epoch: 21 Idx: 5000 Loss: 0.00964729009441134
Epoch: 22 Idx: 0 Loss: 0.010459394208918
Epoch: 22 Idx: 5000 Loss: 0.023047422530759205
Epoch: 23 Idx: 0 Loss: 0.02235382464932922
Epoch: 23 Idx: 5000 Loss: 0.018756937023589756
Epoch: 24 Idx: 0 Loss: 0.007505172277430654
Epoch: 24 Idx: 5000 Loss: 0.01228917653294225
Epoch: 25 Idx: 0 Loss: 0.012759342732625212
Epoch: 25 Idx: 5000 Loss: 0.007275557251960741
Epoch: 26 Idx: 0 Loss: 0.009392177188870074
Epoch: 26 Idx: 5000 Loss: 0.018394695233652045
Epoch: 27 Idx: 0 Loss: 0.031131893345021828
Epoch: 27 Idx: 5000 Loss: 0.012484481402756743
Epoch: 28 Idx: 0 Loss: 0.0199049508596155
Epoch: 28 Idx: 5000 Loss: 0.009020478136524854
Epoch: 29 Idx: 0 Loss: 0.02425426254700431
Epoch: 29 Idx: 5000 Loss: 0.010486955148436712
Epoch: 30 Idx: 0 Loss: 0.012897624217235809
Epoch: 30 Idx: 5000 Loss: 0.016512171138590537
Epoch: 31 Idx: 0 Loss: 0.022976360242512716
Epoch: 31 Idx: 5000 Loss: 0.01923730326642656
Epoch: 32 Idx: 0 Loss: 0.03926791539572753
Epoch: 32 Idx: 5000 Loss: 0.01722511236143308
Epoch: 33 Idx: 0 Loss: 0.007875924643324534
Epoch: 33 Idx: 5000 Loss: 0.016114210307049674
Epoch: 34 Idx: 0 Loss: 0.014912111675722194
Epoch: 34 Idx: 5000 Loss: 0.015546340101174332
Epoch: 35 Idx: 0 Loss: 0.02288903375993513
Epoch: 35 Idx: 5000 Loss: 0.007002839474157691
Epoch: 36 Idx: 0 Loss: 0.019227417603022337
Epoch: 36 Idx: 5000 Loss: 0.017477575140662126
Epoch: 37 Idx: 0 Loss: 0.020048904723194566
Epoch: 37 Idx: 5000 Loss: 0.018329444608943965
Epoch: 38 Idx: 0 Loss: 0.011855069470405682
Epoch: 38 Idx: 5000 Loss: 0.02081954113551262
Epoch: 39 Idx: 0 Loss: 0.020257212961818996
Epoch: 39 Idx: 5000 Loss: 0.04443405343935352
Epoch: 40 Idx: 0 Loss: 0.013549900108554402
Epoch: 40 Idx: 5000 Loss: 0.04388018176638802
Epoch: 41 Idx: 0 Loss: 0.008361701551575038
Epoch: 41 Idx: 5000 Loss: 0.007405274301214462
Epoch: 42 Idx: 0 Loss: 0.027350485634374717
Epoch: 42 Idx: 5000 Loss: 0.00941217107022406
Epoch: 43 Idx: 0 Loss: 0.004131742989664419
Epoch: 43 Idx: 5000 Loss: 0.00828329564728767
Epoch: 44 Idx: 0 Loss: 0.023885375135353142
Epoch: 44 Idx: 5000 Loss: 0.011299609689929942
Epoch: 45 Idx: 0 Loss: 0.03935233375417244
Epoch: 45 Idx: 5000 Loss: 0.025547236557725714
Epoch: 46 Idx: 0 Loss: 0.00795162268374363
Epoch: 46 Idx: 5000 Loss: 0.010454520064347586
Epoch: 47 Idx: 0 Loss: 0.02134083609844637
Epoch: 47 Idx: 5000 Loss: 0.012999357853430869
Epoch: 48 Idx: 0 Loss: 0.009457952697920437
Epoch: 48 Idx: 5000 Loss: 0.018190658798672453
Epoch: 49 Idx: 0 Loss: 0.04832834315609924
Epoch: 49 Idx: 5000 Loss: 0.012253206398127131
Len (direct inputs):  561
Inputs len 1568 15 4130
Len (direct inputs):  2577
Starting sliding window evaluation...
Step 6/7
Val onto:  [('edas', 'sigkdd')] test_onto:  [('conference', 'edas')]
Training size: 106045 Testing size: 7817
Epoch: 0 Idx: 0 Loss: 0.24572654261705087
Epoch: 0 Idx: 5000 Loss: 0.00990581864448996
Epoch: 1 Idx: 0 Loss: 0.007448204798472507
Epoch: 1 Idx: 5000 Loss: 0.00994192464834134
Epoch: 2 Idx: 0 Loss: 0.012019962970722201
Epoch: 2 Idx: 5000 Loss: 0.01569181525015182
Epoch: 3 Idx: 0 Loss: 0.024193850426596702
Epoch: 3 Idx: 5000 Loss: 0.017107930549489163
Epoch: 4 Idx: 0 Loss: 0.026804427799855986
Epoch: 4 Idx: 5000 Loss: 0.022925558286309492
Epoch: 5 Idx: 0 Loss: 0.012881587875748567
Epoch: 5 Idx: 5000 Loss: 0.013724133357324985
Epoch: 6 Idx: 0 Loss: 0.009937288686388955
Epoch: 6 Idx: 5000 Loss: 0.01615677192634183
Epoch: 7 Idx: 0 Loss: 0.02431659829844399
Epoch: 7 Idx: 5000 Loss: 0.038498739153582796
Epoch: 8 Idx: 0 Loss: 0.015438919690512212
Epoch: 8 Idx: 5000 Loss: 0.02256589256870238
Epoch: 9 Idx: 0 Loss: 0.03745385931404549
Epoch: 9 Idx: 5000 Loss: 0.02452940464966828
Epoch: 10 Idx: 0 Loss: 0.016194062334869804
Epoch: 10 Idx: 5000 Loss: 0.019099935082422033
Epoch: 11 Idx: 0 Loss: 0.020415068914025284
Epoch: 11 Idx: 5000 Loss: 0.015121721639661356
Epoch: 12 Idx: 0 Loss: 0.006552236040441868
Epoch: 12 Idx: 5000 Loss: 0.011268924862541028
Epoch: 13 Idx: 0 Loss: 0.01765442308988449
Epoch: 13 Idx: 5000 Loss: 0.02087425082980036
Epoch: 14 Idx: 0 Loss: 0.020290264167119917
Epoch: 14 Idx: 5000 Loss: 0.01371484676178141
Epoch: 15 Idx: 0 Loss: 0.03125151569720397
Epoch: 15 Idx: 5000 Loss: 0.004914512266356648
Epoch: 16 Idx: 0 Loss: 0.013531029924912865
Epoch: 16 Idx: 5000 Loss: 0.027249403017920597
Epoch: 17 Idx: 0 Loss: 0.010261297028360816
Epoch: 17 Idx: 5000 Loss: 0.01044882910508971
Epoch: 18 Idx: 0 Loss: 0.012049024704514669
Epoch: 18 Idx: 5000 Loss: 0.007928012294612749
Epoch: 19 Idx: 0 Loss: 0.007902600319915479
Epoch: 19 Idx: 5000 Loss: 0.011853371656608418
Epoch: 20 Idx: 0 Loss: 0.008954068525447823
Epoch: 20 Idx: 5000 Loss: 0.007675235387609328
Epoch: 21 Idx: 0 Loss: 0.008481453885124099
Epoch: 21 Idx: 5000 Loss: 0.02047059221773387
Epoch: 22 Idx: 0 Loss: 0.013585863458038932
Epoch: 22 Idx: 5000 Loss: 0.014311361549165603
Epoch: 23 Idx: 0 Loss: 0.022284224061786838
Epoch: 23 Idx: 5000 Loss: 0.012959459333141818
Epoch: 24 Idx: 0 Loss: 0.03036641144295297
Epoch: 24 Idx: 5000 Loss: 0.023446995224646487
Epoch: 25 Idx: 0 Loss: 0.015825923500361782
Epoch: 25 Idx: 5000 Loss: 0.016393925669470413
Epoch: 26 Idx: 0 Loss: 0.012663643596540904
Epoch: 26 Idx: 5000 Loss: 0.013325404404274276
Epoch: 27 Idx: 0 Loss: 0.03260097242347891
Epoch: 27 Idx: 5000 Loss: 0.01235401309534153
Epoch: 28 Idx: 0 Loss: 0.015973319396784168
Epoch: 28 Idx: 5000 Loss: 0.010481120944467159
Epoch: 29 Idx: 0 Loss: 0.006187056400667126
Epoch: 29 Idx: 5000 Loss: 0.01591861335074323
Epoch: 30 Idx: 0 Loss: 0.0141804333894311
Epoch: 30 Idx: 5000 Loss: 0.011683571215363054
Epoch: 31 Idx: 0 Loss: 0.010096272545904159
Epoch: 31 Idx: 5000 Loss: 0.038483585067002155
Epoch: 32 Idx: 0 Loss: 0.016560070333455544
Epoch: 32 Idx: 5000 Loss: 0.010751951603638777
Epoch: 33 Idx: 0 Loss: 0.024218021079100614
Epoch: 33 Idx: 5000 Loss: 0.023851681528434074
Epoch: 34 Idx: 0 Loss: 0.018626994718246307
Epoch: 34 Idx: 5000 Loss: 0.02322620959085647
Epoch: 35 Idx: 0 Loss: 0.006578694725750962
Epoch: 35 Idx: 5000 Loss: 0.033015157034256996
Epoch: 36 Idx: 0 Loss: 0.03031822078594936
Epoch: 36 Idx: 5000 Loss: 0.004049780128556823
Epoch: 37 Idx: 0 Loss: 0.010783751237950591
Epoch: 37 Idx: 5000 Loss: 0.012007766609142736
Epoch: 38 Idx: 0 Loss: 0.03417215062661633
Epoch: 38 Idx: 5000 Loss: 0.019607907433032683
Epoch: 39 Idx: 0 Loss: 0.006700123781284814
Epoch: 39 Idx: 5000 Loss: 0.010340972231273244
Epoch: 40 Idx: 0 Loss: 0.014460051656325017
Epoch: 40 Idx: 5000 Loss: 0.01253590390833555
Epoch: 41 Idx: 0 Loss: 0.023091112631458195
Epoch: 41 Idx: 5000 Loss: 0.005462884121678406
Epoch: 42 Idx: 0 Loss: 0.018714198086868514
Epoch: 42 Idx: 5000 Loss: 0.025545936566157716
Epoch: 43 Idx: 0 Loss: 0.011357311568005481
Epoch: 43 Idx: 5000 Loss: 0.025644400927507124
Epoch: 44 Idx: 0 Loss: 0.013506092395894365
Epoch: 44 Idx: 5000 Loss: 0.022455092665110202
Epoch: 45 Idx: 0 Loss: 0.015083023479988173
Epoch: 45 Idx: 5000 Loss: 0.05595042941270546
Epoch: 46 Idx: 0 Loss: 0.01653670275426669
Epoch: 46 Idx: 5000 Loss: 0.011891001301153642
Epoch: 47 Idx: 0 Loss: 0.023917847594530098
Epoch: 47 Idx: 5000 Loss: 0.015426310623336149
Epoch: 48 Idx: 0 Loss: 0.021236003590113364
Epoch: 48 Idx: 5000 Loss: 0.026009554375140295
Epoch: 49 Idx: 0 Loss: 0.0114089125401109
Epoch: 49 Idx: 5000 Loss: 0.02717026949234879
Len (direct inputs):  877
Inputs len 5600 17 7800
Len (direct inputs):  2217
Starting sliding window evaluation...
Step 8/7
Val onto:  [('cmt', 'iasted')] test_onto:  [('confof', 'sigkdd')]
Training size: 111351 Testing size: 2336
Epoch: 0 Idx: 0 Loss: 0.20610183948561372
Epoch: 0 Idx: 5000 Loss: 0.013695385364049684
Epoch: 1 Idx: 0 Loss: 0.007659784784215613
Epoch: 1 Idx: 5000 Loss: 0.008175511884448145
Epoch: 2 Idx: 0 Loss: 0.012351921457082908
Epoch: 2 Idx: 5000 Loss: 0.02732858455951092
Epoch: 3 Idx: 0 Loss: 0.017334861962132
Epoch: 3 Idx: 5000 Loss: 0.011801765241924014
Epoch: 4 Idx: 0 Loss: 0.010882117541921853
Epoch: 4 Idx: 5000 Loss: 0.02107420461511195
Epoch: 5 Idx: 0 Loss: 0.0062459463305961135
Epoch: 5 Idx: 5000 Loss: 0.014825053972848843
Epoch: 6 Idx: 0 Loss: 0.026730289125167003
Epoch: 6 Idx: 5000 Loss: 0.027039599226243726
Epoch: 7 Idx: 0 Loss: 0.058292123559883724
Epoch: 7 Idx: 5000 Loss: 0.0075378014901299455
Epoch: 8 Idx: 0 Loss: 0.008820665295303074
Epoch: 8 Idx: 5000 Loss: 0.012417023987367671
Epoch: 9 Idx: 0 Loss: 0.01213379331283267
Epoch: 9 Idx: 5000 Loss: 0.015164373770530155
Epoch: 10 Idx: 0 Loss: 0.006938942931717303
Epoch: 10 Idx: 5000 Loss: 0.020539642363650946
Epoch: 11 Idx: 0 Loss: 0.019704339458996467
Epoch: 11 Idx: 5000 Loss: 0.037837052670242954
Epoch: 12 Idx: 0 Loss: 0.012130548497279833
Epoch: 12 Idx: 5000 Loss: 0.018958789887777452
Epoch: 13 Idx: 0 Loss: 0.02433975997756013
Epoch: 13 Idx: 5000 Loss: 0.026277805921731515
Epoch: 14 Idx: 0 Loss: 0.027198339617883108
Epoch: 14 Idx: 5000 Loss: 0.011644889842585616
Epoch: 15 Idx: 0 Loss: 0.021514521104654766
Epoch: 15 Idx: 5000 Loss: 0.0053051862001317895
Epoch: 16 Idx: 0 Loss: 0.029212633699493704
Epoch: 16 Idx: 5000 Loss: 0.025484800885306778
Epoch: 17 Idx: 0 Loss: 0.010688465988225622
Epoch: 17 Idx: 5000 Loss: 0.006773944428570846
Epoch: 18 Idx: 0 Loss: 0.01367107320672937
Epoch: 18 Idx: 5000 Loss: 0.01333483157028898
Epoch: 19 Idx: 0 Loss: 0.02079914889000979
Epoch: 19 Idx: 5000 Loss: 0.011047229954801807
Epoch: 20 Idx: 0 Loss: 0.016706077323272346
Epoch: 20 Idx: 5000 Loss: 0.017344431252497874
Epoch: 21 Idx: 0 Loss: 0.017855930603490104
Epoch: 21 Idx: 5000 Loss: 0.02624503617201923
Epoch: 22 Idx: 0 Loss: 0.014960519007607675
Epoch: 22 Idx: 5000 Loss: 0.022201707136615834
Epoch: 23 Idx: 0 Loss: 0.04091767144072124
Epoch: 23 Idx: 5000 Loss: 0.011516368836640362
Epoch: 24 Idx: 0 Loss: 0.018911690310654643
Epoch: 24 Idx: 5000 Loss: 0.023795094871779818
Epoch: 25 Idx: 0 Loss: 0.01031728319181123
Epoch: 25 Idx: 5000 Loss: 0.018952882830541043
Epoch: 26 Idx: 0 Loss: 0.012326046411460071
Epoch: 26 Idx: 5000 Loss: 0.005594410431595388
Epoch: 27 Idx: 0 Loss: 0.011182129295425496
Epoch: 27 Idx: 5000 Loss: 0.036927162702871455
Epoch: 28 Idx: 0 Loss: 0.01607844598608455
Epoch: 28 Idx: 5000 Loss: 0.010831480936077791
Epoch: 29 Idx: 0 Loss: 0.011835546444772688
Epoch: 29 Idx: 5000 Loss: 0.013991645929883407
Epoch: 30 Idx: 0 Loss: 0.014087550685205969
Epoch: 30 Idx: 5000 Loss: 0.016026181800508073
Epoch: 31 Idx: 0 Loss: 0.018244240319509955
Epoch: 31 Idx: 5000 Loss: 0.01236986166311006
Epoch: 32 Idx: 0 Loss: 0.02039479531205084
Epoch: 32 Idx: 5000 Loss: 0.019001679877572283
Epoch: 33 Idx: 0 Loss: 0.01896626220317996
Epoch: 33 Idx: 5000 Loss: 0.016537351220395888
Epoch: 34 Idx: 0 Loss: 0.010188467440438994
Epoch: 34 Idx: 5000 Loss: 0.016547433608143106
Epoch: 35 Idx: 0 Loss: 0.014562450704018522
Epoch: 35 Idx: 5000 Loss: 0.010419190636406579
Epoch: 36 Idx: 0 Loss: 0.015365315737737056
Epoch: 36 Idx: 5000 Loss: 0.009943333982096835
Epoch: 37 Idx: 0 Loss: 0.02253979959243751
Epoch: 37 Idx: 5000 Loss: 0.020639170105443732
Epoch: 38 Idx: 0 Loss: 0.03523400602801711
Epoch: 38 Idx: 5000 Loss: 0.05158003499763538
Epoch: 39 Idx: 0 Loss: 0.017674228338046428
Epoch: 39 Idx: 5000 Loss: 0.016275534033510387
Epoch: 40 Idx: 0 Loss: 0.03142123127264816
Epoch: 40 Idx: 5000 Loss: 0.009994743930081235
Epoch: 41 Idx: 0 Loss: 0.014961094268979125
Epoch: 41 Idx: 5000 Loss: 0.011333533186930464
Epoch: 42 Idx: 0 Loss: 0.02438782806554661
Epoch: 42 Idx: 5000 Loss: 0.011947637067210762
Epoch: 43 Idx: 0 Loss: 0.015198278780573128
Epoch: 43 Idx: 5000 Loss: 0.03100848534145803
Epoch: 44 Idx: 0 Loss: 0.03187954553818691
Epoch: 44 Idx: 5000 Loss: 0.010505726254957963
Epoch: 45 Idx: 0 Loss: 0.026137070541919355
Epoch: 45 Idx: 5000 Loss: 0.0073118421790214425
Epoch: 46 Idx: 0 Loss: 0.011277741031488944
Epoch: 46 Idx: 5000 Loss: 0.0506053898545017
Epoch: 47 Idx: 0 Loss: 0.030338206231716132
Epoch: 47 Idx: 5000 Loss: 0.020189403291633108
Epoch: 48 Idx: 0 Loss: 0.02173587909511288
Epoch: 48 Idx: 5000 Loss: 0.02078808073595427
Epoch: 49 Idx: 0 Loss: 0.022183187818786178
Epoch: 49 Idx: 5000 Loss: 0.025621453405133367
Len (direct inputs):  2088
Inputs len 1862 7 2329
Len (direct inputs):  474
Starting sliding window evaluation...
Step 10/7
Val onto:  [('cmt', 'ekaw')] test_onto:  [('ekaw', 'iasted')]
Training size: 104431 Testing size: 11474
Epoch: 0 Idx: 0 Loss: 0.2265132447858691
Epoch: 0 Idx: 5000 Loss: 0.048918177469997826
Epoch: 1 Idx: 0 Loss: 0.02218408370671398
Epoch: 1 Idx: 5000 Loss: 0.01237827307956773
Epoch: 2 Idx: 0 Loss: 0.008611599418518157
Epoch: 2 Idx: 5000 Loss: 0.018521910362126085
Epoch: 3 Idx: 0 Loss: 0.008528279191798088
Epoch: 3 Idx: 5000 Loss: 0.014024949273546802
Epoch: 4 Idx: 0 Loss: 0.014338017404949721
Epoch: 4 Idx: 5000 Loss: 0.01758543494233042
Epoch: 5 Idx: 0 Loss: 0.021438995423230797
Epoch: 5 Idx: 5000 Loss: 0.015734866817748744
Epoch: 6 Idx: 0 Loss: 0.016624360009500794
Epoch: 6 Idx: 5000 Loss: 0.01924988236811603
Epoch: 7 Idx: 0 Loss: 0.02470767155936366
Epoch: 7 Idx: 5000 Loss: 0.008178375740031017
Epoch: 8 Idx: 0 Loss: 0.006519625616915245
Epoch: 8 Idx: 5000 Loss: 0.010446841508187922
Epoch: 9 Idx: 0 Loss: 0.01406669536487056
Epoch: 9 Idx: 5000 Loss: 0.01078600527188112
Epoch: 10 Idx: 0 Loss: 0.004230614119015223
Epoch: 10 Idx: 5000 Loss: 0.012095850521027258
Epoch: 11 Idx: 0 Loss: 0.012211005175233755
Epoch: 11 Idx: 5000 Loss: 0.019680898995360976
Epoch: 12 Idx: 0 Loss: 0.008417877101350554
Epoch: 12 Idx: 5000 Loss: 0.0158433309174254
Epoch: 13 Idx: 0 Loss: 0.009202568233133842
Epoch: 13 Idx: 5000 Loss: 0.008676392876931862
Epoch: 14 Idx: 0 Loss: 0.030711558681605064
Epoch: 14 Idx: 5000 Loss: 0.011287129602399859
Epoch: 15 Idx: 0 Loss: 0.008113181114403937
Epoch: 15 Idx: 5000 Loss: 0.02224158704559966
Epoch: 16 Idx: 0 Loss: 0.05270503917308523
Epoch: 16 Idx: 5000 Loss: 0.019018516259314694
Epoch: 17 Idx: 0 Loss: 0.02130079625752989
Epoch: 17 Idx: 5000 Loss: 0.015340248946540324
Epoch: 18 Idx: 0 Loss: 0.019681234255399672
Epoch: 18 Idx: 5000 Loss: 0.008355666708918226
Epoch: 19 Idx: 0 Loss: 0.01555730527460792
Epoch: 19 Idx: 5000 Loss: 0.00863524311842035
Epoch: 20 Idx: 0 Loss: 0.011534566297012242
Epoch: 20 Idx: 5000 Loss: 0.016449824317585255
Epoch: 21 Idx: 0 Loss: 0.021570235071527145
Epoch: 21 Idx: 5000 Loss: 0.0326637329597896
Epoch: 22 Idx: 0 Loss: 0.004059308684044887
Epoch: 22 Idx: 5000 Loss: 0.013933230982639266
Epoch: 23 Idx: 0 Loss: 0.015775596734040442
Epoch: 23 Idx: 5000 Loss: 0.013584763689244215
Epoch: 24 Idx: 0 Loss: 0.020537842063814437
Epoch: 24 Idx: 5000 Loss: 0.013263223700072392
Epoch: 25 Idx: 0 Loss: 0.01410334605778289
Epoch: 25 Idx: 5000 Loss: 0.024210675459183314
Epoch: 26 Idx: 0 Loss: 0.01010625244762958
Epoch: 26 Idx: 5000 Loss: 0.024556887163946085
Epoch: 27 Idx: 0 Loss: 0.022615951843062608
Epoch: 27 Idx: 5000 Loss: 0.014528066681486285
Epoch: 28 Idx: 0 Loss: 0.014585700808907462
Epoch: 28 Idx: 5000 Loss: 0.011454624691755564
Epoch: 29 Idx: 0 Loss: 0.02236769394852067
Epoch: 29 Idx: 5000 Loss: 0.01323705428000805
Epoch: 30 Idx: 0 Loss: 0.019032306006569417
Epoch: 30 Idx: 5000 Loss: 0.026232632658781417
Epoch: 31 Idx: 0 Loss: 0.01996586705271888
Epoch: 31 Idx: 5000 Loss: 0.02590033043845779
Epoch: 32 Idx: 0 Loss: 0.009872536585328311
Epoch: 32 Idx: 5000 Loss: 0.020014717143800796
Epoch: 33 Idx: 0 Loss: 0.006451284931395856
Traceback (most recent call last):
  File "main.py", line 514, in <module>
    outputs = model(node_elems, inp_elems)
  File "/u/harshitk/miniconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "main.py", line 312, in forward
    feature_emb = self.name_embedding(features[i]) #  dim: (2, batch_size, max_types, max_paths, max_pathlen, 512)
  File "/u/harshitk/miniconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/u/harshitk/miniconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/sparse.py", line 126, in forward
    self.norm_type, self.scale_grad_by_freq, self.sparse)
  File "/u/harshitk/miniconda3/envs/py37/lib/python3.7/site-packages/torch/nn/functional.py", line 1814, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
KeyboardInterrupt

------------------------------------------------------------
Sender: LSF System <rer@dccxc207>
Subject: Job 4066852: <python main.py 5 5 False True> in cluster <dcc> Exited

Job <python main.py 5 5 False True> was submitted from host <dccxl004> by user <harshitk> in cluster <dcc> at Tue Sep 15 15:48:38 2020
Job was executed on host(s) <dccxc207>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep 15 15:48:39 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/VeeAlign/src> was used as the working directory.
Started at Tue Sep 15 15:48:39 2020
Terminated at Wed Sep 16 04:38:39 2020
Results reported at Wed Sep 16 04:38:39 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python main.py 5 5 False True
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   46098.96 sec.
    Max Memory :                                 2913 MB
    Average Memory :                             2748.59 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40504.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                13
    Run time :                                   46200 sec.
    Turnaround time :                            46201 sec.

The output (if any) is above this job summary.

