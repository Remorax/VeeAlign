2020-09-15 15:49:39.041490: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-09-15 15:49:42.169259: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1
2020-09-15 15:49:42.289617: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:14:00.0 name: Tesla K80 computeCapability: 3.7
coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s
2020-09-15 15:49:42.289718: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-09-15 15:49:42.291813: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
2020-09-15 15:49:42.293280: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2020-09-15 15:49:42.293701: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2020-09-15 15:49:42.295533: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2020-09-15 15:49:42.296871: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10
2020-09-15 15:49:42.297133: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudnn.so.7'; dlerror: libcudnn.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/share/lsf-9.1.3/10.1/linux3.10-glibc2.17-x86_64/lib:=/opt/share/gcc-4.9.2_rhel6/x86_64/lib/:/opt/share/gcc-4.9.2_rhel6/x86_64/lib64:/opt/share/Python-3.6.2/x86_64/lib:=/opt/share/gcc-5.4.0/x86_64/lib/:/opt/share/gcc-5.4.0/x86_64/lib64:/opt/share/isl-0.17/x86_64/lib/:/opt/share/protobuf-3.1.0/x86_64/lib/:/opt/share/leveldb-1.19/x86_64/lib/:/opt/share/boost-1.62.0/x86_64/lib/:/opt/share/torch-7/x86_64/install/lib:/opt/share/Python-2.7.12/x86_64/lib:/opt/share/Python-3.5.2/x86_64/lib:/opt/share/cuDNN-v5.1-8.0/cuda/lib64:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/opt/share/cuda-8.0/
2020-09-15 15:49:42.297157: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1753] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2020-09-15 15:49:42.297472: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-09-15 15:49:42.304970: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2600055000 Hz
2020-09-15 15:49:42.305155: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c8926630e0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-09-15 15:49:42.305175: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-09-15 15:49:42.307120: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-09-15 15:49:42.307145: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      
Prefix path:  /dccstor/cogfin/arvind/da/VeeAlign/
Ontologies being aligned are:  [('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/confOf.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/conference.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/conference.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/conference.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/conference.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/confOf.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/conference.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/confOf.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/confOf.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/confOf.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl')]
Total number of extracted unique classes and properties from entire RA set:  829
Constructing abbrevation resolution dict....
Results after abbreviation resolution:  {'PC': 'Program Committee', 'OC': 'Organizing Committee'}
Resolving abbreviations...
Number of entities: 119639
Starting sliding window evaluation...
Step 0/7
Val onto:  [('confof', 'ekaw')] test_onto:  [('conference', 'sigkdd')]
Training size: 112565 Testing size: 3871
Epoch: 0 Idx: 0 Loss: 0.1885269360330324
Epoch: 0 Idx: 5000 Loss: 0.004744612795484068
Epoch: 1 Idx: 0 Loss: 0.024497152064552566
Epoch: 1 Idx: 5000 Loss: 0.01570236650803378
Epoch: 2 Idx: 0 Loss: 0.01167389868743816
Epoch: 2 Idx: 5000 Loss: 0.01236398951545387
Epoch: 3 Idx: 0 Loss: 0.019147195324106157
Epoch: 3 Idx: 5000 Loss: 0.006846355164099278
Epoch: 4 Idx: 0 Loss: 0.019089945320199413
Epoch: 4 Idx: 5000 Loss: 0.01225532199217346
Epoch: 5 Idx: 0 Loss: 0.02544032906344551
Epoch: 5 Idx: 5000 Loss: 0.010926008701368808
Epoch: 6 Idx: 0 Loss: 0.012585175128039645
Epoch: 6 Idx: 5000 Loss: 0.01630915868178862
Epoch: 7 Idx: 0 Loss: 0.01991118424884418
Epoch: 7 Idx: 5000 Loss: 0.010629893733488535
Epoch: 8 Idx: 0 Loss: 0.007255518110504736
Epoch: 8 Idx: 5000 Loss: 0.008415698567490089
Epoch: 9 Idx: 0 Loss: 0.03844498437714609
Epoch: 9 Idx: 5000 Loss: 0.013946658983524565
Epoch: 10 Idx: 0 Loss: 0.008446388870233409
Epoch: 10 Idx: 5000 Loss: 0.011945128936426845
Epoch: 11 Idx: 0 Loss: 0.020265126718163065
Epoch: 11 Idx: 5000 Loss: 0.018740555722802966
Epoch: 12 Idx: 0 Loss: 0.00851987940015599
Epoch: 12 Idx: 5000 Loss: 0.012618070196923625
Epoch: 13 Idx: 0 Loss: 0.011920249473620578
Epoch: 13 Idx: 5000 Loss: 0.008323439615607447
Epoch: 14 Idx: 0 Loss: 0.03729657231002646
Epoch: 14 Idx: 5000 Loss: 0.01031190570116398
Epoch: 15 Idx: 0 Loss: 0.01099231896550046
Epoch: 15 Idx: 5000 Loss: 0.015079781965886366
Epoch: 16 Idx: 0 Loss: 0.0048911176411370865
Epoch: 16 Idx: 5000 Loss: 0.007708471188673393
Epoch: 17 Idx: 0 Loss: 0.008555261190659405
Epoch: 17 Idx: 5000 Loss: 0.012950803825375666
Epoch: 18 Idx: 0 Loss: 0.01751570149314779
Epoch: 18 Idx: 5000 Loss: 0.014214045769449176
Epoch: 19 Idx: 0 Loss: 0.024462696755764665
Epoch: 19 Idx: 5000 Loss: 0.008598137886261245
Epoch: 20 Idx: 0 Loss: 0.013123710370913111
Epoch: 20 Idx: 5000 Loss: 0.028412486305837772
Epoch: 21 Idx: 0 Loss: 0.015099516828876114
Epoch: 21 Idx: 5000 Loss: 0.02822579807978986
Epoch: 22 Idx: 0 Loss: 0.012282977371514972
Epoch: 22 Idx: 5000 Loss: 0.01007019590663862
Epoch: 23 Idx: 0 Loss: 0.022069981288443893
Epoch: 23 Idx: 5000 Loss: 0.01589102561525727
Epoch: 24 Idx: 0 Loss: 0.006753501569382099
Epoch: 24 Idx: 5000 Loss: 0.015235412403895312
Epoch: 25 Idx: 0 Loss: 0.01028501962821099
Epoch: 25 Idx: 5000 Loss: 0.012256533514258035
Epoch: 26 Idx: 0 Loss: 0.009459257978452873
Epoch: 26 Idx: 5000 Loss: 0.022571657326013017
Epoch: 27 Idx: 0 Loss: 0.018540137290164564
Epoch: 27 Idx: 5000 Loss: 0.03185521410791522
Epoch: 28 Idx: 0 Loss: 0.011538464251519606
Epoch: 28 Idx: 5000 Loss: 0.018836966873069814
Epoch: 29 Idx: 0 Loss: 0.013843166424412936
Epoch: 29 Idx: 5000 Loss: 0.00883407467253016
Epoch: 30 Idx: 0 Loss: 0.005206511754007201
Epoch: 30 Idx: 5000 Loss: 0.0074273181324678375
Epoch: 31 Idx: 0 Loss: 0.025292358339102303
Epoch: 31 Idx: 5000 Loss: 0.009700351326483472
Epoch: 32 Idx: 0 Loss: 0.023044689903430532
Epoch: 32 Idx: 5000 Loss: 0.009915799581901297
Epoch: 33 Idx: 0 Loss: 0.024459797837123626
Epoch: 33 Idx: 5000 Loss: 0.007921686050442717
Epoch: 34 Idx: 0 Loss: 0.013364627110137764
Epoch: 34 Idx: 5000 Loss: 0.01371181010382352
Epoch: 35 Idx: 0 Loss: 0.02216445342529341
Epoch: 35 Idx: 5000 Loss: 0.013392882647879967
Epoch: 36 Idx: 0 Loss: 0.017594276678851217
Epoch: 36 Idx: 5000 Loss: 0.03125360997734573
Epoch: 37 Idx: 0 Loss: 0.011642103263124606
Epoch: 37 Idx: 5000 Loss: 0.01963442170010131
Epoch: 38 Idx: 0 Loss: 0.0392257552379358
Epoch: 38 Idx: 5000 Loss: 0.014779411485650835
Epoch: 39 Idx: 0 Loss: 0.014370275291279927
Epoch: 39 Idx: 5000 Loss: 0.019106445079416966
Epoch: 40 Idx: 0 Loss: 0.011769442355762902
Epoch: 40 Idx: 5000 Loss: 0.014065608588283495
Epoch: 41 Idx: 0 Loss: 0.012419785469271565
Epoch: 41 Idx: 5000 Loss: 0.012407925906418583
Epoch: 42 Idx: 0 Loss: 0.009983855215145207
Epoch: 42 Idx: 5000 Loss: 0.008052998042623638
Epoch: 43 Idx: 0 Loss: 0.02074598971853496
Epoch: 43 Idx: 5000 Loss: 0.02062659807809186
Epoch: 44 Idx: 0 Loss: 0.028402362283777523
Epoch: 44 Idx: 5000 Loss: 0.023272899193322892
Epoch: 45 Idx: 0 Loss: 0.013084912224890085
Epoch: 45 Idx: 5000 Loss: 0.01754735876403592
Epoch: 46 Idx: 0 Loss: 0.017956824196198234
Epoch: 46 Idx: 5000 Loss: 0.038295468525604844
Epoch: 47 Idx: 0 Loss: 0.011359805227164786
Epoch: 47 Idx: 5000 Loss: 0.00944145142783635
Epoch: 48 Idx: 0 Loss: 0.004589969332216948
Epoch: 48 Idx: 5000 Loss: 0.018701896160349302
Epoch: 49 Idx: 0 Loss: 0.016404532920743554
Epoch: 49 Idx: 5000 Loss: 0.014149606638793274
Len (direct inputs):  429
Inputs len 2744 15 3856
Len (direct inputs):  1127
Starting sliding window evaluation...
Step 2/7
Val onto:  [('conference', 'ekaw')] test_onto:  [('cmt', 'sigkdd')]
Training size: 111450 Testing size: 2364
Epoch: 0 Idx: 0 Loss: 0.1283912857638167
Epoch: 0 Idx: 5000 Loss: 0.011657614168994586
Epoch: 1 Idx: 0 Loss: 0.01857000655652035
Epoch: 1 Idx: 5000 Loss: 0.033317475217088055
Epoch: 2 Idx: 0 Loss: 0.032907103660921116
Epoch: 2 Idx: 5000 Loss: 0.013180595668548336
Epoch: 3 Idx: 0 Loss: 0.03873136131478529
Epoch: 3 Idx: 5000 Loss: 0.007388173439094516
Epoch: 4 Idx: 0 Loss: 0.01144968425113611
Epoch: 4 Idx: 5000 Loss: 0.014570822016915657
Epoch: 5 Idx: 0 Loss: 0.007982253155101466
Epoch: 5 Idx: 5000 Loss: 0.02792449652114331
Epoch: 6 Idx: 0 Loss: 0.026172308478172927
Epoch: 6 Idx: 5000 Loss: 0.016934229702834223
Epoch: 7 Idx: 0 Loss: 0.009884610630941417
Epoch: 7 Idx: 5000 Loss: 0.01700342152601879
Epoch: 8 Idx: 0 Loss: 0.019191632327102807
Epoch: 8 Idx: 5000 Loss: 0.01986327560083548
Epoch: 9 Idx: 0 Loss: 0.009263213164442029
Epoch: 9 Idx: 5000 Loss: 0.01840511980848674
Epoch: 10 Idx: 0 Loss: 0.010556011752661952
Epoch: 10 Idx: 5000 Loss: 0.015278456387570343
Epoch: 11 Idx: 0 Loss: 0.010530197235959111
Epoch: 11 Idx: 5000 Loss: 0.010203270847759204
Epoch: 12 Idx: 0 Loss: 0.013132588497078764
Epoch: 12 Idx: 5000 Loss: 0.009652268118995685
Epoch: 13 Idx: 0 Loss: 0.007904581112276404
Epoch: 13 Idx: 5000 Loss: 0.009328467785511264
Epoch: 14 Idx: 0 Loss: 0.01097074181405804
Epoch: 14 Idx: 5000 Loss: 0.015713286606153162
Epoch: 15 Idx: 0 Loss: 0.009849779396722623
Epoch: 15 Idx: 5000 Loss: 0.011808155196455053
Epoch: 16 Idx: 0 Loss: 0.02262117016364782
Epoch: 16 Idx: 5000 Loss: 0.035620065369361825
Epoch: 17 Idx: 0 Loss: 0.025655816179673257
Epoch: 17 Idx: 5000 Loss: 0.015250897621844076
Epoch: 18 Idx: 0 Loss: 0.00863513316322136
Epoch: 18 Idx: 5000 Loss: 0.014046119801065595
Epoch: 19 Idx: 0 Loss: 0.01646307624554939
Epoch: 19 Idx: 5000 Loss: 0.01485848513519767
Epoch: 20 Idx: 0 Loss: 0.04587497952357153
Epoch: 20 Idx: 5000 Loss: 0.024011984999086352
Epoch: 21 Idx: 0 Loss: 0.014750210903708656
Epoch: 21 Idx: 5000 Loss: 0.02216981956664562
Epoch: 22 Idx: 0 Loss: 0.012612802022475
Epoch: 22 Idx: 5000 Loss: 0.005947155597039056
Epoch: 23 Idx: 0 Loss: 0.007319042552293602
Epoch: 23 Idx: 5000 Loss: 0.009745452241612074
Epoch: 24 Idx: 0 Loss: 0.012173218623790047
Epoch: 24 Idx: 5000 Loss: 0.03818867147877194
Epoch: 25 Idx: 0 Loss: 0.02507556632795405
Epoch: 25 Idx: 5000 Loss: 0.039603040357478775
Epoch: 26 Idx: 0 Loss: 0.016180081143214776
Epoch: 26 Idx: 5000 Loss: 0.01010111146573358
Epoch: 27 Idx: 0 Loss: 0.0147069024786387
Epoch: 27 Idx: 5000 Loss: 0.019237726886573735
Epoch: 28 Idx: 0 Loss: 0.036190633670318016
Epoch: 28 Idx: 5000 Loss: 0.008853858231689247
Epoch: 29 Idx: 0 Loss: 0.01206228779465506
Epoch: 29 Idx: 5000 Loss: 0.025061431677262755
Epoch: 30 Idx: 0 Loss: 0.028294592741390012
Epoch: 30 Idx: 5000 Loss: 0.03845907795489542
Epoch: 31 Idx: 0 Loss: 0.031074092410815133
Epoch: 31 Idx: 5000 Loss: 0.008830527956265951
Epoch: 32 Idx: 0 Loss: 0.016754239862037196
Epoch: 32 Idx: 5000 Loss: 0.01671882662093628
Epoch: 33 Idx: 0 Loss: 0.020510210939384595
Epoch: 33 Idx: 5000 Loss: 0.02370239700299472
Epoch: 34 Idx: 0 Loss: 0.016879060670807848
Epoch: 34 Idx: 5000 Loss: 0.007082109282906826
Epoch: 35 Idx: 0 Loss: 0.016620970202982425
Epoch: 35 Idx: 5000 Loss: 0.008136907712212978
Epoch: 36 Idx: 0 Loss: 0.027288706694610088
Epoch: 36 Idx: 5000 Loss: 0.022808073696022942
Epoch: 37 Idx: 0 Loss: 0.002728620103650981
Epoch: 37 Idx: 5000 Loss: 0.009710883824005815
Epoch: 38 Idx: 0 Loss: 0.017358071974370982
Epoch: 38 Idx: 5000 Loss: 0.013237647647210821
Epoch: 39 Idx: 0 Loss: 0.016709570269236945
Epoch: 39 Idx: 5000 Loss: 0.02388490726735333
Epoch: 40 Idx: 0 Loss: 0.007719324664214631
Epoch: 40 Idx: 5000 Loss: 0.014737938971837602
Epoch: 41 Idx: 0 Loss: 0.014145854465575588
Epoch: 41 Idx: 5000 Loss: 0.008719019731304501
Epoch: 42 Idx: 0 Loss: 0.014911145459372596
Epoch: 42 Idx: 5000 Loss: 0.009350960362097809
Epoch: 43 Idx: 0 Loss: 0.026134730435823126
Epoch: 43 Idx: 5000 Loss: 0.01710514985880158
Epoch: 44 Idx: 0 Loss: 0.011333854763925077
Epoch: 44 Idx: 5000 Loss: 0.031246189958569027
Epoch: 45 Idx: 0 Loss: 0.0081763232272303
Epoch: 45 Idx: 5000 Loss: 0.018940638278327624
Epoch: 46 Idx: 0 Loss: 0.012122209133972177
Epoch: 46 Idx: 5000 Loss: 0.014032835243665625
Epoch: 47 Idx: 0 Loss: 0.00896483398774713
Epoch: 47 Idx: 5000 Loss: 0.02425139668506552
Epoch: 48 Idx: 0 Loss: 0.011200547688463298
Epoch: 48 Idx: 5000 Loss: 0.018722140787447248
Epoch: 49 Idx: 0 Loss: 0.014944815670893244
Epoch: 49 Idx: 5000 Loss: 0.01092206994444112
Len (direct inputs):  1737
Inputs len 1372 12 2352
Len (direct inputs):  992
Starting sliding window evaluation...
Step 4/7
Val onto:  [('ekaw', 'sigkdd')] test_onto:  [('cmt', 'conference')]
Training size: 111356 Testing size: 4145
Epoch: 0 Idx: 0 Loss: 0.16585057466675093
Epoch: 0 Idx: 5000 Loss: 0.009416921271193736
Epoch: 1 Idx: 0 Loss: 0.01829987029731215
Epoch: 1 Idx: 5000 Loss: 0.00704693724044185
Epoch: 2 Idx: 0 Loss: 0.03364389357690247
Epoch: 2 Idx: 5000 Loss: 0.01305848909570043
Epoch: 3 Idx: 0 Loss: 0.014644593331268293
Epoch: 3 Idx: 5000 Loss: 0.009744531894549605
Epoch: 4 Idx: 0 Loss: 0.010592794575990796
Epoch: 4 Idx: 5000 Loss: 0.019146226631086248
Epoch: 5 Idx: 0 Loss: 0.017776151554862115
Traceback (most recent call last):
  File "main.py", line 514, in <module>
    outputs = model(node_elems, inp_elems)
  File "/u/harshitk/miniconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "main.py", line 312, in forward
    feature_emb = self.name_embedding(features[i]) #  dim: (2, batch_size, max_types, max_paths, max_pathlen, 512)
  File "/u/harshitk/miniconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/u/harshitk/miniconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/sparse.py", line 126, in forward
    self.norm_type, self.scale_grad_by_freq, self.sparse)
  File "/u/harshitk/miniconda3/envs/py37/lib/python3.7/site-packages/torch/nn/functional.py", line 1814, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
KeyboardInterrupt

------------------------------------------------------------
Sender: LSF System <rer@dccxc238>
Subject: Job 4066887: <python main.py 6 18 False False> in cluster <dcc> Exited

Job <python main.py 6 18 False False> was submitted from host <dccxl004> by user <harshitk> in cluster <dcc> at Tue Sep 15 15:48:40 2020
Job was executed on host(s) <dccxc238>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep 15 15:49:37 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/VeeAlign/src> was used as the working directory.
Started at Tue Sep 15 15:49:37 2020
Terminated at Wed Sep 16 04:38:38 2020
Results reported at Wed Sep 16 04:38:38 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python main.py 6 18 False False
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   46082.60 sec.
    Max Memory :                                 3001 MB
    Average Memory :                             2762.65 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40416.00 MB
    Max Swap :                                   2 MB
    Max Processes :                              3
    Max Threads :                                13
    Run time :                                   46167 sec.
    Turnaround time :                            46198 sec.

The output (if any) is above this job summary.

