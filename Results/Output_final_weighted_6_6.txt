2020-09-15 15:49:38.949503: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-09-15 15:49:42.069594: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1
2020-09-15 15:49:42.187761: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:1e:00.0 name: Tesla K80 computeCapability: 3.7
coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s
2020-09-15 15:49:42.187847: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-09-15 15:49:42.189801: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
2020-09-15 15:49:42.191166: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2020-09-15 15:49:42.191989: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2020-09-15 15:49:42.193863: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2020-09-15 15:49:42.195293: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10
2020-09-15 15:49:42.195513: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudnn.so.7'; dlerror: libcudnn.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/share/lsf-9.1.3/10.1/linux3.10-glibc2.17-x86_64/lib:=/opt/share/gcc-4.9.2_rhel6/x86_64/lib/:/opt/share/gcc-4.9.2_rhel6/x86_64/lib64:/opt/share/Python-3.6.2/x86_64/lib:=/opt/share/gcc-5.4.0/x86_64/lib/:/opt/share/gcc-5.4.0/x86_64/lib64:/opt/share/isl-0.17/x86_64/lib/:/opt/share/protobuf-3.1.0/x86_64/lib/:/opt/share/leveldb-1.19/x86_64/lib/:/opt/share/boost-1.62.0/x86_64/lib/:/opt/share/torch-7/x86_64/install/lib:/opt/share/Python-2.7.12/x86_64/lib:/opt/share/Python-3.5.2/x86_64/lib:/opt/share/cuDNN-v5.1-8.0/cuda/lib64:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/opt/share/cuda-8.0/
2020-09-15 15:49:42.195534: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1753] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2020-09-15 15:49:42.195851: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-09-15 15:49:42.202929: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2600120000 Hz
2020-09-15 15:49:42.203133: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55aaeb50f700 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-09-15 15:49:42.203153: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-09-15 15:49:42.205014: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-09-15 15:49:42.205051: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      
Prefix path:  /dccstor/cogfin/arvind/da/VeeAlign/
Ontologies being aligned are:  [('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/confOf.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/conference.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/conference.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/conference.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/conference.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/confOf.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/conference.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/confOf.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/confOf.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/confOf.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl')]
Total number of extracted unique classes and properties from entire RA set:  829
Constructing abbrevation resolution dict....
Results after abbreviation resolution:  {'PC': 'Program Committee', 'OC': 'Organizing Committee'}
Resolving abbreviations...
Number of entities: 119639
Starting sliding window evaluation...
Step 0/7
Val onto:  [('confof', 'ekaw')] test_onto:  [('conference', 'sigkdd')]
Training size: 112565 Testing size: 3871
Epoch: 0 Idx: 0 Loss: 0.18256385944998915
Epoch: 0 Idx: 5000 Loss: 0.010647784179654349
Epoch: 1 Idx: 0 Loss: 0.011368516154226568
Epoch: 1 Idx: 5000 Loss: 0.012302037590765888
Epoch: 2 Idx: 0 Loss: 0.012402371306082646
Epoch: 2 Idx: 5000 Loss: 0.02732109640055551
Epoch: 3 Idx: 0 Loss: 0.045879883485470514
Epoch: 3 Idx: 5000 Loss: 0.0237099176974618
Epoch: 4 Idx: 0 Loss: 0.009156566407088887
Epoch: 4 Idx: 5000 Loss: 0.009919762990317191
Epoch: 5 Idx: 0 Loss: 0.03685169151808235
Epoch: 5 Idx: 5000 Loss: 0.016848600478000707
Epoch: 6 Idx: 0 Loss: 0.025211364838866525
Epoch: 6 Idx: 5000 Loss: 0.015360642355503586
Epoch: 7 Idx: 0 Loss: 0.011015948513369818
Epoch: 7 Idx: 5000 Loss: 0.010812050220430776
Epoch: 8 Idx: 0 Loss: 0.01766946543892744
Epoch: 8 Idx: 5000 Loss: 0.015297617689651516
Epoch: 9 Idx: 0 Loss: 0.006621284061283461
Epoch: 9 Idx: 5000 Loss: 0.011928629167208271
Epoch: 10 Idx: 0 Loss: 0.011801256929738045
Epoch: 10 Idx: 5000 Loss: 0.03421119073907686
Epoch: 11 Idx: 0 Loss: 0.024691447623482597
Epoch: 11 Idx: 5000 Loss: 0.011912841254025607
Epoch: 12 Idx: 0 Loss: 0.014487476870580876
Epoch: 12 Idx: 5000 Loss: 0.019683333920859396
Epoch: 13 Idx: 0 Loss: 0.011654392960188296
Epoch: 13 Idx: 5000 Loss: 0.01785850724493866
Epoch: 14 Idx: 0 Loss: 0.03067736594142186
Epoch: 14 Idx: 5000 Loss: 0.02466238518125939
Epoch: 15 Idx: 0 Loss: 0.023521864042909686
Epoch: 15 Idx: 5000 Loss: 0.011512062549374001
Epoch: 16 Idx: 0 Loss: 0.012353956760994142
Epoch: 16 Idx: 5000 Loss: 0.009661832000251645
Epoch: 17 Idx: 0 Loss: 0.01161896759480487
Epoch: 17 Idx: 5000 Loss: 0.010240813093902412
Epoch: 18 Idx: 0 Loss: 0.026231894143906802
Epoch: 18 Idx: 5000 Loss: 0.016997551499075652
Epoch: 19 Idx: 0 Loss: 0.02045005700016809
Epoch: 19 Idx: 5000 Loss: 0.025738350522925862
Epoch: 20 Idx: 0 Loss: 0.012827020566921737
Epoch: 20 Idx: 5000 Loss: 0.01151494987877995
Epoch: 21 Idx: 0 Loss: 0.010388931474869523
Epoch: 21 Idx: 5000 Loss: 0.04184336310297975
Epoch: 22 Idx: 0 Loss: 0.016551155554313364
Epoch: 22 Idx: 5000 Loss: 0.01788608814353389
Epoch: 23 Idx: 0 Loss: 0.015132313838141488
Epoch: 23 Idx: 5000 Loss: 0.022085575496708608
Epoch: 24 Idx: 0 Loss: 0.025643032723625454
Epoch: 24 Idx: 5000 Loss: 0.017712769511941923
Epoch: 25 Idx: 0 Loss: 0.016426352383819107
Epoch: 25 Idx: 5000 Loss: 0.026357698076602577
Epoch: 26 Idx: 0 Loss: 0.036244464066993595
Epoch: 26 Idx: 5000 Loss: 0.01247463074235192
Epoch: 27 Idx: 0 Loss: 0.01169129250376308
Epoch: 27 Idx: 5000 Loss: 0.01447444331221941
Epoch: 28 Idx: 0 Loss: 0.012140659489138849
Epoch: 28 Idx: 5000 Loss: 0.023511505500187303
Epoch: 29 Idx: 0 Loss: 0.012977280787305248
Epoch: 29 Idx: 5000 Loss: 0.019485347419955105
Epoch: 30 Idx: 0 Loss: 0.03149431511089886
Epoch: 30 Idx: 5000 Loss: 0.004940730491919681
Epoch: 31 Idx: 0 Loss: 0.021223255665714945
Epoch: 31 Idx: 5000 Loss: 0.009471405756494574
Epoch: 32 Idx: 0 Loss: 0.009477474994825874
Epoch: 32 Idx: 5000 Loss: 0.03466131698284424
Epoch: 33 Idx: 0 Loss: 0.00908442079088466
Epoch: 33 Idx: 5000 Loss: 0.015096485574367567
Epoch: 34 Idx: 0 Loss: 0.029514276175938655
Epoch: 34 Idx: 5000 Loss: 0.02403814442787088
Epoch: 35 Idx: 0 Loss: 0.017983467439784878
Epoch: 35 Idx: 5000 Loss: 0.027139144546506816
Epoch: 36 Idx: 0 Loss: 0.02443163040862739
Epoch: 36 Idx: 5000 Loss: 0.010916108415090135
Epoch: 37 Idx: 0 Loss: 0.015676427596174988
Epoch: 37 Idx: 5000 Loss: 0.006343786709454021
Epoch: 38 Idx: 0 Loss: 0.01471900550320375
Epoch: 38 Idx: 5000 Loss: 0.019901453162433896
Epoch: 39 Idx: 0 Loss: 0.012339910297582022
Epoch: 39 Idx: 5000 Loss: 0.012174578871451054
Epoch: 40 Idx: 0 Loss: 0.005330799978023557
Epoch: 40 Idx: 5000 Loss: 0.008831934578797601
Epoch: 41 Idx: 0 Loss: 0.015243832751548347
Epoch: 41 Idx: 5000 Loss: 0.020241391764550507
Epoch: 42 Idx: 0 Loss: 0.012851406034790422
Epoch: 42 Idx: 5000 Loss: 0.00647798942588484
Epoch: 43 Idx: 0 Loss: 0.010847235916540276
Epoch: 43 Idx: 5000 Loss: 0.012448571122567274
Epoch: 44 Idx: 0 Loss: 0.009195954246391693
Epoch: 44 Idx: 5000 Loss: 0.04451332520262479
Epoch: 45 Idx: 0 Loss: 0.008028924169458603
Epoch: 45 Idx: 5000 Loss: 0.025901044491662514
Epoch: 46 Idx: 0 Loss: 0.011559132132417563
Epoch: 46 Idx: 5000 Loss: 0.014636542553924655
Epoch: 47 Idx: 0 Loss: 0.008773330280754638
Epoch: 47 Idx: 5000 Loss: 0.028605782915371525
Epoch: 48 Idx: 0 Loss: 0.012768360722306875
Epoch: 48 Idx: 5000 Loss: 0.021779940155443413
Epoch: 49 Idx: 0 Loss: 0.017362794794811568
Epoch: 49 Idx: 5000 Loss: 0.020736879517112446
Len (direct inputs):  429
Inputs len 2744 15 3856
Len (direct inputs):  1127
Starting sliding window evaluation...
Step 2/7
Val onto:  [('conference', 'ekaw')] test_onto:  [('cmt', 'sigkdd')]
Training size: 111450 Testing size: 2364
Epoch: 0 Idx: 0 Loss: 0.1420549630350476
Epoch: 0 Idx: 5000 Loss: 0.02680750696355358
Epoch: 1 Idx: 0 Loss: 0.017711446728561415
Epoch: 1 Idx: 5000 Loss: 0.019004860423539452
Epoch: 2 Idx: 0 Loss: 0.01628291042093077
Epoch: 2 Idx: 5000 Loss: 0.011675498783413295
Epoch: 3 Idx: 0 Loss: 0.009843270346419346
Epoch: 3 Idx: 5000 Loss: 0.02466003743115817
Epoch: 4 Idx: 0 Loss: 0.02325634800105569
Epoch: 4 Idx: 5000 Loss: 0.015618396518134153
Epoch: 5 Idx: 0 Loss: 0.016324403953908002
Epoch: 5 Idx: 5000 Loss: 0.018009589699208784
Epoch: 6 Idx: 0 Loss: 0.011381836497945951
Epoch: 6 Idx: 5000 Loss: 0.019586609006995104
Epoch: 7 Idx: 0 Loss: 0.008765100545955636
Epoch: 7 Idx: 5000 Loss: 0.011036577213437237
Epoch: 8 Idx: 0 Loss: 0.0212314029325672
Epoch: 8 Idx: 5000 Loss: 0.01690223731177577
Epoch: 9 Idx: 0 Loss: 0.003942706202704039
Epoch: 9 Idx: 5000 Loss: 0.008212146926020594
Epoch: 10 Idx: 0 Loss: 0.009903830579291447
Epoch: 10 Idx: 5000 Loss: 0.041373066235537825
Epoch: 11 Idx: 0 Loss: 0.01725454284041111
Epoch: 11 Idx: 5000 Loss: 0.010097636150233116
Epoch: 12 Idx: 0 Loss: 0.019542730202399203
Epoch: 12 Idx: 5000 Loss: 0.02144195010216677
Epoch: 13 Idx: 0 Loss: 0.011206155026845376
Epoch: 13 Idx: 5000 Loss: 0.01643415455244951
Epoch: 14 Idx: 0 Loss: 0.01326146704459611
Epoch: 14 Idx: 5000 Loss: 0.006013789462019807
Epoch: 15 Idx: 0 Loss: 0.008263472821879293
Epoch: 15 Idx: 5000 Loss: 0.010324661046119659
Epoch: 16 Idx: 0 Loss: 0.0179562761180075
Epoch: 16 Idx: 5000 Loss: 0.016951882949652036
Epoch: 17 Idx: 0 Loss: 0.012077432013964147
Epoch: 17 Idx: 5000 Loss: 0.02935247143154201
Epoch: 18 Idx: 0 Loss: 0.017882332794218206
Epoch: 18 Idx: 5000 Loss: 0.006333176953470571
Epoch: 19 Idx: 0 Loss: 0.03199147884939578
Epoch: 19 Idx: 5000 Loss: 0.007620197844141771
Epoch: 20 Idx: 0 Loss: 0.019011371191995946
Epoch: 20 Idx: 5000 Loss: 0.00643541470005329
Epoch: 21 Idx: 0 Loss: 0.02130006329384
Epoch: 21 Idx: 5000 Loss: 0.04667815143936567
Epoch: 22 Idx: 0 Loss: 0.03409453592760637
Epoch: 22 Idx: 5000 Loss: 0.00740668562619029
Epoch: 23 Idx: 0 Loss: 0.008671056932414895
Epoch: 23 Idx: 5000 Loss: 0.011694990831333074
Epoch: 24 Idx: 0 Loss: 0.009403434230449025
Epoch: 24 Idx: 5000 Loss: 0.015687527730450138
Epoch: 25 Idx: 0 Loss: 0.013245851672444128
Epoch: 25 Idx: 5000 Loss: 0.0094955933081923
Epoch: 26 Idx: 0 Loss: 0.009804081754802404
Epoch: 26 Idx: 5000 Loss: 0.009678797802924844
Epoch: 27 Idx: 0 Loss: 0.014149525850907993
Epoch: 27 Idx: 5000 Loss: 0.011198742177350226
Epoch: 28 Idx: 0 Loss: 0.025781300302202853
Epoch: 28 Idx: 5000 Loss: 0.009092744434702664
Epoch: 29 Idx: 0 Loss: 0.03374084775440321
Epoch: 29 Idx: 5000 Loss: 0.01773658984626532
Epoch: 30 Idx: 0 Loss: 0.03343009905402234
Epoch: 30 Idx: 5000 Loss: 0.009973433934606138
Epoch: 31 Idx: 0 Loss: 0.029274161396156108
Epoch: 31 Idx: 5000 Loss: 0.015442538703560216
Epoch: 32 Idx: 0 Loss: 0.01011441528913383
Epoch: 32 Idx: 5000 Loss: 0.013866697045853775
Epoch: 33 Idx: 0 Loss: 0.02225526655213622
Epoch: 33 Idx: 5000 Loss: 0.011347191221789973
Epoch: 34 Idx: 0 Loss: 0.013515470998598218
Epoch: 34 Idx: 5000 Loss: 0.011295384256292788
Epoch: 35 Idx: 0 Loss: 0.01082171367972665
Epoch: 35 Idx: 5000 Loss: 0.012341491598505165
Epoch: 36 Idx: 0 Loss: 0.009360232126263524
Epoch: 36 Idx: 5000 Loss: 0.008645963745622764
Epoch: 37 Idx: 0 Loss: 0.0067777973007612175
Epoch: 37 Idx: 5000 Loss: 0.014578202457218165
Epoch: 38 Idx: 0 Loss: 0.01700220804740013
Epoch: 38 Idx: 5000 Loss: 0.01636617306332057
Epoch: 39 Idx: 0 Loss: 0.021070370417263343
Epoch: 39 Idx: 5000 Loss: 0.010592956660892206
Epoch: 40 Idx: 0 Loss: 0.03431913324974023
Epoch: 40 Idx: 5000 Loss: 0.017245885213460842
Epoch: 41 Idx: 0 Loss: 0.012990684209254356
Epoch: 41 Idx: 5000 Loss: 0.011021944971173129
Epoch: 42 Idx: 0 Loss: 0.009124984464684228
Epoch: 42 Idx: 5000 Loss: 0.014903499541476555
Epoch: 43 Idx: 0 Loss: 0.018076650049963314
Epoch: 43 Idx: 5000 Loss: 0.009669734781781668
Epoch: 44 Idx: 0 Loss: 0.015901240085352486
Epoch: 44 Idx: 5000 Loss: 0.01309108029163222
Epoch: 45 Idx: 0 Loss: 0.023184211517891257
Epoch: 45 Idx: 5000 Loss: 0.015945374005253417
Epoch: 46 Idx: 0 Loss: 0.01120644658123511
Epoch: 46 Idx: 5000 Loss: 0.009052729366733764
Epoch: 47 Idx: 0 Loss: 0.013825534952491732
Epoch: 47 Idx: 5000 Loss: 0.020879944654187983
Epoch: 48 Idx: 0 Loss: 0.014674719260618307
Epoch: 48 Idx: 5000 Loss: 0.012029986289687605
Epoch: 49 Idx: 0 Loss: 0.015917073547160453
Epoch: 49 Idx: 5000 Loss: 0.014143884763901757
Len (direct inputs):  1737
Inputs len 1372 12 2352
Len (direct inputs):  992
Starting sliding window evaluation...
Step 4/7
Val onto:  [('ekaw', 'sigkdd')] test_onto:  [('cmt', 'conference')]
Training size: 111356 Testing size: 4145
Epoch: 0 Idx: 0 Loss: 0.12232898874119837
Epoch: 0 Idx: 5000 Loss: 0.025784925926300713
Epoch: 1 Idx: 0 Loss: 0.022956324911348046
Epoch: 1 Idx: 5000 Loss: 0.008487474915357703
Epoch: 2 Idx: 0 Loss: 0.019450630054462546
Epoch: 2 Idx: 5000 Loss: 0.021919138703205873
Epoch: 3 Idx: 0 Loss: 0.00956050001148877
Epoch: 3 Idx: 5000 Loss: 0.016668613722874037
Epoch: 4 Idx: 0 Loss: 0.01642247191962518
Epoch: 4 Idx: 5000 Loss: 0.009927972417935629
Epoch: 5 Idx: 0 Loss: 0.012522807126334237
Epoch: 5 Idx: 5000 Loss: 0.011342345144890995
Epoch: 6 Idx: 0 Loss: 0.023476707624188506
Epoch: 6 Idx: 5000 Loss: 0.015349688732261788
Epoch: 7 Idx: 0 Loss: 0.021989692322880037
Epoch: 7 Idx: 5000 Loss: 0.010608859662614708
Epoch: 8 Idx: 0 Loss: 0.010707554905743065
Epoch: 8 Idx: 5000 Loss: 0.016161947606938946
Epoch: 9 Idx: 0 Loss: 0.02615873572080878
Epoch: 9 Idx: 5000 Loss: 0.005788376279517321
Epoch: 10 Idx: 0 Loss: 0.012600753399038126
Epoch: 10 Idx: 5000 Loss: 0.016886649302439116
Epoch: 11 Idx: 0 Loss: 0.01695227516230962
Epoch: 11 Idx: 5000 Loss: 0.006893255816646415
Epoch: 12 Idx: 0 Loss: 0.01929104787842735
Epoch: 12 Idx: 5000 Loss: 0.02123038466092572
Epoch: 13 Idx: 0 Loss: 0.009356700917823332
Epoch: 13 Idx: 5000 Loss: 0.012852206700550135
Epoch: 14 Idx: 0 Loss: 0.02858014531202995
Epoch: 14 Idx: 5000 Loss: 0.0088175679188431
Epoch: 15 Idx: 0 Loss: 0.008247232577747992
Epoch: 15 Idx: 5000 Loss: 0.013542512284050963
Epoch: 16 Idx: 0 Loss: 0.018092361810150615
Epoch: 16 Idx: 5000 Loss: 0.042650471660026994
Epoch: 17 Idx: 0 Loss: 0.011350067699508774
Epoch: 17 Idx: 5000 Loss: 0.00723205564185873
Epoch: 18 Idx: 0 Loss: 0.0100097561705378
Epoch: 18 Idx: 5000 Loss: 0.007974371549569613
Epoch: 19 Idx: 0 Loss: 0.0066917749547377
Epoch: 19 Idx: 5000 Loss: 0.009205116900631423
Epoch: 20 Idx: 0 Loss: 0.004841646079196202
Epoch: 20 Idx: 5000 Loss: 0.012196523939451738
Epoch: 21 Idx: 0 Loss: 0.03839321349054629
Epoch: 21 Idx: 5000 Loss: 0.025407385464884493
Epoch: 22 Idx: 0 Loss: 0.01266575599453728
Epoch: 22 Idx: 5000 Loss: 0.021130971737444375
Epoch: 23 Idx: 0 Loss: 0.005836385969780313
Epoch: 23 Idx: 5000 Loss: 0.006821531117193189
Epoch: 24 Idx: 0 Loss: 0.010364367329612205
Epoch: 24 Idx: 5000 Loss: 0.01088320329300501
Epoch: 25 Idx: 0 Loss: 0.007470232458891726
Epoch: 25 Idx: 5000 Loss: 0.007769554702412489
Epoch: 26 Idx: 0 Loss: 0.020846491231456522
Epoch: 26 Idx: 5000 Loss: 0.007413904154827549
Epoch: 27 Idx: 0 Loss: 0.019364395614236012
Epoch: 27 Idx: 5000 Loss: 0.00861859653980378
Epoch: 28 Idx: 0 Loss: 0.020128125568506922
Epoch: 28 Idx: 5000 Loss: 0.015281610878318731
Epoch: 29 Idx: 0 Loss: 0.012801333078403268
Epoch: 29 Idx: 5000 Loss: 0.014904637937833958
Epoch: 30 Idx: 0 Loss: 0.011501869921514356
Epoch: 30 Idx: 5000 Loss: 0.006283699172186895
Epoch: 31 Idx: 0 Loss: 0.04787277355406173
Epoch: 31 Idx: 5000 Loss: 0.022539470319778036
Epoch: 32 Idx: 0 Loss: 0.007579946883449963
Epoch: 32 Idx: 5000 Loss: 0.018140672227726057
Epoch: 33 Idx: 0 Loss: 0.007572383821657547
Epoch: 33 Idx: 5000 Loss: 0.025317296063258367
Epoch: 34 Idx: 0 Loss: 0.016035099896446844
Epoch: 34 Idx: 5000 Loss: 0.00978024412915129
Epoch: 35 Idx: 0 Loss: 0.048020718387296026
Epoch: 35 Idx: 5000 Loss: 0.013896608449959363
Epoch: 36 Idx: 0 Loss: 0.016088993272965244
Epoch: 36 Idx: 5000 Loss: 0.006229634161625873
Epoch: 37 Idx: 0 Loss: 0.01658110900843651
Epoch: 37 Idx: 5000 Loss: 0.009142529913501138
Epoch: 38 Idx: 0 Loss: 0.028220736800892512
Epoch: 38 Idx: 5000 Loss: 0.012070260054670651
Epoch: 39 Idx: 0 Loss: 0.01413716226608817
Epoch: 39 Idx: 5000 Loss: 0.004828678470267338
Epoch: 40 Idx: 0 Loss: 0.02434354315819304
Epoch: 40 Idx: 5000 Loss: 0.009926449004624104
Epoch: 41 Idx: 0 Loss: 0.00969965564106719
Epoch: 41 Idx: 5000 Loss: 0.02208477686130339
Epoch: 42 Idx: 0 Loss: 0.0377105536696997
Epoch: 42 Idx: 5000 Loss: 0.010589286719301724
Epoch: 43 Idx: 0 Loss: 0.006074281525051558
Epoch: 43 Idx: 5000 Loss: 0.011277477578192252
Epoch: 44 Idx: 0 Loss: 0.02087138246490622
Epoch: 44 Idx: 5000 Loss: 0.014726979261672064
Epoch: 45 Idx: 0 Loss: 0.016786203397726386
Epoch: 45 Idx: 5000 Loss: 0.009254018965238834
Epoch: 46 Idx: 0 Loss: 0.015100005893012119
Epoch: 46 Idx: 5000 Loss: 0.01892007231448122
Epoch: 47 Idx: 0 Loss: 0.018638252698817963
Epoch: 47 Idx: 5000 Loss: 0.015645472906237908
Epoch: 48 Idx: 0 Loss: 0.016548467570348554
Epoch: 48 Idx: 5000 Loss: 0.014818896295997834
Epoch: 49 Idx: 0 Loss: 0.02777726337844892
Epoch: 49 Idx: 5000 Loss: 0.00856812843197549
Len (direct inputs):  561
Inputs len 1568 15 4130
Len (direct inputs):  2577
Starting sliding window evaluation...
Step 6/7
Val onto:  [('edas', 'sigkdd')] test_onto:  [('conference', 'edas')]
Training size: 106045 Testing size: 7817
Epoch: 0 Idx: 0 Loss: 0.21240854913748103
Epoch: 0 Idx: 5000 Loss: 0.01878798908874054
Epoch: 1 Idx: 0 Loss: 0.010584835421620234
Epoch: 1 Idx: 5000 Loss: 0.015119506301549882
Epoch: 2 Idx: 0 Loss: 0.01609354693332475
Epoch: 2 Idx: 5000 Loss: 0.04818047578192044
Epoch: 3 Idx: 0 Loss: 0.015707119143054792
Epoch: 3 Idx: 5000 Loss: 0.015951998427989895
Epoch: 4 Idx: 0 Loss: 0.03512514560909672
Epoch: 4 Idx: 5000 Loss: 0.024368502291277832
Epoch: 5 Idx: 0 Loss: 0.02014838456972618
Epoch: 5 Idx: 5000 Loss: 0.0069455209180905525
Epoch: 6 Idx: 0 Loss: 0.01116474212144019
Epoch: 6 Idx: 5000 Loss: 0.008206668210432895
Epoch: 7 Idx: 0 Loss: 0.01615019624835079
Epoch: 7 Idx: 5000 Loss: 0.055912784055509965
Epoch: 8 Idx: 0 Loss: 0.019008845032744227
Epoch: 8 Idx: 5000 Loss: 0.035345464582763546
Epoch: 9 Idx: 0 Loss: 0.009567357788943425
Epoch: 9 Idx: 5000 Loss: 0.02452078777686113
Epoch: 10 Idx: 0 Loss: 0.0096298802277312
Epoch: 10 Idx: 5000 Loss: 0.006053085678265247
Epoch: 11 Idx: 0 Loss: 0.024596738258375255
Epoch: 11 Idx: 5000 Loss: 0.018626483766940604
Epoch: 12 Idx: 0 Loss: 0.007887746022220872
Epoch: 12 Idx: 5000 Loss: 0.013132998044496486
Epoch: 13 Idx: 0 Loss: 0.005769213939405436
Epoch: 13 Idx: 5000 Loss: 0.007233213274385107
Epoch: 14 Idx: 0 Loss: 0.03751410094631875
Epoch: 14 Idx: 5000 Loss: 0.029566194766204673
Epoch: 15 Idx: 0 Loss: 0.005965591042963849
Epoch: 15 Idx: 5000 Loss: 0.009706860645597564
Epoch: 16 Idx: 0 Loss: 0.016911383273964284
Epoch: 16 Idx: 5000 Loss: 0.014858090890159832
Epoch: 17 Idx: 0 Loss: 0.010218588590246417
Epoch: 17 Idx: 5000 Loss: 0.010543198402847858
Epoch: 18 Idx: 0 Loss: 0.01352338190755349
Epoch: 18 Idx: 5000 Loss: 0.022164798228633778
Epoch: 19 Idx: 0 Loss: 0.00827607252890426
Epoch: 19 Idx: 5000 Loss: 0.012707229418818699
Epoch: 20 Idx: 0 Loss: 0.011946032759027902
Epoch: 20 Idx: 5000 Loss: 0.0162431408632477
Epoch: 21 Idx: 0 Loss: 0.015467634557704567
Epoch: 21 Idx: 5000 Loss: 0.023026719232163748
Epoch: 22 Idx: 0 Loss: 0.012215633987593767
Epoch: 22 Idx: 5000 Loss: 0.012049927314404857
Epoch: 23 Idx: 0 Loss: 0.013435834120900127
Epoch: 23 Idx: 5000 Loss: 0.011473562111874967
Epoch: 24 Idx: 0 Loss: 0.013082910164267545
Epoch: 24 Idx: 5000 Loss: 0.010892483246876711
Epoch: 25 Idx: 0 Loss: 0.011418763818068448
Epoch: 25 Idx: 5000 Loss: 0.04466060089319329
Epoch: 26 Idx: 0 Loss: 0.004473697718142117
Epoch: 26 Idx: 5000 Loss: 0.03042276153419762
Epoch: 27 Idx: 0 Loss: 0.029339277366466444
Epoch: 27 Idx: 5000 Loss: 0.014208892417152327
Epoch: 28 Idx: 0 Loss: 0.017232492748810987
Epoch: 28 Idx: 5000 Loss: 0.0242965027546019
Epoch: 29 Idx: 0 Loss: 0.006875985195818821
Epoch: 29 Idx: 5000 Loss: 0.01662209775500782
Epoch: 30 Idx: 0 Loss: 0.005568136938418095
Epoch: 30 Idx: 5000 Loss: 0.014231732659721226
Epoch: 31 Idx: 0 Loss: 0.045910162794242745
Epoch: 31 Idx: 5000 Loss: 0.015165767669059756
Epoch: 32 Idx: 0 Loss: 0.013750907647026992
Epoch: 32 Idx: 5000 Loss: 0.014698829803803173
Epoch: 33 Idx: 0 Loss: 0.016460418548518374
Epoch: 33 Idx: 5000 Loss: 0.04373928360313844
Epoch: 34 Idx: 0 Loss: 0.017175752272647136
Epoch: 34 Idx: 5000 Loss: 0.016600177765308182
Epoch: 35 Idx: 0 Loss: 0.007871944584067697
Epoch: 35 Idx: 5000 Loss: 0.014939067541691678
Epoch: 36 Idx: 0 Loss: 0.010097826199254159
Epoch: 36 Idx: 5000 Loss: 0.009659772844635673
Epoch: 37 Idx: 0 Loss: 0.0270784365061125
Epoch: 37 Idx: 5000 Loss: 0.009397698710089806
Epoch: 38 Idx: 0 Loss: 0.012446780837712744
Epoch: 38 Idx: 5000 Loss: 0.009021616603449634
Epoch: 39 Idx: 0 Loss: 0.005970367864422916
Epoch: 39 Idx: 5000 Loss: 0.02124434908177468
Epoch: 40 Idx: 0 Loss: 0.004034845649167159
Epoch: 40 Idx: 5000 Loss: 0.0470804676510773
Epoch: 41 Idx: 0 Loss: 0.022057209199771423
Epoch: 41 Idx: 5000 Loss: 0.007309981946755755
Epoch: 42 Idx: 0 Loss: 0.010905538553479647
Epoch: 42 Idx: 5000 Loss: 0.03951112710505851
Epoch: 43 Idx: 0 Loss: 0.006044759516096515
Epoch: 43 Idx: 5000 Loss: 0.019469149197182976
Epoch: 44 Idx: 0 Loss: 0.01469722615146268
Epoch: 44 Idx: 5000 Loss: 0.012731208718918452
Epoch: 45 Idx: 0 Loss: 0.01567556741949044
Epoch: 45 Idx: 5000 Loss: 0.023400711699143456
Epoch: 46 Idx: 0 Loss: 0.015082371580755774
Epoch: 46 Idx: 5000 Loss: 0.010510925679001384
Epoch: 47 Idx: 0 Loss: 0.013347044841232001
Epoch: 47 Idx: 5000 Loss: 0.016311361530440553
Epoch: 48 Idx: 0 Loss: 0.02432485442439101
Epoch: 48 Idx: 5000 Loss: 0.013894716034254097
Epoch: 49 Idx: 0 Loss: 0.01630052887349591
Epoch: 49 Idx: 5000 Loss: 0.010112594245311739
Len (direct inputs):  877
Inputs len 5600 17 7800
Len (direct inputs):  2217
Starting sliding window evaluation...
Step 8/7
Val onto:  [('cmt', 'iasted')] test_onto:  [('confof', 'sigkdd')]
Training size: 111351 Testing size: 2336
Epoch: 0 Idx: 0 Loss: 0.20576614337856475
Epoch: 0 Idx: 5000 Loss: 0.00813830467976315
Epoch: 1 Idx: 0 Loss: 0.019617995201595986
Epoch: 1 Idx: 5000 Loss: 0.007760700255124946
Epoch: 2 Idx: 0 Loss: 0.014219010742442884
Epoch: 2 Idx: 5000 Loss: 0.012655643833398431
Epoch: 3 Idx: 0 Loss: 0.01769001438048404
Epoch: 3 Idx: 5000 Loss: 0.021150245010844056
Epoch: 4 Idx: 0 Loss: 0.014468777966268258
Epoch: 4 Idx: 5000 Loss: 0.022912465314132588
Epoch: 5 Idx: 0 Loss: 0.012080665451740328
Epoch: 5 Idx: 5000 Loss: 0.009059058175988866
Epoch: 6 Idx: 0 Loss: 0.014727355744015399
Traceback (most recent call last):
  File "main.py", line 514, in <module>
    outputs = model(node_elems, inp_elems)
  File "/u/harshitk/miniconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "main.py", line 312, in forward
    feature_emb = self.name_embedding(features[i]) #  dim: (2, batch_size, max_types, max_paths, max_pathlen, 512)
  File "/u/harshitk/miniconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/u/harshitk/miniconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/sparse.py", line 126, in forward
    self.norm_type, self.scale_grad_by_freq, self.sparse)
  File "/u/harshitk/miniconda3/envs/py37/lib/python3.7/site-packages/torch/nn/functional.py", line 1814, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
KeyboardInterrupt

------------------------------------------------------------
Sender: LSF System <rer@dccxc251>
Subject: Job 4066880: <python main.py 6 6 False True> in cluster <dcc> Exited

Job <python main.py 6 6 False True> was submitted from host <dccxl004> by user <harshitk> in cluster <dcc> at Tue Sep 15 15:48:40 2020
Job was executed on host(s) <dccxc251>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep 15 15:49:37 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/VeeAlign/src> was used as the working directory.
Started at Tue Sep 15 15:49:37 2020
Terminated at Wed Sep 16 04:38:38 2020
Results reported at Wed Sep 16 04:38:38 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python main.py 6 6 False True
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   46079.10 sec.
    Max Memory :                                 2930 MB
    Average Memory :                             2733.36 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40487.00 MB
    Max Swap :                                   2 MB
    Max Processes :                              3
    Max Threads :                                13
    Run time :                                   46167 sec.
    Turnaround time :                            46198 sec.

The output (if any) is above this job summary.

