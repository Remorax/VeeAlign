2020-09-15 15:48:43.527312: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-09-15 15:48:50.718033: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1
2020-09-15 15:48:50.835195: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:1e:00.0 name: Tesla K80 computeCapability: 3.7
coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s
2020-09-15 15:48:50.835271: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-09-15 15:48:50.837481: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
2020-09-15 15:48:50.858267: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2020-09-15 15:48:50.892975: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2020-09-15 15:48:50.935196: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2020-09-15 15:48:50.957092: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10
2020-09-15 15:48:50.957612: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudnn.so.7'; dlerror: libcudnn.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/share/lsf-9.1.3/10.1/linux3.10-glibc2.17-x86_64/lib:=/opt/share/gcc-4.9.2_rhel6/x86_64/lib/:/opt/share/gcc-4.9.2_rhel6/x86_64/lib64:/opt/share/Python-3.6.2/x86_64/lib:=/opt/share/gcc-5.4.0/x86_64/lib/:/opt/share/gcc-5.4.0/x86_64/lib64:/opt/share/isl-0.17/x86_64/lib/:/opt/share/protobuf-3.1.0/x86_64/lib/:/opt/share/leveldb-1.19/x86_64/lib/:/opt/share/boost-1.62.0/x86_64/lib/:/opt/share/torch-7/x86_64/install/lib:/opt/share/Python-2.7.12/x86_64/lib:/opt/share/Python-3.5.2/x86_64/lib:/opt/share/cuDNN-v5.1-8.0/cuda/lib64:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/opt/share/cuda-8.0/
2020-09-15 15:48:50.957635: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1753] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2020-09-15 15:48:50.958101: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-09-15 15:48:50.995456: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2599940000 Hz
2020-09-15 15:48:50.995738: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x564b038e7de0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-09-15 15:48:50.995761: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-09-15 15:48:50.998673: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-09-15 15:48:50.998717: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      
Prefix path:  /dccstor/cogfin/arvind/da/VeeAlign/
Ontologies being aligned are:  [('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/confOf.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/conference.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/conference.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/conference.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/conference.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/confOf.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/conference.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/confOf.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/confOf.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/confOf.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl')]
Total number of extracted unique classes and properties from entire RA set:  829
Constructing abbrevation resolution dict....
Results after abbreviation resolution:  {'PC': 'Program Committee', 'OC': 'Organizing Committee'}
Resolving abbreviations...
Number of entities: 119639
Starting sliding window evaluation...
Step 0/7
Val onto:  [('confof', 'ekaw')] test_onto:  [('conference', 'sigkdd')]
Training size: 112565 Testing size: 3871
Epoch: 0 Idx: 0 Loss: 0.19759427681922065
Epoch: 0 Idx: 5000 Loss: 0.027819466457365712
Epoch: 1 Idx: 0 Loss: 0.022375561937777497
Epoch: 1 Idx: 5000 Loss: 0.013066551027660548
Epoch: 2 Idx: 0 Loss: 0.010216569348426447
Epoch: 2 Idx: 5000 Loss: 0.013588365392911268
Epoch: 3 Idx: 0 Loss: 0.02147152155680688
Epoch: 3 Idx: 5000 Loss: 0.015336119452043596
Epoch: 4 Idx: 0 Loss: 0.013760069355579706
Epoch: 4 Idx: 5000 Loss: 0.017403708804276566
Epoch: 5 Idx: 0 Loss: 0.012950991894892057
Epoch: 5 Idx: 5000 Loss: 0.007230090972473156
Epoch: 6 Idx: 0 Loss: 0.015175755819860092
Epoch: 6 Idx: 5000 Loss: 0.014749398607041483
Epoch: 7 Idx: 0 Loss: 0.026159405180685873
Epoch: 7 Idx: 5000 Loss: 0.020254946379901183
Epoch: 8 Idx: 0 Loss: 0.011591883565486436
Epoch: 8 Idx: 5000 Loss: 0.006101721460077594
Epoch: 9 Idx: 0 Loss: 0.015993228933121842
Epoch: 9 Idx: 5000 Loss: 0.009212015812171303
Epoch: 10 Idx: 0 Loss: 0.019626644804160782
Epoch: 10 Idx: 5000 Loss: 0.010876877278972422
Epoch: 11 Idx: 0 Loss: 0.008119968116152398
Epoch: 11 Idx: 5000 Loss: 0.012831836489081115
Epoch: 12 Idx: 0 Loss: 0.010023944657402105
Epoch: 12 Idx: 5000 Loss: 0.026453944652082284
Epoch: 13 Idx: 0 Loss: 0.013615801552817677
Epoch: 13 Idx: 5000 Loss: 0.005619607779226212
Epoch: 14 Idx: 0 Loss: 0.025197430276246817
Epoch: 14 Idx: 5000 Loss: 0.020655802136733913
Epoch: 15 Idx: 0 Loss: 0.01258813114418767
Epoch: 15 Idx: 5000 Loss: 0.01521595843719134
Epoch: 16 Idx: 0 Loss: 0.016140884752842623
Epoch: 16 Idx: 5000 Loss: 0.014885984795345648
Epoch: 17 Idx: 0 Loss: 0.016369815003479324
Epoch: 17 Idx: 5000 Loss: 0.026402482605173586
Epoch: 18 Idx: 0 Loss: 0.034856056689609644
Epoch: 18 Idx: 5000 Loss: 0.016154914508167965
Epoch: 19 Idx: 0 Loss: 0.030590731757917034
Epoch: 19 Idx: 5000 Loss: 0.012241862687468966
Epoch: 20 Idx: 0 Loss: 0.0231213430535561
Epoch: 20 Idx: 5000 Loss: 0.009496424344085562
Epoch: 21 Idx: 0 Loss: 0.011323670262552023
Epoch: 21 Idx: 5000 Loss: 0.014826605054893017
Epoch: 22 Idx: 0 Loss: 0.009094587100516104
Epoch: 22 Idx: 5000 Loss: 0.03506471713897915
Epoch: 23 Idx: 0 Loss: 0.020730158309949613
Epoch: 23 Idx: 5000 Loss: 0.03398791679716615
Epoch: 24 Idx: 0 Loss: 0.010272313011662886
Epoch: 24 Idx: 5000 Loss: 0.015576082372165127
Epoch: 25 Idx: 0 Loss: 0.017187303091098678
Epoch: 25 Idx: 5000 Loss: 0.013613119384759627
Epoch: 26 Idx: 0 Loss: 0.006944036546071723
Epoch: 26 Idx: 5000 Loss: 0.0177726453585769
Epoch: 27 Idx: 0 Loss: 0.012140559515138673
Epoch: 27 Idx: 5000 Loss: 0.01593475683271006
Epoch: 28 Idx: 0 Loss: 0.02178655100337485
Epoch: 28 Idx: 5000 Loss: 0.0116798792825895
Epoch: 29 Idx: 0 Loss: 0.01024744243193355
Epoch: 29 Idx: 5000 Loss: 0.011081468547127951
Epoch: 30 Idx: 0 Loss: 0.015426463609596878
Epoch: 30 Idx: 5000 Loss: 0.013818025861968671
Epoch: 31 Idx: 0 Loss: 0.017092233485558564
Epoch: 31 Idx: 5000 Loss: 0.012556818611247935
Epoch: 32 Idx: 0 Loss: 0.015118049205244486
Epoch: 32 Idx: 5000 Loss: 0.013257106249921307
Epoch: 33 Idx: 0 Loss: 0.005407375390549386
Epoch: 33 Idx: 5000 Loss: 0.010462086265863058
Epoch: 34 Idx: 0 Loss: 0.009889930859137877
Epoch: 34 Idx: 5000 Loss: 0.011058713481008951
Epoch: 35 Idx: 0 Loss: 0.01731918554766756
Epoch: 35 Idx: 5000 Loss: 0.011857067240296563
Epoch: 36 Idx: 0 Loss: 0.007137044666108742
Epoch: 36 Idx: 5000 Loss: 0.021526786806190242
Epoch: 37 Idx: 0 Loss: 0.011563319045272505
Epoch: 37 Idx: 5000 Loss: 0.013385986270055196
Epoch: 38 Idx: 0 Loss: 0.025687619980844185
Epoch: 38 Idx: 5000 Loss: 0.02223509206239794
Epoch: 39 Idx: 0 Loss: 0.015477247178024655
Epoch: 39 Idx: 5000 Loss: 0.007921204438677543
Epoch: 40 Idx: 0 Loss: 0.012577379972961948
Epoch: 40 Idx: 5000 Loss: 0.015279408532141542
Epoch: 41 Idx: 0 Loss: 0.018496573282930882
Epoch: 41 Idx: 5000 Loss: 0.04344317017811867
Epoch: 42 Idx: 0 Loss: 0.013408233759114131
Epoch: 42 Idx: 5000 Loss: 0.008535712074631158
Epoch: 43 Idx: 0 Loss: 0.023377251322315364
Epoch: 43 Idx: 5000 Loss: 0.012486749202023356
Epoch: 44 Idx: 0 Loss: 0.019782012946013228
Epoch: 44 Idx: 5000 Loss: 0.011646904296051454
Epoch: 45 Idx: 0 Loss: 0.00740247521949727
Epoch: 45 Idx: 5000 Loss: 0.025453829037762617
Epoch: 46 Idx: 0 Loss: 0.010508871617346332
Epoch: 46 Idx: 5000 Loss: 0.017408124808446473
Epoch: 47 Idx: 0 Loss: 0.020167685743068015
Epoch: 47 Idx: 5000 Loss: 0.004080933472140807
Epoch: 48 Idx: 0 Loss: 0.03176855912838263
Epoch: 48 Idx: 5000 Loss: 0.013317127632678546
Epoch: 49 Idx: 0 Loss: 0.017439902270956063
Epoch: 49 Idx: 5000 Loss: 0.021596014123827532
Len (direct inputs):  429
Inputs len 2744 15 3856
Len (direct inputs):  1127
Starting sliding window evaluation...
Step 2/7
Val onto:  [('conference', 'ekaw')] test_onto:  [('cmt', 'sigkdd')]
Training size: 111450 Testing size: 2364
Epoch: 0 Idx: 0 Loss: 0.14899505845647582
Epoch: 0 Idx: 5000 Loss: 0.02026885982351122
Epoch: 1 Idx: 0 Loss: 0.025541245135769098
Epoch: 1 Idx: 5000 Loss: 0.03122177403856451
Epoch: 2 Idx: 0 Loss: 0.008602724088496213
Epoch: 2 Idx: 5000 Loss: 0.010878143468037147
Epoch: 3 Idx: 0 Loss: 0.007137070660367118
Epoch: 3 Idx: 5000 Loss: 0.00851924506323452
Epoch: 4 Idx: 0 Loss: 0.02403427417301527
Epoch: 4 Idx: 5000 Loss: 0.04545698730774526
Epoch: 5 Idx: 0 Loss: 0.023573680984402302
Epoch: 5 Idx: 5000 Loss: 0.014058249019250868
Epoch: 6 Idx: 0 Loss: 0.03554431094790565
Epoch: 6 Idx: 5000 Loss: 0.012316853154083922
Epoch: 7 Idx: 0 Loss: 0.01710532472420856
Epoch: 7 Idx: 5000 Loss: 0.017096728685882433
Epoch: 8 Idx: 0 Loss: 0.007032826880255911
Epoch: 8 Idx: 5000 Loss: 0.012539690765665898
Epoch: 9 Idx: 0 Loss: 0.009308069216965953
Epoch: 9 Idx: 5000 Loss: 0.00926845747823578
Epoch: 10 Idx: 0 Loss: 0.00893800715761173
Epoch: 10 Idx: 5000 Loss: 0.018593724645664868
Epoch: 11 Idx: 0 Loss: 0.027341435622722392
Epoch: 11 Idx: 5000 Loss: 0.0056592425050809875
Epoch: 12 Idx: 0 Loss: 0.015389576562035358
Epoch: 12 Idx: 5000 Loss: 0.014404344171505252
Epoch: 13 Idx: 0 Loss: 0.013665670752421167
Epoch: 13 Idx: 5000 Loss: 0.011538874884586773
Epoch: 14 Idx: 0 Loss: 0.009395528565775118
Epoch: 14 Idx: 5000 Loss: 0.019270348221638003
Epoch: 15 Idx: 0 Loss: 0.02172358299666863
Epoch: 15 Idx: 5000 Loss: 0.023963390531324708
Epoch: 16 Idx: 0 Loss: 0.020911716961504227
Epoch: 16 Idx: 5000 Loss: 0.015720875071290316
Epoch: 17 Idx: 0 Loss: 0.01663424323551841
Epoch: 17 Idx: 5000 Loss: 0.012961022378853207
Epoch: 18 Idx: 0 Loss: 0.011019816625542569
Epoch: 18 Idx: 5000 Loss: 0.013371794248672249
Epoch: 19 Idx: 0 Loss: 0.025527146737515036
Epoch: 19 Idx: 5000 Loss: 0.007630495852591926
Epoch: 20 Idx: 0 Loss: 0.03260718181979312
Epoch: 20 Idx: 5000 Loss: 0.006049074755623773
Epoch: 21 Idx: 0 Loss: 0.009221894102740142
Epoch: 21 Idx: 5000 Loss: 0.011092340790396489
Epoch: 22 Idx: 0 Loss: 0.0036687411094733233
Epoch: 22 Idx: 5000 Loss: 0.009290907643733054
Epoch: 23 Idx: 0 Loss: 0.012391624294426895
Epoch: 23 Idx: 5000 Loss: 0.014864538122205706
Epoch: 24 Idx: 0 Loss: 0.020117031427977196
Epoch: 24 Idx: 5000 Loss: 0.010987511149666533
Epoch: 25 Idx: 0 Loss: 0.021089538335293166
Epoch: 25 Idx: 5000 Loss: 0.009808498928859091
Epoch: 26 Idx: 0 Loss: 0.009111288934973823
Epoch: 26 Idx: 5000 Loss: 0.020778466376735787
Epoch: 27 Idx: 0 Loss: 0.009039385755426253
Epoch: 27 Idx: 5000 Loss: 0.006249127855754357
Epoch: 28 Idx: 0 Loss: 0.011871052981083452
Epoch: 28 Idx: 5000 Loss: 0.011103049544708539
Epoch: 29 Idx: 0 Loss: 0.01010616492913077
Epoch: 29 Idx: 5000 Loss: 0.03380945373225338
Epoch: 30 Idx: 0 Loss: 0.01196314240278304
Epoch: 30 Idx: 5000 Loss: 0.019323592801810986
Epoch: 31 Idx: 0 Loss: 0.04098015976940753
Epoch: 31 Idx: 5000 Loss: 0.02846173623866259
Epoch: 32 Idx: 0 Loss: 0.012587876364711908
Epoch: 32 Idx: 5000 Loss: 0.018116678102895654
Epoch: 33 Idx: 0 Loss: 0.01196528001549179
Epoch: 33 Idx: 5000 Loss: 0.008509661913578008
Epoch: 34 Idx: 0 Loss: 0.011794249611350009
Epoch: 34 Idx: 5000 Loss: 0.04881263984693313
Epoch: 35 Idx: 0 Loss: 0.012383141098386652
Epoch: 35 Idx: 5000 Loss: 0.027237325763315112
Epoch: 36 Idx: 0 Loss: 0.012416574167924604
Epoch: 36 Idx: 5000 Loss: 0.018942533782463602
Epoch: 37 Idx: 0 Loss: 0.013299767461126759
Epoch: 37 Idx: 5000 Loss: 0.027770949813131572
Epoch: 38 Idx: 0 Loss: 0.01460675253029867
Epoch: 38 Idx: 5000 Loss: 0.016111167035657938
Epoch: 39 Idx: 0 Loss: 0.009485891270872349
Epoch: 39 Idx: 5000 Loss: 0.012760499442881351
Epoch: 40 Idx: 0 Loss: 0.00992649663424146
Epoch: 40 Idx: 5000 Loss: 0.010815930769289774
Epoch: 41 Idx: 0 Loss: 0.011506641200633801
Epoch: 41 Idx: 5000 Loss: 0.011209668831015464
Epoch: 42 Idx: 0 Loss: 0.01597645022008508
Epoch: 42 Idx: 5000 Loss: 0.022243566350965336
Epoch: 43 Idx: 0 Loss: 0.015338617128266691
Epoch: 43 Idx: 5000 Loss: 0.004763911018226055
Epoch: 44 Idx: 0 Loss: 0.020334681828053015
Epoch: 44 Idx: 5000 Loss: 0.016932464557294774
Epoch: 45 Idx: 0 Loss: 0.013828868032008633
Epoch: 45 Idx: 5000 Loss: 0.010066011814855925
Epoch: 46 Idx: 0 Loss: 0.0203865667198105
Epoch: 46 Idx: 5000 Loss: 0.03045426019011714
Epoch: 47 Idx: 0 Loss: 0.02582460204405572
Epoch: 47 Idx: 5000 Loss: 0.029466467095859837
Epoch: 48 Idx: 0 Loss: 0.030899308958143457
Epoch: 48 Idx: 5000 Loss: 0.022660465864567585
Epoch: 49 Idx: 0 Loss: 0.009357734652229235
Epoch: 49 Idx: 5000 Loss: 0.0276342207534958
Len (direct inputs):  1737
Inputs len 1372 12 2352
Len (direct inputs):  992
Starting sliding window evaluation...
Step 4/7
Val onto:  [('ekaw', 'sigkdd')] test_onto:  [('cmt', 'conference')]
Training size: 111356 Testing size: 4145
Epoch: 0 Idx: 0 Loss: 0.15342610653646377
Epoch: 0 Idx: 5000 Loss: 0.014244218376089584
Epoch: 1 Idx: 0 Loss: 0.01892595496876205
Epoch: 1 Idx: 5000 Loss: 0.01081558818452967
Epoch: 2 Idx: 0 Loss: 0.024805180960357758
Epoch: 2 Idx: 5000 Loss: 0.007151550570096312
Epoch: 3 Idx: 0 Loss: 0.016992330873031473
Epoch: 3 Idx: 5000 Loss: 0.010716583006671951
Epoch: 4 Idx: 0 Loss: 0.01649174911939235
Epoch: 4 Idx: 5000 Loss: 0.011023604010951924
Epoch: 5 Idx: 0 Loss: 0.021301786542942912
Epoch: 5 Idx: 5000 Loss: 0.012650261950465726
Epoch: 6 Idx: 0 Loss: 0.014961963706609049
Epoch: 6 Idx: 5000 Loss: 0.006129403553284507
Epoch: 7 Idx: 0 Loss: 0.02738067386180242
Epoch: 7 Idx: 5000 Loss: 0.011004131813320606
Epoch: 8 Idx: 0 Loss: 0.008374950225345488
Epoch: 8 Idx: 5000 Loss: 0.013203969616313573
Epoch: 9 Idx: 0 Loss: 0.03827267809218153
Traceback (most recent call last):
  File "main.py", line 514, in <module>
    outputs = model(node_elems, inp_elems)
  File "/u/harshitk/miniconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "main.py", line 312, in forward
    feature_emb = self.name_embedding(features[i]) #  dim: (2, batch_size, max_types, max_paths, max_pathlen, 512)
  File "/u/harshitk/miniconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/u/harshitk/miniconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/sparse.py", line 126, in forward
    self.norm_type, self.scale_grad_by_freq, self.sparse)
  File "/u/harshitk/miniconda3/envs/py37/lib/python3.7/site-packages/torch/nn/functional.py", line 1814, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
KeyboardInterrupt

------------------------------------------------------------
Sender: LSF System <rer@dccxc274>
Subject: Job 4066840: <python main.py 4 21 False True> in cluster <dcc> Exited

Job <python main.py 4 21 False True> was submitted from host <dccxl004> by user <harshitk> in cluster <dcc> at Tue Sep 15 15:48:37 2020
Job was executed on host(s) <dccxc274>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep 15 15:48:38 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/VeeAlign/src> was used as the working directory.
Started at Tue Sep 15 15:48:38 2020
Terminated at Wed Sep 16 04:38:39 2020
Results reported at Wed Sep 16 04:38:39 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python main.py 4 21 False True
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   46190.55 sec.
    Max Memory :                                 2975 MB
    Average Memory :                             2737.33 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40442.00 MB
    Max Swap :                                   2 MB
    Max Processes :                              3
    Max Threads :                                13
    Run time :                                   46222 sec.
    Turnaround time :                            46202 sec.

The output (if any) is above this job summary.

