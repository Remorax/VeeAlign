2020-09-15 15:48:43.707872: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-09-15 15:48:50.859587: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1
2020-09-15 15:48:50.974817: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:1e:00.0 name: Tesla K80 computeCapability: 3.7
coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s
2020-09-15 15:48:50.974897: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-09-15 15:48:50.977353: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
2020-09-15 15:48:50.979011: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2020-09-15 15:48:50.979513: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2020-09-15 15:48:50.981729: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2020-09-15 15:48:50.983484: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10
2020-09-15 15:48:50.983907: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudnn.so.7'; dlerror: libcudnn.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/share/lsf-9.1.3/10.1/linux3.10-glibc2.17-x86_64/lib:=/opt/share/gcc-4.9.2_rhel6/x86_64/lib/:/opt/share/gcc-4.9.2_rhel6/x86_64/lib64:/opt/share/Python-3.6.2/x86_64/lib:=/opt/share/gcc-5.4.0/x86_64/lib/:/opt/share/gcc-5.4.0/x86_64/lib64:/opt/share/isl-0.17/x86_64/lib/:/opt/share/protobuf-3.1.0/x86_64/lib/:/opt/share/leveldb-1.19/x86_64/lib/:/opt/share/boost-1.62.0/x86_64/lib/:/opt/share/torch-7/x86_64/install/lib:/opt/share/Python-2.7.12/x86_64/lib:/opt/share/Python-3.5.2/x86_64/lib:/opt/share/cuDNN-v5.1-8.0/cuda/lib64:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/opt/share/cuda-8.0/
2020-09-15 15:48:50.983931: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1753] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2020-09-15 15:48:50.984401: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-09-15 15:48:51.022963: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2599975000 Hz
2020-09-15 15:48:51.023277: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c17c3c2860 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-09-15 15:48:51.023298: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-09-15 15:48:51.026302: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-09-15 15:48:51.026336: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      
Prefix path:  /dccstor/cogfin/arvind/da/VeeAlign/
Ontologies being aligned are:  [('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/confOf.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/conference.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/conference.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/conference.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/conference.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/confOf.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/conference.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/confOf.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/confOf.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/confOf.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl')]
Total number of extracted unique classes and properties from entire RA set:  829
Constructing abbrevation resolution dict....
Results after abbreviation resolution:  {'PC': 'Program Committee', 'OC': 'Organizing Committee'}
Resolving abbreviations...
Number of entities: 119639
Starting sliding window evaluation...
Step 0/7
Val onto:  [('confof', 'ekaw')] test_onto:  [('conference', 'sigkdd')]
Training size: 112565 Testing size: 3871
Epoch: 0 Idx: 0 Loss: 0.1938280867498537
Epoch: 0 Idx: 5000 Loss: 0.0041833101704729595
Epoch: 1 Idx: 0 Loss: 0.03289278168096932
Epoch: 1 Idx: 5000 Loss: 0.009121394257222814
Epoch: 2 Idx: 0 Loss: 0.0068256911554361
Epoch: 2 Idx: 5000 Loss: 0.027574245463113215
Epoch: 3 Idx: 0 Loss: 0.010439895843358785
Epoch: 3 Idx: 5000 Loss: 0.02825160788668741
Epoch: 4 Idx: 0 Loss: 0.008032638713532159
Epoch: 4 Idx: 5000 Loss: 0.010604170608192558
Epoch: 5 Idx: 0 Loss: 0.016078774155128043
Epoch: 5 Idx: 5000 Loss: 0.013469365808432034
Epoch: 6 Idx: 0 Loss: 0.017443134795089698
Epoch: 6 Idx: 5000 Loss: 0.02703390177804608
Epoch: 7 Idx: 0 Loss: 0.021561032737047507
Epoch: 7 Idx: 5000 Loss: 0.007698110896946766
Epoch: 8 Idx: 0 Loss: 0.026711364166051882
Epoch: 8 Idx: 5000 Loss: 0.008017033228973756
Epoch: 9 Idx: 0 Loss: 0.0231291932169045
Epoch: 9 Idx: 5000 Loss: 0.00984667382725464
Epoch: 10 Idx: 0 Loss: 0.008109757154188565
Epoch: 10 Idx: 5000 Loss: 0.03127378183019343
Epoch: 11 Idx: 0 Loss: 0.024595528899630032
Epoch: 11 Idx: 5000 Loss: 0.016287775860740906
Epoch: 12 Idx: 0 Loss: 0.009851883384231753
Epoch: 12 Idx: 5000 Loss: 0.021730718466994405
Epoch: 13 Idx: 0 Loss: 0.023599183596212953
Epoch: 13 Idx: 5000 Loss: 0.007638125948318211
Epoch: 14 Idx: 0 Loss: 0.013681674705339847
Epoch: 14 Idx: 5000 Loss: 0.03507072020695504
Epoch: 15 Idx: 0 Loss: 0.025919556991866575
Epoch: 15 Idx: 5000 Loss: 0.008379476236137787
Epoch: 16 Idx: 0 Loss: 0.007804863526269355
Epoch: 16 Idx: 5000 Loss: 0.013608269774527199
Epoch: 17 Idx: 0 Loss: 0.014936432973744772
Epoch: 17 Idx: 5000 Loss: 0.0101508924173623
Epoch: 18 Idx: 0 Loss: 0.024803468582587168
Epoch: 18 Idx: 5000 Loss: 0.02559579540382251
Epoch: 19 Idx: 0 Loss: 0.01707191835361979
Epoch: 19 Idx: 5000 Loss: 0.013479940744209233
Epoch: 20 Idx: 0 Loss: 0.01611187654725986
Epoch: 20 Idx: 5000 Loss: 0.008273626876522201
Epoch: 21 Idx: 0 Loss: 0.016458572968095567
Epoch: 21 Idx: 5000 Loss: 0.04511467845811484
Epoch: 22 Idx: 0 Loss: 0.013553832992672515
Epoch: 22 Idx: 5000 Loss: 0.054392692334344975
Epoch: 23 Idx: 0 Loss: 0.010188575292029567
Epoch: 23 Idx: 5000 Loss: 0.009387761078455091
Epoch: 24 Idx: 0 Loss: 0.007165033469022537
Epoch: 24 Idx: 5000 Loss: 0.01570091335056908
Epoch: 25 Idx: 0 Loss: 0.006123881018071819
Epoch: 25 Idx: 5000 Loss: 0.018099834719674184
Epoch: 26 Idx: 0 Loss: 0.012027291495785176
Epoch: 26 Idx: 5000 Loss: 0.021859172652919232
Epoch: 27 Idx: 0 Loss: 0.013997367397282464
Epoch: 27 Idx: 5000 Loss: 0.026595564059118264
Epoch: 28 Idx: 0 Loss: 0.013911693531129662
Epoch: 28 Idx: 5000 Loss: 0.01575458151986378
Epoch: 29 Idx: 0 Loss: 0.020706254747301412
Epoch: 29 Idx: 5000 Loss: 0.03142308343276504
Epoch: 30 Idx: 0 Loss: 0.018933211160402308
Epoch: 30 Idx: 5000 Loss: 0.023436842429331257
Epoch: 31 Idx: 0 Loss: 0.012658375842422461
Epoch: 31 Idx: 5000 Loss: 0.014302567137597866
Epoch: 32 Idx: 0 Loss: 0.02604937936752314
Epoch: 32 Idx: 5000 Loss: 0.008038347902384273
Epoch: 33 Idx: 0 Loss: 0.009477234399415134
Epoch: 33 Idx: 5000 Loss: 0.009979482672310053
Epoch: 34 Idx: 0 Loss: 0.02256324476487428
Epoch: 34 Idx: 5000 Loss: 0.025989255099460987
Epoch: 35 Idx: 0 Loss: 0.008879463990672756
Epoch: 35 Idx: 5000 Loss: 0.015810779450143527
Epoch: 36 Idx: 0 Loss: 0.015941359343284053
Epoch: 36 Idx: 5000 Loss: 0.01266572654961578
Epoch: 37 Idx: 0 Loss: 0.013333685700469798
Epoch: 37 Idx: 5000 Loss: 0.01664328045851131
Epoch: 38 Idx: 0 Loss: 0.009851191725612942
Epoch: 38 Idx: 5000 Loss: 0.025936157030592856
Epoch: 39 Idx: 0 Loss: 0.00916061268749839
Epoch: 39 Idx: 5000 Loss: 0.014898962367390063
Epoch: 40 Idx: 0 Loss: 0.011707025650826616
Epoch: 40 Idx: 5000 Loss: 0.014748586954856086
Epoch: 41 Idx: 0 Loss: 0.01062236455473395
Epoch: 41 Idx: 5000 Loss: 0.02576285473184496
Epoch: 42 Idx: 0 Loss: 0.04082996411229689
Epoch: 42 Idx: 5000 Loss: 0.018325985050232668
Epoch: 43 Idx: 0 Loss: 0.022194428007670047
Epoch: 43 Idx: 5000 Loss: 0.013267704623042033
Epoch: 44 Idx: 0 Loss: 0.007427788917276475
Epoch: 44 Idx: 5000 Loss: 0.010414824575407933
Epoch: 45 Idx: 0 Loss: 0.007611740182654929
Epoch: 45 Idx: 5000 Loss: 0.028653581334683875
Epoch: 46 Idx: 0 Loss: 0.029475496598891517
Epoch: 46 Idx: 5000 Loss: 0.018266578763091817
Epoch: 47 Idx: 0 Loss: 0.021522851415181232
Epoch: 47 Idx: 5000 Loss: 0.016206002069417736
Epoch: 48 Idx: 0 Loss: 0.01171040959876964
Epoch: 48 Idx: 5000 Loss: 0.020064217314104092
Epoch: 49 Idx: 0 Loss: 0.03474658952898016
Epoch: 49 Idx: 5000 Loss: 0.013522193887260078
Len (direct inputs):  429
Inputs len 2744 15 3856
Len (direct inputs):  1127
Starting sliding window evaluation...
Step 2/7
Val onto:  [('conference', 'ekaw')] test_onto:  [('cmt', 'sigkdd')]
Training size: 111450 Testing size: 2364
Epoch: 0 Idx: 0 Loss: 0.14903646169958296
Epoch: 0 Idx: 5000 Loss: 0.027250208237364695
Epoch: 1 Idx: 0 Loss: 0.02263132715525787
Epoch: 1 Idx: 5000 Loss: 0.01991875907364469
Epoch: 2 Idx: 0 Loss: 0.04256305794498463
Epoch: 2 Idx: 5000 Loss: 0.01208484763901132
Epoch: 3 Idx: 0 Loss: 0.014824621974896842
Epoch: 3 Idx: 5000 Loss: 0.013179495859405461
Epoch: 4 Idx: 0 Loss: 0.010011870269872025
Epoch: 4 Idx: 5000 Loss: 0.011702602670090427
Epoch: 5 Idx: 0 Loss: 0.022654396189674846
Epoch: 5 Idx: 5000 Loss: 0.01221013044913832
Epoch: 6 Idx: 0 Loss: 0.009903915755248047
Epoch: 6 Idx: 5000 Loss: 0.019743229812909366
Epoch: 7 Idx: 0 Loss: 0.020816419270717652
Epoch: 7 Idx: 5000 Loss: 0.01680528148556168
Epoch: 8 Idx: 0 Loss: 0.005810302622978904
Epoch: 8 Idx: 5000 Loss: 0.01345466921813477
Epoch: 9 Idx: 0 Loss: 0.017304417715843474
Epoch: 9 Idx: 5000 Loss: 0.009789808586722137
Epoch: 10 Idx: 0 Loss: 0.010714779410249228
Epoch: 10 Idx: 5000 Loss: 0.012162753054928801
Epoch: 11 Idx: 0 Loss: 0.019530815346348766
Epoch: 11 Idx: 5000 Loss: 0.011410293641269057
Epoch: 12 Idx: 0 Loss: 0.040792899756913796
Epoch: 12 Idx: 5000 Loss: 0.028700107358114797
Epoch: 13 Idx: 0 Loss: 0.006680201336747978
Epoch: 13 Idx: 5000 Loss: 0.024329628770809804
Epoch: 14 Idx: 0 Loss: 0.012477320031956552
Epoch: 14 Idx: 5000 Loss: 0.01581775829891371
Epoch: 15 Idx: 0 Loss: 0.02408306567789233
Epoch: 15 Idx: 5000 Loss: 0.022893221760605667
Epoch: 16 Idx: 0 Loss: 0.019701114298832378
Epoch: 16 Idx: 5000 Loss: 0.020265871713376987
Epoch: 17 Idx: 0 Loss: 0.01599568251184573
Epoch: 17 Idx: 5000 Loss: 0.014792675742978836
Epoch: 18 Idx: 0 Loss: 0.04659746145065112
Epoch: 18 Idx: 5000 Loss: 0.0151878099417557
Epoch: 19 Idx: 0 Loss: 0.021990841983097597
Epoch: 19 Idx: 5000 Loss: 0.022853372354203632
Epoch: 20 Idx: 0 Loss: 0.00894382934999524
Epoch: 20 Idx: 5000 Loss: 0.008559058701901538
Epoch: 21 Idx: 0 Loss: 0.016014382460077723
Epoch: 21 Idx: 5000 Loss: 0.01850416058286387
Epoch: 22 Idx: 0 Loss: 0.006051841542860714
Epoch: 22 Idx: 5000 Loss: 0.00928850180144307
Epoch: 23 Idx: 0 Loss: 0.006297126734358713
Epoch: 23 Idx: 5000 Loss: 0.012131350165745382
Epoch: 24 Idx: 0 Loss: 0.01735071364690222
Epoch: 24 Idx: 5000 Loss: 0.012390072832308965
Epoch: 25 Idx: 0 Loss: 0.016945500900897782
Epoch: 25 Idx: 5000 Loss: 0.022599824740573003
Epoch: 26 Idx: 0 Loss: 0.01685057132685696
Epoch: 26 Idx: 5000 Loss: 0.019055247858836652
Epoch: 27 Idx: 0 Loss: 0.02231434905778843
Epoch: 27 Idx: 5000 Loss: 0.015836166188250934
Epoch: 28 Idx: 0 Loss: 0.014078242024330322
Epoch: 28 Idx: 5000 Loss: 0.004656847442413379
Epoch: 29 Idx: 0 Loss: 0.01127209037708195
Epoch: 29 Idx: 5000 Loss: 0.015455225469864012
Epoch: 30 Idx: 0 Loss: 0.007392828064161204
Epoch: 30 Idx: 5000 Loss: 0.008033965173542974
Epoch: 31 Idx: 0 Loss: 0.028967627778470714
Epoch: 31 Idx: 5000 Loss: 0.03224753502887349
Epoch: 32 Idx: 0 Loss: 0.014059521527028353
Epoch: 32 Idx: 5000 Loss: 0.017328570484825973
Epoch: 33 Idx: 0 Loss: 0.03290193203790736
Epoch: 33 Idx: 5000 Loss: 0.024627843717321317
Epoch: 34 Idx: 0 Loss: 0.025780837259441382
Epoch: 34 Idx: 5000 Loss: 0.014744762284366403
Epoch: 35 Idx: 0 Loss: 0.00910257782575211
Epoch: 35 Idx: 5000 Loss: 0.015554738907051972
Epoch: 36 Idx: 0 Loss: 0.02106304123081272
Epoch: 36 Idx: 5000 Loss: 0.021191482031043065
Epoch: 37 Idx: 0 Loss: 0.00848688688026753
Epoch: 37 Idx: 5000 Loss: 0.0233186409887704
Epoch: 38 Idx: 0 Loss: 0.0284275306972425
Epoch: 38 Idx: 5000 Loss: 0.02212552847867265
Epoch: 39 Idx: 0 Loss: 0.029826726925886053
Epoch: 39 Idx: 5000 Loss: 0.008908608482454465
Epoch: 40 Idx: 0 Loss: 0.010333868251890794
Epoch: 40 Idx: 5000 Loss: 0.020908190763407726
Epoch: 41 Idx: 0 Loss: 0.017298427532332315
Epoch: 41 Idx: 5000 Loss: 0.01389656865356992
Epoch: 42 Idx: 0 Loss: 0.02046697608865724
Epoch: 42 Idx: 5000 Loss: 0.02415681557175597
Epoch: 43 Idx: 0 Loss: 0.005510603723980612
Epoch: 43 Idx: 5000 Loss: 0.01503290806865323
Epoch: 44 Idx: 0 Loss: 0.01798338618073552
Epoch: 44 Idx: 5000 Loss: 0.01875043000814551
Epoch: 45 Idx: 0 Loss: 0.027347594930501713
Epoch: 45 Idx: 5000 Loss: 0.022768295568387484
Epoch: 46 Idx: 0 Loss: 0.015311782792274074
Epoch: 46 Idx: 5000 Loss: 0.012156667009342315
Epoch: 47 Idx: 0 Loss: 0.007048205069022332
Epoch: 47 Idx: 5000 Loss: 0.01377922306702603
Epoch: 48 Idx: 0 Loss: 0.004631796593515022
Epoch: 48 Idx: 5000 Loss: 0.020961514710123506
Epoch: 49 Idx: 0 Loss: 0.006735138966064852
Epoch: 49 Idx: 5000 Loss: 0.009607810242249697
Len (direct inputs):  1737
Inputs len 1372 12 2352
Len (direct inputs):  992
Starting sliding window evaluation...
Step 4/7
Val onto:  [('ekaw', 'sigkdd')] test_onto:  [('cmt', 'conference')]
Training size: 111356 Testing size: 4145
Epoch: 0 Idx: 0 Loss: 0.150875736212905
Epoch: 0 Idx: 5000 Loss: 0.04071318476859278
Epoch: 1 Idx: 0 Loss: 0.017315372615596458
Epoch: 1 Idx: 5000 Loss: 0.017303720238058956
Epoch: 2 Idx: 0 Loss: 0.012864450276798906
Epoch: 2 Idx: 5000 Loss: 0.018163068811616247
Epoch: 3 Idx: 0 Loss: 0.021950430032479408
Epoch: 3 Idx: 5000 Loss: 0.006990627806180089
Epoch: 4 Idx: 0 Loss: 0.013044316905923146
Epoch: 4 Idx: 5000 Loss: 0.012831254760191379
Epoch: 5 Idx: 0 Loss: 0.022236156192763937
Epoch: 5 Idx: 5000 Loss: 0.011221655016823014
Epoch: 6 Idx: 0 Loss: 0.011039655878036242
Epoch: 6 Idx: 5000 Loss: 0.024891798121468307
Epoch: 7 Idx: 0 Loss: 0.05148014871323671
Epoch: 7 Idx: 5000 Loss: 0.02158658149062917
Epoch: 8 Idx: 0 Loss: 0.005479097826080616
Epoch: 8 Idx: 5000 Loss: 0.009509366166434602
Epoch: 9 Idx: 0 Loss: 0.017066948073372647
Epoch: 9 Idx: 5000 Loss: 0.030937589419221728
Epoch: 10 Idx: 0 Loss: 0.008346240583891515
Epoch: 10 Idx: 5000 Loss: 0.007593507965745032
Epoch: 11 Idx: 0 Loss: 0.03098028396879227
Epoch: 11 Idx: 5000 Loss: 0.022176807914522985
Epoch: 12 Idx: 0 Loss: 0.007640710337761113
Epoch: 12 Idx: 5000 Loss: 0.013256487251830307
Epoch: 13 Idx: 0 Loss: 0.005480178362023052
Epoch: 13 Idx: 5000 Loss: 0.031389913447413056
Epoch: 14 Idx: 0 Loss: 0.01757782778125614
Epoch: 14 Idx: 5000 Loss: 0.012405445021866407
Epoch: 15 Idx: 0 Loss: 0.009440792985179754
Epoch: 15 Idx: 5000 Loss: 0.010521591167679699
Epoch: 16 Idx: 0 Loss: 0.014504958634296383
Epoch: 16 Idx: 5000 Loss: 0.01512212303969588
Epoch: 17 Idx: 0 Loss: 0.016952445985343574
Epoch: 17 Idx: 5000 Loss: 0.009980116551756325
Epoch: 18 Idx: 0 Loss: 0.01027032590420127
Epoch: 18 Idx: 5000 Loss: 0.04310917456710624
Epoch: 19 Idx: 0 Loss: 0.025838472015537964
Epoch: 19 Idx: 5000 Loss: 0.017919291476380047
Epoch: 20 Idx: 0 Loss: 0.02497362777059545
Epoch: 20 Idx: 5000 Loss: 0.007659582812579776
Epoch: 21 Idx: 0 Loss: 0.007757674888503994
Epoch: 21 Idx: 5000 Loss: 0.008281052403879174
Epoch: 22 Idx: 0 Loss: 0.01619887948522256
Epoch: 22 Idx: 5000 Loss: 0.011950817912599325
Epoch: 23 Idx: 0 Loss: 0.018276248640589403
Epoch: 23 Idx: 5000 Loss: 0.021588587589743097
Epoch: 24 Idx: 0 Loss: 0.02044533725073644
Epoch: 24 Idx: 5000 Loss: 0.013198241714677136
Epoch: 25 Idx: 0 Loss: 0.012877930223857015
Epoch: 25 Idx: 5000 Loss: 0.015164635752513846
Epoch: 26 Idx: 0 Loss: 0.020124717430194217
Epoch: 26 Idx: 5000 Loss: 0.017503208145539614
Epoch: 27 Idx: 0 Loss: 0.03172762583049973
Epoch: 27 Idx: 5000 Loss: 0.01127213717660028
Epoch: 28 Idx: 0 Loss: 0.01573628836462631
Epoch: 28 Idx: 5000 Loss: 0.016188691170383192
Epoch: 29 Idx: 0 Loss: 0.01361828840240349
Epoch: 29 Idx: 5000 Loss: 0.01575214769024327
Epoch: 30 Idx: 0 Loss: 0.0041084469227808335
Epoch: 30 Idx: 5000 Loss: 0.01554518086546344
Epoch: 31 Idx: 0 Loss: 0.021702529571504096
Epoch: 31 Idx: 5000 Loss: 0.020644366999883236
Epoch: 32 Idx: 0 Loss: 0.012343297309908609
Epoch: 32 Idx: 5000 Loss: 0.012588657875794355
Epoch: 33 Idx: 0 Loss: 0.008531056525162265
Epoch: 33 Idx: 5000 Loss: 0.02148949730921315
Epoch: 34 Idx: 0 Loss: 0.01995839893207048
Epoch: 34 Idx: 5000 Loss: 0.02081130324350973
Epoch: 35 Idx: 0 Loss: 0.016406836946878674
Epoch: 35 Idx: 5000 Loss: 0.012914685769452136
Epoch: 36 Idx: 0 Loss: 0.012428153823903984
Epoch: 36 Idx: 5000 Loss: 0.01046175502736232
Epoch: 37 Idx: 0 Loss: 0.015603735933480014
Epoch: 37 Idx: 5000 Loss: 0.014181068583592468
Epoch: 38 Idx: 0 Loss: 0.015084231774905029
Epoch: 38 Idx: 5000 Loss: 0.012687199052601272
Epoch: 39 Idx: 0 Loss: 0.012497943671508643
Epoch: 39 Idx: 5000 Loss: 0.017477492341923622
Epoch: 40 Idx: 0 Loss: 0.016696881866757073
Epoch: 40 Idx: 5000 Loss: 0.011929775145286328
Epoch: 41 Idx: 0 Loss: 0.00802280618220928
Epoch: 41 Idx: 5000 Loss: 0.02209302254837652
Epoch: 42 Idx: 0 Loss: 0.049989795869740936
Epoch: 42 Idx: 5000 Loss: 0.014306499580117088
Epoch: 43 Idx: 0 Loss: 0.008192341904976266
Epoch: 43 Idx: 5000 Loss: 0.007634786797173753
Epoch: 44 Idx: 0 Loss: 0.02651022656009706
Epoch: 44 Idx: 5000 Loss: 0.014825852954583624
Epoch: 45 Idx: 0 Loss: 0.014163004494101891
Epoch: 45 Idx: 5000 Loss: 0.02173932458877528
Epoch: 46 Idx: 0 Loss: 0.017220225159496554
Epoch: 46 Idx: 5000 Loss: 0.03720835866999671
Epoch: 47 Idx: 0 Loss: 0.01662405965342652
Epoch: 47 Idx: 5000 Loss: 0.026616342776971932
Epoch: 48 Idx: 0 Loss: 0.019928212708905643
Epoch: 48 Idx: 5000 Loss: 0.028402058883891655
Epoch: 49 Idx: 0 Loss: 0.020093581526264878
Epoch: 49 Idx: 5000 Loss: 0.015829462185444107
Len (direct inputs):  561
Inputs len 1568 15 4130
Len (direct inputs):  2577
Starting sliding window evaluation...
Step 6/7
Val onto:  [('edas', 'sigkdd')] test_onto:  [('conference', 'edas')]
Training size: 106045 Testing size: 7817
Epoch: 0 Idx: 0 Loss: 0.24851640213034756
Epoch: 0 Idx: 5000 Loss: 0.01887375736063952
Epoch: 1 Idx: 0 Loss: 0.04476865444071998
Epoch: 1 Idx: 5000 Loss: 0.012840689511551881
Epoch: 2 Idx: 0 Loss: 0.03681453200127885
Epoch: 2 Idx: 5000 Loss: 0.017265701378461423
Epoch: 3 Idx: 0 Loss: 0.017304515716268756
Epoch: 3 Idx: 5000 Loss: 0.02172102735879599
Epoch: 4 Idx: 0 Loss: 0.01856052449957426
Epoch: 4 Idx: 5000 Loss: 0.021343123773707725
Epoch: 5 Idx: 0 Loss: 0.03584925828900981
Epoch: 5 Idx: 5000 Loss: 0.004978741015795367
Epoch: 6 Idx: 0 Loss: 0.019763064854394975
Epoch: 6 Idx: 5000 Loss: 0.01874715581869846
Epoch: 7 Idx: 0 Loss: 0.03369281914810561
Epoch: 7 Idx: 5000 Loss: 0.012004251778172326
Epoch: 8 Idx: 0 Loss: 0.020927990111060397
Epoch: 8 Idx: 5000 Loss: 0.027364277466117724
Epoch: 9 Idx: 0 Loss: 0.022615599499894546
Epoch: 9 Idx: 5000 Loss: 0.009056762527851493
Epoch: 10 Idx: 0 Loss: 0.009164258376555814
Epoch: 10 Idx: 5000 Loss: 0.015445230268971398
Epoch: 11 Idx: 0 Loss: 0.0035248724954385037
Epoch: 11 Idx: 5000 Loss: 0.020581579294453377
Epoch: 12 Idx: 0 Loss: 0.015739692725415182
Epoch: 12 Idx: 5000 Loss: 0.018075592434471877
Epoch: 13 Idx: 0 Loss: 0.009604826258014414
Epoch: 13 Idx: 5000 Loss: 0.012744408052796176
Epoch: 14 Idx: 0 Loss: 0.02167414464058133
Epoch: 14 Idx: 5000 Loss: 0.006765483192220853
Epoch: 15 Idx: 0 Loss: 0.03193778548360786
Epoch: 15 Idx: 5000 Loss: 0.010672010382379662
Epoch: 16 Idx: 0 Loss: 0.012451793446902195
Epoch: 16 Idx: 5000 Loss: 0.01684827927392852
Epoch: 17 Idx: 0 Loss: 0.009379220180800613
Epoch: 17 Idx: 5000 Loss: 0.024356988964637654
Epoch: 18 Idx: 0 Loss: 0.01073231331802214
Epoch: 18 Idx: 5000 Loss: 0.011569075211063058
Epoch: 19 Idx: 0 Loss: 0.008900743011869328
Epoch: 19 Idx: 5000 Loss: 0.02524819028120647
Epoch: 20 Idx: 0 Loss: 0.011488701746763199
Epoch: 20 Idx: 5000 Loss: 0.011122010008638923
Epoch: 21 Idx: 0 Loss: 0.014343839268962025
Epoch: 21 Idx: 5000 Loss: 0.018635762573088837
Epoch: 22 Idx: 0 Loss: 0.0066572065202904224
Epoch: 22 Idx: 5000 Loss: 0.029009608524497705
Epoch: 23 Idx: 0 Loss: 0.013479229352572153
Epoch: 23 Idx: 5000 Loss: 0.025451520941153546
Epoch: 24 Idx: 0 Loss: 0.006072743801013919
Epoch: 24 Idx: 5000 Loss: 0.006208501226606926
Epoch: 25 Idx: 0 Loss: 0.01281975843999014
Epoch: 25 Idx: 5000 Loss: 0.017019822880648544
Epoch: 26 Idx: 0 Loss: 0.024994360132368373
Epoch: 26 Idx: 5000 Loss: 0.009786892004331747
Epoch: 27 Idx: 0 Loss: 0.008540741450438594
Epoch: 27 Idx: 5000 Loss: 0.03210165616152493
Epoch: 28 Idx: 0 Loss: 0.018145791771104525
Epoch: 28 Idx: 5000 Loss: 0.021780248096454888
Epoch: 29 Idx: 0 Loss: 0.012063137676349743
Epoch: 29 Idx: 5000 Loss: 0.03462239223309767
Epoch: 30 Idx: 0 Loss: 0.020850687442413522
Epoch: 30 Idx: 5000 Loss: 0.00864697255965797
Epoch: 31 Idx: 0 Loss: 0.012165346411312662
Epoch: 31 Idx: 5000 Loss: 0.021790077058074224
Epoch: 32 Idx: 0 Loss: 0.013479654036198236
Epoch: 32 Idx: 5000 Loss: 0.009001168755601231
Epoch: 33 Idx: 0 Loss: 0.014056671871414373
Epoch: 33 Idx: 5000 Loss: 0.02810265329169597
Epoch: 34 Idx: 0 Loss: 0.006673399252559526
Epoch: 34 Idx: 5000 Loss: 0.01335868466900637
Epoch: 35 Idx: 0 Loss: 0.035678617516500324
Epoch: 35 Idx: 5000 Loss: 0.013737175866069855
Epoch: 36 Idx: 0 Loss: 0.014567793731255914
Epoch: 36 Idx: 5000 Loss: 0.00904159245616634
Epoch: 37 Idx: 0 Loss: 0.01825551090649222
Epoch: 37 Idx: 5000 Loss: 0.015997583881936565
Epoch: 38 Idx: 0 Loss: 0.01454896886030366
Epoch: 38 Idx: 5000 Loss: 0.007128095300954307
Epoch: 39 Idx: 0 Loss: 0.01577058708122967
Epoch: 39 Idx: 5000 Loss: 0.01726380603531247
Epoch: 40 Idx: 0 Loss: 0.015025036602727186
Epoch: 40 Idx: 5000 Loss: 0.019707056136777253
Epoch: 41 Idx: 0 Loss: 0.020203754950303986
Epoch: 41 Idx: 5000 Loss: 0.005430735955868762
Epoch: 42 Idx: 0 Loss: 0.011061899441649259
Epoch: 42 Idx: 5000 Loss: 0.010950612134831283
Epoch: 43 Idx: 0 Loss: 0.01859939123631959
Epoch: 43 Idx: 5000 Loss: 0.012295334757533749
Epoch: 44 Idx: 0 Loss: 0.011755442901521404
Epoch: 44 Idx: 5000 Loss: 0.013100251926998843
Epoch: 45 Idx: 0 Loss: 0.01783609617435296
Epoch: 45 Idx: 5000 Loss: 0.016282398292732286
Epoch: 46 Idx: 0 Loss: 0.018260827774422405
Epoch: 46 Idx: 5000 Loss: 0.011597588005281899
Epoch: 47 Idx: 0 Loss: 0.013699475221696757
Epoch: 47 Idx: 5000 Loss: 0.0061219398504050535
Epoch: 48 Idx: 0 Loss: 0.011282662707265516
Epoch: 48 Idx: 5000 Loss: 0.04134649814121694
Epoch: 49 Idx: 0 Loss: 0.007686671071737968
Epoch: 49 Idx: 5000 Loss: 0.009717779623649439
Len (direct inputs):  877
Inputs len 5600 17 7800
Len (direct inputs):  2217
Starting sliding window evaluation...
Step 8/7
Val onto:  [('cmt', 'iasted')] test_onto:  [('confof', 'sigkdd')]
Training size: 111351 Testing size: 2336
Epoch: 0 Idx: 0 Loss: 0.20225328246025837
Epoch: 0 Idx: 5000 Loss: 0.031057712994782008
Epoch: 1 Idx: 0 Loss: 0.020942496132197518
Epoch: 1 Idx: 5000 Loss: 0.010901147913169188
Epoch: 2 Idx: 0 Loss: 0.00661266632299537
Epoch: 2 Idx: 5000 Loss: 0.014981886727877227
Epoch: 3 Idx: 0 Loss: 0.017929486057043094
Epoch: 3 Idx: 5000 Loss: 0.00872101879120438
Epoch: 4 Idx: 0 Loss: 0.011285563053546688
Epoch: 4 Idx: 5000 Loss: 0.035728015210927874
Epoch: 5 Idx: 0 Loss: 0.007369176574390622
Epoch: 5 Idx: 5000 Loss: 0.007520074175905042
Epoch: 6 Idx: 0 Loss: 0.011380995989531878
Epoch: 6 Idx: 5000 Loss: 0.009349910988395712
Epoch: 7 Idx: 0 Loss: 0.03233906326369722
Epoch: 7 Idx: 5000 Loss: 0.019735172974822364
Epoch: 8 Idx: 0 Loss: 0.006314604791643168
Epoch: 8 Idx: 5000 Loss: 0.018400695447140607
Epoch: 9 Idx: 0 Loss: 0.020706247697526145
Epoch: 9 Idx: 5000 Loss: 0.015672091156196274
Epoch: 10 Idx: 0 Loss: 0.013608640199080663
Epoch: 10 Idx: 5000 Loss: 0.0264898355837873
Epoch: 11 Idx: 0 Loss: 0.017858891992386734
Epoch: 11 Idx: 5000 Loss: 0.014400059106463461
Epoch: 12 Idx: 0 Loss: 0.010557945127987996
Epoch: 12 Idx: 5000 Loss: 0.016947821561436694
Epoch: 13 Idx: 0 Loss: 0.010464447145764992
Epoch: 13 Idx: 5000 Loss: 0.009580317991673993
Epoch: 14 Idx: 0 Loss: 0.024808022072362886
Epoch: 14 Idx: 5000 Loss: 0.01945263721178422
Epoch: 15 Idx: 0 Loss: 0.011006009204106191
Epoch: 15 Idx: 5000 Loss: 0.009367522736234726
Epoch: 16 Idx: 0 Loss: 0.013303044746876932
Epoch: 16 Idx: 5000 Loss: 0.019664131463932852
Epoch: 17 Idx: 0 Loss: 0.02225359563151495
Epoch: 17 Idx: 5000 Loss: 0.027553538073904284
Epoch: 18 Idx: 0 Loss: 0.012428508399462558
Epoch: 18 Idx: 5000 Loss: 0.02627938293466599
Epoch: 19 Idx: 0 Loss: 0.02058097559977811
Epoch: 19 Idx: 5000 Loss: 0.01457481444235358
Epoch: 20 Idx: 0 Loss: 0.006965797107022886
Epoch: 20 Idx: 5000 Loss: 0.01103119579876238
Epoch: 21 Idx: 0 Loss: 0.03390330997229009
Traceback (most recent call last):
  File "main.py", line 505, in <module>
    inputs = np.array(to_feature(inputs_all[batch_start: batch_end]))
  File "main.py", line 387, in to_feature
    for elem in inputs_lenpadded]
  File "main.py", line 387, in <listcomp>
    for elem in inputs_lenpadded]
  File "main.py", line 386, in <listcomp>
    for nbr_type in ent] for ent in elem]
  File "main.py", line 386, in <listcomp>
    for nbr_type in ent] for ent in elem]
  File "main.py", line 385, in <listcomp>
    for i in range(max_paths - len(nbr_type))]
  File "main.py", line 384, in <listcomp>
    inputs_pathpadded = [[[nbr_type + [[0 for j in range(max_pathlen)]
KeyboardInterrupt

------------------------------------------------------------
Sender: LSF System <rer@dccxc239>
Subject: Job 4066831: <python main.py 4 10 False False> in cluster <dcc> Exited

Job <python main.py 4 10 False False> was submitted from host <dccxl004> by user <harshitk> in cluster <dcc> at Tue Sep 15 15:48:37 2020
Job was executed on host(s) <dccxc239>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep 15 15:48:38 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/VeeAlign/src> was used as the working directory.
Started at Tue Sep 15 15:48:38 2020
Terminated at Wed Sep 16 04:38:40 2020
Results reported at Wed Sep 16 04:38:40 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python main.py 4 10 False False
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   46190.80 sec.
    Max Memory :                                 2911 MB
    Average Memory :                             2728.86 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40506.00 MB
    Max Swap :                                   2 MB
    Max Processes :                              3
    Max Threads :                                13
    Run time :                                   46229 sec.
    Turnaround time :                            46203 sec.

The output (if any) is above this job summary.

