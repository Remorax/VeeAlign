2020-09-15 15:49:39.028703: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-09-15 15:49:42.170020: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1
2020-09-15 15:49:42.294081: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:1b:00.0 name: Tesla K80 computeCapability: 3.7
coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s
2020-09-15 15:49:42.294180: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-09-15 15:49:42.296206: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
2020-09-15 15:49:42.297618: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2020-09-15 15:49:42.298446: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2020-09-15 15:49:42.300279: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2020-09-15 15:49:42.301661: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10
2020-09-15 15:49:42.301887: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudnn.so.7'; dlerror: libcudnn.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/share/lsf-9.1.3/10.1/linux3.10-glibc2.17-x86_64/lib:=/opt/share/gcc-4.9.2_rhel6/x86_64/lib/:/opt/share/gcc-4.9.2_rhel6/x86_64/lib64:/opt/share/Python-3.6.2/x86_64/lib:=/opt/share/gcc-5.4.0/x86_64/lib/:/opt/share/gcc-5.4.0/x86_64/lib64:/opt/share/isl-0.17/x86_64/lib/:/opt/share/protobuf-3.1.0/x86_64/lib/:/opt/share/leveldb-1.19/x86_64/lib/:/opt/share/boost-1.62.0/x86_64/lib/:/opt/share/torch-7/x86_64/install/lib:/opt/share/Python-2.7.12/x86_64/lib:/opt/share/Python-3.5.2/x86_64/lib:/opt/share/cuDNN-v5.1-8.0/cuda/lib64:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/opt/share/cuda-8.0/
2020-09-15 15:49:42.301909: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1753] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2020-09-15 15:49:42.302240: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-09-15 15:49:42.309287: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2600075000 Hz
2020-09-15 15:49:42.309485: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x563657a12340 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-09-15 15:49:42.309506: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-09-15 15:49:42.311413: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-09-15 15:49:42.311456: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      
Prefix path:  /dccstor/cogfin/arvind/da/VeeAlign/
Ontologies being aligned are:  [('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/confOf.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/conference.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/conference.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/conference.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/conference.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/confOf.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/conference.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/confOf.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/confOf.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/confOf.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl')]
Total number of extracted unique classes and properties from entire RA set:  829
Constructing abbrevation resolution dict....
Results after abbreviation resolution:  {'PC': 'Program Committee', 'OC': 'Organizing Committee'}
Resolving abbreviations...
Number of entities: 119639
Starting sliding window evaluation...
Step 0/7
Val onto:  [('confof', 'ekaw')] test_onto:  [('conference', 'sigkdd')]
Training size: 112565 Testing size: 3871
Epoch: 0 Idx: 0 Loss: 0.17916947733652297
Epoch: 0 Idx: 5000 Loss: 0.0107603254576538
Epoch: 1 Idx: 0 Loss: 0.01017242088640836
Epoch: 1 Idx: 5000 Loss: 0.00969400387629069
Epoch: 2 Idx: 0 Loss: 0.006343500259023601
Epoch: 2 Idx: 5000 Loss: 0.02010302282582527
Epoch: 3 Idx: 0 Loss: 0.030076858600803186
Epoch: 3 Idx: 5000 Loss: 0.03151182827467307
Epoch: 4 Idx: 0 Loss: 0.021080983148359298
Epoch: 4 Idx: 5000 Loss: 0.013947387307897867
Epoch: 5 Idx: 0 Loss: 0.02338609908084474
Epoch: 5 Idx: 5000 Loss: 0.012282091417782474
Epoch: 6 Idx: 0 Loss: 0.013551083360762748
Epoch: 6 Idx: 5000 Loss: 0.010374174459330705
Epoch: 7 Idx: 0 Loss: 0.010069870487944955
Epoch: 7 Idx: 5000 Loss: 0.01559016938076704
Epoch: 8 Idx: 0 Loss: 0.01656759348995486
Epoch: 8 Idx: 5000 Loss: 0.007281092304384378
Epoch: 9 Idx: 0 Loss: 0.008537066340129434
Epoch: 9 Idx: 5000 Loss: 0.018228976807804262
Epoch: 10 Idx: 0 Loss: 0.015216747889632665
Epoch: 10 Idx: 5000 Loss: 0.016121098478556452
Epoch: 11 Idx: 0 Loss: 0.014314889272641112
Epoch: 11 Idx: 5000 Loss: 0.00922979897778346
Epoch: 12 Idx: 0 Loss: 0.008945470662762894
Epoch: 12 Idx: 5000 Loss: 0.011350207342596574
Epoch: 13 Idx: 0 Loss: 0.014538657760542213
Epoch: 13 Idx: 5000 Loss: 0.007633919655964518
Epoch: 14 Idx: 0 Loss: 0.0374783464918924
Epoch: 14 Idx: 5000 Loss: 0.008419003336612697
Epoch: 15 Idx: 0 Loss: 0.009228475854643418
Epoch: 15 Idx: 5000 Loss: 0.01709614400720609
Epoch: 16 Idx: 0 Loss: 0.015118220347403781
Epoch: 16 Idx: 5000 Loss: 0.011612477645894476
Epoch: 17 Idx: 0 Loss: 0.013014039895204131
Epoch: 17 Idx: 5000 Loss: 0.026328174572418934
Epoch: 18 Idx: 0 Loss: 0.014260059594707952
Epoch: 18 Idx: 5000 Loss: 0.014195662834097419
Epoch: 19 Idx: 0 Loss: 0.014255461654619331
Epoch: 19 Idx: 5000 Loss: 0.029623876496870247
Epoch: 20 Idx: 0 Loss: 0.00633096160254839
Epoch: 20 Idx: 5000 Loss: 0.011266508534694101
Epoch: 21 Idx: 0 Loss: 0.009972617574805775
Epoch: 21 Idx: 5000 Loss: 0.013366406630883653
Epoch: 22 Idx: 0 Loss: 0.008091351714398906
Epoch: 22 Idx: 5000 Loss: 0.01014099091260555
Epoch: 23 Idx: 0 Loss: 0.013746589967266095
Epoch: 23 Idx: 5000 Loss: 0.01814867299092377
Epoch: 24 Idx: 0 Loss: 0.01265870375165884
Epoch: 24 Idx: 5000 Loss: 0.012643420522277836
Epoch: 25 Idx: 0 Loss: 0.01327844686836419
Epoch: 25 Idx: 5000 Loss: 0.012834537117231187
Epoch: 26 Idx: 0 Loss: 0.014542765174511627
Epoch: 26 Idx: 5000 Loss: 0.027878614867820815
Epoch: 27 Idx: 0 Loss: 0.014516807697387871
Epoch: 27 Idx: 5000 Loss: 0.02333463728637871
Epoch: 28 Idx: 0 Loss: 0.004066140325007864
Epoch: 28 Idx: 5000 Loss: 0.015433350016467466
Epoch: 29 Idx: 0 Loss: 0.014897328340915866
Epoch: 29 Idx: 5000 Loss: 0.034961107821565675
Epoch: 30 Idx: 0 Loss: 0.0141182055925997
Epoch: 30 Idx: 5000 Loss: 0.004368745575508105
Epoch: 31 Idx: 0 Loss: 0.014425197526129223
Epoch: 31 Idx: 5000 Loss: 0.010004561975268653
Epoch: 32 Idx: 0 Loss: 0.012333709001513378
Epoch: 32 Idx: 5000 Loss: 0.015491750874146034
Epoch: 33 Idx: 0 Loss: 0.012010429482256326
Epoch: 33 Idx: 5000 Loss: 0.008802721038733283
Epoch: 34 Idx: 0 Loss: 0.010092094648823397
Epoch: 34 Idx: 5000 Loss: 0.016813274540779497
Epoch: 35 Idx: 0 Loss: 0.007373465008247466
Epoch: 35 Idx: 5000 Loss: 0.011426543203390864
Epoch: 36 Idx: 0 Loss: 0.02458608309115383
Epoch: 36 Idx: 5000 Loss: 0.010029236865131045
Epoch: 37 Idx: 0 Loss: 0.012991087158780352
Epoch: 37 Idx: 5000 Loss: 0.01926458002515361
Epoch: 38 Idx: 0 Loss: 0.016612759704093226
Epoch: 38 Idx: 5000 Loss: 0.021867018790062205
Epoch: 39 Idx: 0 Loss: 0.013259668628134389
Epoch: 39 Idx: 5000 Loss: 0.014877424125157766
Epoch: 40 Idx: 0 Loss: 0.027051704302153867
Epoch: 40 Idx: 5000 Loss: 0.009883023339188852
Epoch: 41 Idx: 0 Loss: 0.011867927906545124
Epoch: 41 Idx: 5000 Loss: 0.007798031083271154
Epoch: 42 Idx: 0 Loss: 0.014161330871424656
Epoch: 42 Idx: 5000 Loss: 0.008048267317168773
Epoch: 43 Idx: 0 Loss: 0.01614641400161138
Epoch: 43 Idx: 5000 Loss: 0.017066954827673515
Epoch: 44 Idx: 0 Loss: 0.01860070238452663
Epoch: 44 Idx: 5000 Loss: 0.012458331996163212
Epoch: 45 Idx: 0 Loss: 0.012375074033044015
Epoch: 45 Idx: 5000 Loss: 0.007409124138515395
Epoch: 46 Idx: 0 Loss: 0.028700267636459914
Epoch: 46 Idx: 5000 Loss: 0.016306695805205603
Epoch: 47 Idx: 0 Loss: 0.017067872883533994
Epoch: 47 Idx: 5000 Loss: 0.01086335094247367
Epoch: 48 Idx: 0 Loss: 0.01399803332450976
Epoch: 48 Idx: 5000 Loss: 0.03291365326503683
Epoch: 49 Idx: 0 Loss: 0.011645893920917938
Epoch: 49 Idx: 5000 Loss: 0.020007938485390432
Len (direct inputs):  429
Inputs len 2744 15 3856
Len (direct inputs):  1127
Starting sliding window evaluation...
Step 2/7
Val onto:  [('conference', 'ekaw')] test_onto:  [('cmt', 'sigkdd')]
Training size: 111450 Testing size: 2364
Epoch: 0 Idx: 0 Loss: 0.1465703865752817
Epoch: 0 Idx: 5000 Loss: 0.02500943132194074
Epoch: 1 Idx: 0 Loss: 0.018349027482624236
Epoch: 1 Idx: 5000 Loss: 0.02576234933403329
Epoch: 2 Idx: 0 Loss: 0.022407977809430078
Epoch: 2 Idx: 5000 Loss: 0.013119457887332871
Epoch: 3 Idx: 0 Loss: 0.016823935117373155
Epoch: 3 Idx: 5000 Loss: 0.018856982468982104
Epoch: 4 Idx: 0 Loss: 0.009007592632611187
Epoch: 4 Idx: 5000 Loss: 0.03337246882687666
Epoch: 5 Idx: 0 Loss: 0.008065196537736167
Epoch: 5 Idx: 5000 Loss: 0.006154635695609471
Epoch: 6 Idx: 0 Loss: 0.029262585398944788
Epoch: 6 Idx: 5000 Loss: 0.010982575735991072
Epoch: 7 Idx: 0 Loss: 0.027173097621669313
Epoch: 7 Idx: 5000 Loss: 0.01431613347347777
Epoch: 8 Idx: 0 Loss: 0.01489085847933069
Epoch: 8 Idx: 5000 Loss: 0.009806806310612209
Epoch: 9 Idx: 0 Loss: 0.007925351396849847
Epoch: 9 Idx: 5000 Loss: 0.010073391975457804
Epoch: 10 Idx: 0 Loss: 0.01692159377892764
Epoch: 10 Idx: 5000 Loss: 0.008824164448376302
Epoch: 11 Idx: 0 Loss: 0.01877263456288128
Epoch: 11 Idx: 5000 Loss: 0.01079018637198311
Epoch: 12 Idx: 0 Loss: 0.021568308318730457
Epoch: 12 Idx: 5000 Loss: 0.0195659482891446
Epoch: 13 Idx: 0 Loss: 0.01323231222171136
Epoch: 13 Idx: 5000 Loss: 0.012197321801551215
Epoch: 14 Idx: 0 Loss: 0.04716214044838354
Epoch: 14 Idx: 5000 Loss: 0.01000327593293113
Epoch: 15 Idx: 0 Loss: 0.02674703160095703
Epoch: 15 Idx: 5000 Loss: 0.019458407325098112
Epoch: 16 Idx: 0 Loss: 0.017612753658428622
Epoch: 16 Idx: 5000 Loss: 0.04142334530976348
Epoch: 17 Idx: 0 Loss: 0.011855178188179372
Epoch: 17 Idx: 5000 Loss: 0.014644927072625769
Epoch: 18 Idx: 0 Loss: 0.020960265002073562
Epoch: 18 Idx: 5000 Loss: 0.02244721500977298
Epoch: 19 Idx: 0 Loss: 0.02913860168981518
Epoch: 19 Idx: 5000 Loss: 0.015170792835690717
Epoch: 20 Idx: 0 Loss: 0.008818523090428056
Epoch: 20 Idx: 5000 Loss: 0.014551611971990445
Epoch: 21 Idx: 0 Loss: 0.009889505800714187
Epoch: 21 Idx: 5000 Loss: 0.03124883135965067
Epoch: 22 Idx: 0 Loss: 0.05146095492271781
Epoch: 22 Idx: 5000 Loss: 0.00866431350277273
Epoch: 23 Idx: 0 Loss: 0.016203762789682018
Epoch: 23 Idx: 5000 Loss: 0.023055840553907064
Epoch: 24 Idx: 0 Loss: 0.010491888339042197
Epoch: 24 Idx: 5000 Loss: 0.015414792789721857
Epoch: 25 Idx: 0 Loss: 0.01623195298961617
Epoch: 25 Idx: 5000 Loss: 0.018786657861519132
Epoch: 26 Idx: 0 Loss: 0.0422887063310754
Epoch: 26 Idx: 5000 Loss: 0.010528892684040947
Epoch: 27 Idx: 0 Loss: 0.02598310513885163
Epoch: 27 Idx: 5000 Loss: 0.022603372423841497
Epoch: 28 Idx: 0 Loss: 0.009897250144235217
Epoch: 28 Idx: 5000 Loss: 0.008528558452325611
Epoch: 29 Idx: 0 Loss: 0.018039027382237473
Epoch: 29 Idx: 5000 Loss: 0.02216988126193841
Epoch: 30 Idx: 0 Loss: 0.01009049494879869
Epoch: 30 Idx: 5000 Loss: 0.009771656153317827
Epoch: 31 Idx: 0 Loss: 0.013938421262740034
Epoch: 31 Idx: 5000 Loss: 0.011577460455725056
Epoch: 32 Idx: 0 Loss: 0.013762168668136121
Epoch: 32 Idx: 5000 Loss: 0.047793953627424515
Epoch: 33 Idx: 0 Loss: 0.0038778108222122876
Epoch: 33 Idx: 5000 Loss: 0.014010566129994365
Epoch: 34 Idx: 0 Loss: 0.007882024389898198
Epoch: 34 Idx: 5000 Loss: 0.029300169790950275
Epoch: 35 Idx: 0 Loss: 0.028292193146178455
Epoch: 35 Idx: 5000 Loss: 0.013604337375055781
Epoch: 36 Idx: 0 Loss: 0.015310223193063476
Epoch: 36 Idx: 5000 Loss: 0.008904183838576987
Epoch: 37 Idx: 0 Loss: 0.03835412407759424
Epoch: 37 Idx: 5000 Loss: 0.014536898310852787
Epoch: 38 Idx: 0 Loss: 0.020393407353616118
Epoch: 38 Idx: 5000 Loss: 0.022283793005025965
Epoch: 39 Idx: 0 Loss: 0.007232276013620044
Epoch: 39 Idx: 5000 Loss: 0.01790161426505986
Epoch: 40 Idx: 0 Loss: 0.011008986175653233
Epoch: 40 Idx: 5000 Loss: 0.017985997931341074
Epoch: 41 Idx: 0 Loss: 0.01188503094188829
Epoch: 41 Idx: 5000 Loss: 0.014206525059699904
Epoch: 42 Idx: 0 Loss: 0.034989633742340945
Epoch: 42 Idx: 5000 Loss: 0.011060001178583214
Epoch: 43 Idx: 0 Loss: 0.010417410921390051
Epoch: 43 Idx: 5000 Loss: 0.007558091311929456
Epoch: 44 Idx: 0 Loss: 0.019384177570881754
Epoch: 44 Idx: 5000 Loss: 0.01735917103624083
Epoch: 45 Idx: 0 Loss: 0.010217296166696027
Epoch: 45 Idx: 5000 Loss: 0.01196433286439443
Epoch: 46 Idx: 0 Loss: 0.021876853440819174
Epoch: 46 Idx: 5000 Loss: 0.029563345056867366
Epoch: 47 Idx: 0 Loss: 0.002768792789605107
Epoch: 47 Idx: 5000 Loss: 0.03422733608626129
Epoch: 48 Idx: 0 Loss: 0.008373909829944532
Epoch: 48 Idx: 5000 Loss: 0.025491323537571612
Epoch: 49 Idx: 0 Loss: 0.02777280491582506
Epoch: 49 Idx: 5000 Loss: 0.006629871358831316
Len (direct inputs):  1737
Inputs len 1372 12 2352
Len (direct inputs):  992
Starting sliding window evaluation...
Step 4/7
Val onto:  [('ekaw', 'sigkdd')] test_onto:  [('cmt', 'conference')]
Training size: 111356 Testing size: 4145
Epoch: 0 Idx: 0 Loss: 0.15049105911731017
Epoch: 0 Idx: 5000 Loss: 0.01620184353787094
Epoch: 1 Idx: 0 Loss: 0.014207124589114287
Epoch: 1 Idx: 5000 Loss: 0.009654763150526404
Epoch: 2 Idx: 0 Loss: 0.018428695387064566
Epoch: 2 Idx: 5000 Loss: 0.013713893799122598
Epoch: 3 Idx: 0 Loss: 0.016083891102232715
Epoch: 3 Idx: 5000 Loss: 0.010561346669386806
Epoch: 4 Idx: 0 Loss: 0.016785067993440973
Epoch: 4 Idx: 5000 Loss: 0.019664177086763445
Epoch: 5 Idx: 0 Loss: 0.019837538474460552
Epoch: 5 Idx: 5000 Loss: 0.009278199133406639
Epoch: 6 Idx: 0 Loss: 0.018013673522973474
Epoch: 6 Idx: 5000 Loss: 0.030214488857616115
Epoch: 7 Idx: 0 Loss: 0.024166999445794042
Epoch: 7 Idx: 5000 Loss: 0.011314507955291334
Epoch: 8 Idx: 0 Loss: 0.008183294866443497
Epoch: 8 Idx: 5000 Loss: 0.014226471635215647
Epoch: 9 Idx: 0 Loss: 0.014110228289218754
Epoch: 9 Idx: 5000 Loss: 0.01953289967366177
Epoch: 10 Idx: 0 Loss: 0.018728764777778958
Epoch: 10 Idx: 5000 Loss: 0.02322124295886947
Epoch: 11 Idx: 0 Loss: 0.010799093195833515
Epoch: 11 Idx: 5000 Loss: 0.016971091393575992
Epoch: 12 Idx: 0 Loss: 0.025834914625396107
Epoch: 12 Idx: 5000 Loss: 0.017893757384319718
Epoch: 13 Idx: 0 Loss: 0.00862279972189407
Epoch: 13 Idx: 5000 Loss: 0.004586358756421938
Epoch: 14 Idx: 0 Loss: 0.012029012976788786
Epoch: 14 Idx: 5000 Loss: 0.01387436495630273
Epoch: 15 Idx: 0 Loss: 0.020158151124523925
Epoch: 15 Idx: 5000 Loss: 0.017490513581235365
Epoch: 16 Idx: 0 Loss: 0.009699265962730729
Epoch: 16 Idx: 5000 Loss: 0.015783292401508137
Epoch: 17 Idx: 0 Loss: 0.025827297080479792
Epoch: 17 Idx: 5000 Loss: 0.03624453243609911
Epoch: 18 Idx: 0 Loss: 0.01503667664020606
Epoch: 18 Idx: 5000 Loss: 0.011325632070561267
Epoch: 19 Idx: 0 Loss: 0.006935019618028293
Epoch: 19 Idx: 5000 Loss: 0.021088011065946134
Epoch: 20 Idx: 0 Loss: 0.01041424264769075
Epoch: 20 Idx: 5000 Loss: 0.01976573526787278
Epoch: 21 Idx: 0 Loss: 0.02184862244948501
Epoch: 21 Idx: 5000 Loss: 0.017613213841309483
Epoch: 22 Idx: 0 Loss: 0.016347631563503363
Epoch: 22 Idx: 5000 Loss: 0.016389293817334224
Epoch: 23 Idx: 0 Loss: 0.009077299657672082
Epoch: 23 Idx: 5000 Loss: 0.008393046248937816
Epoch: 24 Idx: 0 Loss: 0.0230986956838841
Epoch: 24 Idx: 5000 Loss: 0.02538060088000386
Epoch: 25 Idx: 0 Loss: 0.035467755912297595
Epoch: 25 Idx: 5000 Loss: 0.03492885203665387
Epoch: 26 Idx: 0 Loss: 0.013579513094492807
Epoch: 26 Idx: 5000 Loss: 0.02504631369571241
Epoch: 27 Idx: 0 Loss: 0.01690860294192626
Epoch: 27 Idx: 5000 Loss: 0.009253451483290046
Epoch: 28 Idx: 0 Loss: 0.013778697802076858
Epoch: 28 Idx: 5000 Loss: 0.01005336832002805
Epoch: 29 Idx: 0 Loss: 0.00801136260605422
Epoch: 29 Idx: 5000 Loss: 0.012623106547854989
Epoch: 30 Idx: 0 Loss: 0.029113717268435126
Epoch: 30 Idx: 5000 Loss: 0.006029241348346105
Epoch: 31 Idx: 0 Loss: 0.01569650081043154
Epoch: 31 Idx: 5000 Loss: 0.015450215577812128
Epoch: 32 Idx: 0 Loss: 0.009210687212933914
Epoch: 32 Idx: 5000 Loss: 0.009358141336041405
Epoch: 33 Idx: 0 Loss: 0.004456366378281681
Epoch: 33 Idx: 5000 Loss: 0.013201019641475535
Epoch: 34 Idx: 0 Loss: 0.022650276459370407
Epoch: 34 Idx: 5000 Loss: 0.015793031156492335
Epoch: 35 Idx: 0 Loss: 0.021415805775324682
Epoch: 35 Idx: 5000 Loss: 0.01352513081473314
Epoch: 36 Idx: 0 Loss: 0.011829154848934657
Epoch: 36 Idx: 5000 Loss: 0.041098921076617966
Epoch: 37 Idx: 0 Loss: 0.010877727197303037
Epoch: 37 Idx: 5000 Loss: 0.015698439514664188
Epoch: 38 Idx: 0 Loss: 0.01653724338044824
Epoch: 38 Idx: 5000 Loss: 0.004914488559369107
Epoch: 39 Idx: 0 Loss: 0.015085664349543617
Epoch: 39 Idx: 5000 Loss: 0.012050691136773619
Epoch: 40 Idx: 0 Loss: 0.013456548249663526
Epoch: 40 Idx: 5000 Loss: 0.01616581274109232
Epoch: 41 Idx: 0 Loss: 0.013165816826885312
Epoch: 41 Idx: 5000 Loss: 0.017607512955591507
Epoch: 42 Idx: 0 Loss: 0.021143969426761687
Epoch: 42 Idx: 5000 Loss: 0.01821363578348454
Epoch: 43 Idx: 0 Loss: 0.009231954233738841
Epoch: 43 Idx: 5000 Loss: 0.04064075006258338
Epoch: 44 Idx: 0 Loss: 0.010939731627651186
Epoch: 44 Idx: 5000 Loss: 0.0362957739214539
Epoch: 45 Idx: 0 Loss: 0.014611116526716416
Epoch: 45 Idx: 5000 Loss: 0.02279167489747203
Epoch: 46 Idx: 0 Loss: 0.013118661905647663
Epoch: 46 Idx: 5000 Loss: 0.010238172212524191
Epoch: 47 Idx: 0 Loss: 0.028051577452337238
Epoch: 47 Idx: 5000 Loss: 0.01796723683990805
Epoch: 48 Idx: 0 Loss: 0.01075882197862172
Epoch: 48 Idx: 5000 Loss: 0.017842765607972173
Epoch: 49 Idx: 0 Loss: 0.0377702583121667
Epoch: 49 Idx: 5000 Loss: 0.009216230179430787
Len (direct inputs):  561
Inputs len 1568 15 4130
Len (direct inputs):  2577
Starting sliding window evaluation...
Step 6/7
Val onto:  [('edas', 'sigkdd')] test_onto:  [('conference', 'edas')]
Training size: 106045 Testing size: 7817
Epoch: 0 Idx: 0 Loss: 0.2501142492769806
Epoch: 0 Idx: 5000 Loss: 0.01745988492045813
Epoch: 1 Idx: 0 Loss: 0.014155131819142934
Epoch: 1 Idx: 5000 Loss: 0.0076006263229433345
Epoch: 2 Idx: 0 Loss: 0.008961501771909935
Epoch: 2 Idx: 5000 Loss: 0.01686091404043356
Epoch: 3 Idx: 0 Loss: 0.00799576445764224
Epoch: 3 Idx: 5000 Loss: 0.009732115399063365
Epoch: 4 Idx: 0 Loss: 0.018764098325173423
Epoch: 4 Idx: 5000 Loss: 0.016099880559071324
Epoch: 5 Idx: 0 Loss: 0.029336366052665788
Epoch: 5 Idx: 5000 Loss: 0.01387568335329785
Epoch: 6 Idx: 0 Loss: 0.02255348355046536
Epoch: 6 Idx: 5000 Loss: 0.012192725759811425
Epoch: 7 Idx: 0 Loss: 0.01827186529873965
Epoch: 7 Idx: 5000 Loss: 0.030132539406202252
Epoch: 8 Idx: 0 Loss: 0.01003697552826325
Epoch: 8 Idx: 5000 Loss: 0.013387681725832087
Epoch: 9 Idx: 0 Loss: 0.009312617775088102
Epoch: 9 Idx: 5000 Loss: 0.013141633453679049
Epoch: 10 Idx: 0 Loss: 0.0195281414913019
Epoch: 10 Idx: 5000 Loss: 0.020880484768327073
Epoch: 11 Idx: 0 Loss: 0.008717435473160548
Epoch: 11 Idx: 5000 Loss: 0.01447439623342231
Epoch: 12 Idx: 0 Loss: 0.012345680259253942
Epoch: 12 Idx: 5000 Loss: 0.01869128584410118
Epoch: 13 Idx: 0 Loss: 0.010393054882010665
Epoch: 13 Idx: 5000 Loss: 0.020926787786160864
Epoch: 14 Idx: 0 Loss: 0.013710771546276824
Epoch: 14 Idx: 5000 Loss: 0.01774703130620733
Epoch: 15 Idx: 0 Loss: 0.012535310916024205
Epoch: 15 Idx: 5000 Loss: 0.007297022131185312
Epoch: 16 Idx: 0 Loss: 0.026746690243748775
Epoch: 16 Idx: 5000 Loss: 0.026796190770960217
Epoch: 17 Idx: 0 Loss: 0.015814790907854533
Epoch: 17 Idx: 5000 Loss: 0.01930434565901627
Epoch: 18 Idx: 0 Loss: 0.012896182571273052
Epoch: 18 Idx: 5000 Loss: 0.011238099792192723
Epoch: 19 Idx: 0 Loss: 0.009779256149122175
Epoch: 19 Idx: 5000 Loss: 0.012660050256604403
Epoch: 20 Idx: 0 Loss: 0.04329888789723552
Epoch: 20 Idx: 5000 Loss: 0.012144190392236852
Epoch: 21 Idx: 0 Loss: 0.011653176883045626
Epoch: 21 Idx: 5000 Loss: 0.01159120797326544
Epoch: 22 Idx: 0 Loss: 0.011271637144644022
Epoch: 22 Idx: 5000 Loss: 0.02748063468775485
Epoch: 23 Idx: 0 Loss: 0.008712001515229982
Epoch: 23 Idx: 5000 Loss: 0.01060391320910034
Epoch: 24 Idx: 0 Loss: 0.014047340404687875
Epoch: 24 Idx: 5000 Loss: 0.0067292963363206415
Epoch: 25 Idx: 0 Loss: 0.014089444886950317
Epoch: 25 Idx: 5000 Loss: 0.02600172743755032
Epoch: 26 Idx: 0 Loss: 0.022038021541607826
Epoch: 26 Idx: 5000 Loss: 0.02100311606688826
Epoch: 27 Idx: 0 Loss: 0.02476816971557747
Epoch: 27 Idx: 5000 Loss: 0.018727288479039105
Epoch: 28 Idx: 0 Loss: 0.023915456745662
Epoch: 28 Idx: 5000 Loss: 0.010342636847441774
Epoch: 29 Idx: 0 Loss: 0.0029029265471996767
Epoch: 29 Idx: 5000 Loss: 0.023556036985705
Epoch: 30 Idx: 0 Loss: 0.024324417612303223
Epoch: 30 Idx: 5000 Loss: 0.006947631150818406
Epoch: 31 Idx: 0 Loss: 0.013011597776780952
Epoch: 31 Idx: 5000 Loss: 0.0037659282929314824
Epoch: 32 Idx: 0 Loss: 0.021871358595247
Epoch: 32 Idx: 5000 Loss: 0.008085900438331283
Epoch: 33 Idx: 0 Loss: 0.008083921639045082
Epoch: 33 Idx: 5000 Loss: 0.05093015445793926
Epoch: 34 Idx: 0 Loss: 0.030940320876897194
Epoch: 34 Idx: 5000 Loss: 0.011731323449748057
Epoch: 35 Idx: 0 Loss: 0.02705763277914589
Epoch: 35 Idx: 5000 Loss: 0.014406648931056225
Epoch: 36 Idx: 0 Loss: 0.01481075862551199
Epoch: 36 Idx: 5000 Loss: 0.009096514418388884
Epoch: 37 Idx: 0 Loss: 0.01233691540963482
Epoch: 37 Idx: 5000 Loss: 0.04658290575171767
Epoch: 38 Idx: 0 Loss: 0.015721615896335233
Epoch: 38 Idx: 5000 Loss: 0.013819642258142056
Epoch: 39 Idx: 0 Loss: 0.014285678464424382
Epoch: 39 Idx: 5000 Loss: 0.018918947609281467
Epoch: 40 Idx: 0 Loss: 0.006386914425643654
Epoch: 40 Idx: 5000 Loss: 0.016463341298723462
Epoch: 41 Idx: 0 Loss: 0.01853760150058243
Epoch: 41 Idx: 5000 Loss: 0.011186752974936383
Epoch: 42 Idx: 0 Loss: 0.004286915292297673
Epoch: 42 Idx: 5000 Loss: 0.01945395739314104
Epoch: 43 Idx: 0 Loss: 0.028305794312196823
Epoch: 43 Idx: 5000 Loss: 0.011951781892203136
Epoch: 44 Idx: 0 Loss: 0.023672104874580973
Epoch: 44 Idx: 5000 Loss: 0.017989380602670583
Epoch: 45 Idx: 0 Loss: 0.00996787283200401
Epoch: 45 Idx: 5000 Loss: 0.005527853144430987
Epoch: 46 Idx: 0 Loss: 0.019615307971939732
Epoch: 46 Idx: 5000 Loss: 0.015684874619261706
Epoch: 47 Idx: 0 Loss: 0.009440893286938298
Epoch: 47 Idx: 5000 Loss: 0.020254129083833033
Epoch: 48 Idx: 0 Loss: 0.014717020315675539
Epoch: 48 Idx: 5000 Loss: 0.0259432713175581
Epoch: 49 Idx: 0 Loss: 0.02655668921787799
Epoch: 49 Idx: 5000 Loss: 0.018403015833554085
Len (direct inputs):  877
Inputs len 5600 17 7800
Len (direct inputs):  2217
Starting sliding window evaluation...
Step 8/7
Val onto:  [('cmt', 'iasted')] test_onto:  [('confof', 'sigkdd')]
Training size: 111351 Testing size: 2336
Epoch: 0 Idx: 0 Loss: 0.20660197919181394
Epoch: 0 Idx: 5000 Loss: 0.03271610658560318
Epoch: 1 Idx: 0 Loss: 0.01105137549293273
Epoch: 1 Idx: 5000 Loss: 0.01831814932875501
Epoch: 2 Idx: 0 Loss: 0.024371658598708965
Epoch: 2 Idx: 5000 Loss: 0.014221037434331589
Epoch: 3 Idx: 0 Loss: 0.019729443079044284
Epoch: 3 Idx: 5000 Loss: 0.02696366193007822
Epoch: 4 Idx: 0 Loss: 0.023955347451347482
Epoch: 4 Idx: 5000 Loss: 0.014123716686490494
Epoch: 5 Idx: 0 Loss: 0.013294264312681173
Epoch: 5 Idx: 5000 Loss: 0.01903428639106084
Epoch: 6 Idx: 0 Loss: 0.05158392658783356
Epoch: 6 Idx: 5000 Loss: 0.019585371874006743
Epoch: 7 Idx: 0 Loss: 0.019198664752313712
Epoch: 7 Idx: 5000 Loss: 0.01190381258189881
Epoch: 8 Idx: 0 Loss: 0.008801845034785448
Epoch: 8 Idx: 5000 Loss: 0.02161437819828455
Epoch: 9 Idx: 0 Loss: 0.009595679080737038
Epoch: 9 Idx: 5000 Loss: 0.023323182272440027
Epoch: 10 Idx: 0 Loss: 0.009212386005782381
Epoch: 10 Idx: 5000 Loss: 0.02203896631740066
Epoch: 11 Idx: 0 Loss: 0.009443177422819936
Epoch: 11 Idx: 5000 Loss: 0.004755677668493072
Epoch: 12 Idx: 0 Loss: 0.012288147549331701
Epoch: 12 Idx: 5000 Loss: 0.00792506747498883
Epoch: 13 Idx: 0 Loss: 0.012870285682487662
Epoch: 13 Idx: 5000 Loss: 0.016740201707142792
Epoch: 14 Idx: 0 Loss: 0.01609902967225564
Epoch: 14 Idx: 5000 Loss: 0.00867955105586947
Epoch: 15 Idx: 0 Loss: 0.006278213425198825
Epoch: 15 Idx: 5000 Loss: 0.027716181105751823
Epoch: 16 Idx: 0 Loss: 0.037743989834860624
Epoch: 16 Idx: 5000 Loss: 0.01326634315996116
Epoch: 17 Idx: 0 Loss: 0.012172261145977959
Epoch: 17 Idx: 5000 Loss: 0.025446414796269336
Epoch: 18 Idx: 0 Loss: 0.019534708129776025
Epoch: 18 Idx: 5000 Loss: 0.024561641954731747
Epoch: 19 Idx: 0 Loss: 0.01739926018119735
Epoch: 19 Idx: 5000 Loss: 0.00954991449119811
Epoch: 20 Idx: 0 Loss: 0.00539952449192179
Epoch: 20 Idx: 5000 Loss: 0.021156135120977297
Epoch: 21 Idx: 0 Loss: 0.014928529620208965
Epoch: 21 Idx: 5000 Loss: 0.016013135475458674
Epoch: 22 Idx: 0 Loss: 0.029615036138183705
Epoch: 22 Idx: 5000 Loss: 0.009869394304982721
Epoch: 23 Idx: 0 Loss: 0.007773082876398422
Epoch: 23 Idx: 5000 Loss: 0.014021328533697825
Epoch: 24 Idx: 0 Loss: 0.030012278668444275
Epoch: 24 Idx: 5000 Loss: 0.013102062126319827
Epoch: 25 Idx: 0 Loss: 0.018901342470866753
Epoch: 25 Idx: 5000 Loss: 0.02417979814694708
Epoch: 26 Idx: 0 Loss: 0.009796567079472875
Epoch: 26 Idx: 5000 Loss: 0.013600526361306427
Epoch: 27 Idx: 0 Loss: 0.012819739943924765
Epoch: 27 Idx: 5000 Loss: 0.019703907305417726
Epoch: 28 Idx: 0 Loss: 0.007260116766994066
Epoch: 28 Idx: 5000 Loss: 0.015575182439603007
Epoch: 29 Idx: 0 Loss: 0.007955305785476917
Epoch: 29 Idx: 5000 Loss: 0.012772394834481981
Epoch: 30 Idx: 0 Loss: 0.005558611320579105
Epoch: 30 Idx: 5000 Loss: 0.02409850407472944
Epoch: 31 Idx: 0 Loss: 0.03532764178910723
Epoch: 31 Idx: 5000 Loss: 0.01640533926654292
Epoch: 32 Idx: 0 Loss: 0.00996044857992304
Epoch: 32 Idx: 5000 Loss: 0.04538369884673978
Epoch: 33 Idx: 0 Loss: 0.011889335612633913
Epoch: 33 Idx: 5000 Loss: 0.008265918994182029
Epoch: 34 Idx: 0 Loss: 0.023419466166081586
Epoch: 34 Idx: 5000 Loss: 0.008947082118079515
Epoch: 35 Idx: 0 Loss: 0.022330511344508773
Epoch: 35 Idx: 5000 Loss: 0.030973987387382485
Epoch: 36 Idx: 0 Loss: 0.009185895669167707
Epoch: 36 Idx: 5000 Loss: 0.008096345930049582
Epoch: 37 Idx: 0 Loss: 0.013932279268479189
Epoch: 37 Idx: 5000 Loss: 0.015106248942014646
Epoch: 38 Idx: 0 Loss: 0.013894979513596705
Epoch: 38 Idx: 5000 Loss: 0.009029193893435546
Epoch: 39 Idx: 0 Loss: 0.006605500621728415
Epoch: 39 Idx: 5000 Loss: 0.022141836470015285
Epoch: 40 Idx: 0 Loss: 0.03763608400384611
Epoch: 40 Idx: 5000 Loss: 0.03234724652701629
Epoch: 41 Idx: 0 Loss: 0.01158455481004042
Epoch: 41 Idx: 5000 Loss: 0.012573942895356275
Epoch: 42 Idx: 0 Loss: 0.009061947835150531
Epoch: 42 Idx: 5000 Loss: 0.018702231463386745
Epoch: 43 Idx: 0 Loss: 0.03712182317367206
Epoch: 43 Idx: 5000 Loss: 0.009218037982494296
Epoch: 44 Idx: 0 Loss: 0.01975604043166728
Epoch: 44 Idx: 5000 Loss: 0.020895209961183794
Epoch: 45 Idx: 0 Loss: 0.010689038918059498
Epoch: 45 Idx: 5000 Loss: 0.02002665705933872
Epoch: 46 Idx: 0 Loss: 0.032170712428050835
Traceback (most recent call last):
  File "main.py", line 517, in <module>
    loss.backward()
  File "/u/harshitk/miniconda3/envs/py37/lib/python3.7/site-packages/torch/tensor.py", line 185, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/u/harshitk/miniconda3/envs/py37/lib/python3.7/site-packages/torch/autograd/__init__.py", line 127, in backward
    allow_unreachable=True)  # allow_unreachable flag
KeyboardInterrupt

------------------------------------------------------------
Sender: LSF System <rer@dccxc241>
Subject: Job 4066876: <python main.py 6 4 False True> in cluster <dcc> Exited

Job <python main.py 6 4 False True> was submitted from host <dccxl004> by user <harshitk> in cluster <dcc> at Tue Sep 15 15:48:40 2020
Job was executed on host(s) <dccxc241>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep 15 15:49:37 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/VeeAlign/src> was used as the working directory.
Started at Tue Sep 15 15:49:37 2020
Terminated at Wed Sep 16 04:38:41 2020
Results reported at Wed Sep 16 04:38:41 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python main.py 6 4 False True
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   46081.88 sec.
    Max Memory :                                 2908 MB
    Average Memory :                             2741.39 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40509.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                13
    Run time :                                   46171 sec.
    Turnaround time :                            46201 sec.

The output (if any) is above this job summary.

