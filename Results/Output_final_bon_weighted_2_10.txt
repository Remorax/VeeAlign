2020-09-15 15:49:42.206548: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-09-15 15:49:45.385207: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1
2020-09-15 15:49:45.504875: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:14:00.0 name: Tesla K80 computeCapability: 3.7
coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s
2020-09-15 15:49:45.504991: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-09-15 15:49:45.507105: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
2020-09-15 15:49:45.508588: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2020-09-15 15:49:45.509005: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2020-09-15 15:49:45.510830: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2020-09-15 15:49:45.512154: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10
2020-09-15 15:49:45.512372: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudnn.so.7'; dlerror: libcudnn.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/share/lsf-9.1.3/10.1/linux3.10-glibc2.17-x86_64/lib:=/opt/share/gcc-4.9.2_rhel6/x86_64/lib/:/opt/share/gcc-4.9.2_rhel6/x86_64/lib64:/opt/share/Python-3.6.2/x86_64/lib:=/opt/share/gcc-5.4.0/x86_64/lib/:/opt/share/gcc-5.4.0/x86_64/lib64:/opt/share/isl-0.17/x86_64/lib/:/opt/share/protobuf-3.1.0/x86_64/lib/:/opt/share/leveldb-1.19/x86_64/lib/:/opt/share/boost-1.62.0/x86_64/lib/:/opt/share/torch-7/x86_64/install/lib:/opt/share/Python-2.7.12/x86_64/lib:/opt/share/Python-3.5.2/x86_64/lib:/opt/share/cuDNN-v5.1-8.0/cuda/lib64:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/opt/share/cuda-8.0/
2020-09-15 15:49:45.512395: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1753] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2020-09-15 15:49:45.512738: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-09-15 15:49:45.520676: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2600080000 Hz
2020-09-15 15:49:45.520888: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c6a80a40b0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-09-15 15:49:45.520909: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-09-15 15:49:45.523166: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-09-15 15:49:45.523228: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      
Prefix path:  /dccstor/cogfin/arvind/da/VeeAlign/
Ontologies being aligned are:  [('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/confOf.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/conference.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/conference.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/conference.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/conference.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/confOf.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/conference.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/confOf.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/confOf.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/confOf.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl')]
Total number of extracted unique classes and properties from entire RA set:  829
Constructing abbrevation resolution dict....
Results after abbreviation resolution:  {'PC': 'Program Committee', 'OC': 'Organizing Committee'}
Resolving abbreviations...
Number of entities: 119639
Starting sliding window evaluation...
Step 0/7
Val onto:  [('confof', 'ekaw')] test_onto:  [('conference', 'sigkdd')]
Training size: 112565 Testing size: 3871
Epoch: 0 Idx: 0 Loss: 0.1876895169807623
Epoch: 0 Idx: 5000 Loss: 0.009113438159954957
Epoch: 1 Idx: 0 Loss: 0.02078033366537317
Epoch: 1 Idx: 5000 Loss: 0.013958980347946192
Epoch: 2 Idx: 0 Loss: 0.010727029370831104
Epoch: 2 Idx: 5000 Loss: 0.014785647896706711
Epoch: 3 Idx: 0 Loss: 0.037910415975292835
Epoch: 3 Idx: 5000 Loss: 0.009625795153623308
Epoch: 4 Idx: 0 Loss: 0.010104818520496323
Epoch: 4 Idx: 5000 Loss: 0.0114215220578999
Epoch: 5 Idx: 0 Loss: 0.01514952667282586
Epoch: 5 Idx: 5000 Loss: 0.017417736535570702
Epoch: 6 Idx: 0 Loss: 0.03326718780121815
Epoch: 6 Idx: 5000 Loss: 0.027225169724394906
Epoch: 7 Idx: 0 Loss: 0.009741923550183598
Epoch: 7 Idx: 5000 Loss: 0.03831479774764088
Epoch: 8 Idx: 0 Loss: 0.02603918616785368
Epoch: 8 Idx: 5000 Loss: 0.011800243370883215
Epoch: 9 Idx: 0 Loss: 0.021790881211334073
Epoch: 9 Idx: 5000 Loss: 0.018907406410848655
Epoch: 10 Idx: 0 Loss: 0.010543295099345786
Epoch: 10 Idx: 5000 Loss: 0.040552996904990604
Epoch: 11 Idx: 0 Loss: 0.022710552096860983
Epoch: 11 Idx: 5000 Loss: 0.02618693841699641
Epoch: 12 Idx: 0 Loss: 0.005209903433721408
Epoch: 12 Idx: 5000 Loss: 0.007132503039219536
Epoch: 13 Idx: 0 Loss: 0.008493455923183024
Epoch: 13 Idx: 5000 Loss: 0.0026450418978612345
Epoch: 14 Idx: 0 Loss: 0.012707346396549985
Epoch: 14 Idx: 5000 Loss: 0.012689070475966256
Epoch: 15 Idx: 0 Loss: 0.01466833125365482
Epoch: 15 Idx: 5000 Loss: 0.013527850155839974
Epoch: 16 Idx: 0 Loss: 0.012194034423844775
Epoch: 16 Idx: 5000 Loss: 0.014579493779608037
Epoch: 17 Idx: 0 Loss: 0.013612107387424565
Epoch: 17 Idx: 5000 Loss: 0.02452469335443786
Epoch: 18 Idx: 0 Loss: 0.018113880897659123
Epoch: 18 Idx: 5000 Loss: 0.012270238653543583
Epoch: 19 Idx: 0 Loss: 0.018158719426647044
Epoch: 19 Idx: 5000 Loss: 0.011833248682399702
Epoch: 20 Idx: 0 Loss: 0.005215995612301203
Epoch: 20 Idx: 5000 Loss: 0.01531609322691178
Epoch: 21 Idx: 0 Loss: 0.019180911942876445
Epoch: 21 Idx: 5000 Loss: 0.021524427887206043
Epoch: 22 Idx: 0 Loss: 0.013270597350700826
Epoch: 22 Idx: 5000 Loss: 0.012793583425339412
Epoch: 23 Idx: 0 Loss: 0.008179663722563424
Epoch: 23 Idx: 5000 Loss: 0.017786927232553883
Epoch: 24 Idx: 0 Loss: 0.019998240730482654
Epoch: 24 Idx: 5000 Loss: 0.007125356130330539
Epoch: 25 Idx: 0 Loss: 0.025219640853750516
Epoch: 25 Idx: 5000 Loss: 0.02958934304408252
Epoch: 26 Idx: 0 Loss: 0.006879473852102704
Epoch: 26 Idx: 5000 Loss: 0.03595023952926382
Epoch: 27 Idx: 0 Loss: 0.005567690376194558
Epoch: 27 Idx: 5000 Loss: 0.026552292191076222
Epoch: 28 Idx: 0 Loss: 0.06246381566724686
Epoch: 28 Idx: 5000 Loss: 0.02884301567525719
Epoch: 29 Idx: 0 Loss: 0.012603971802448866
Epoch: 29 Idx: 5000 Loss: 0.01077842628101186
Epoch: 30 Idx: 0 Loss: 0.00990157955478946
Epoch: 30 Idx: 5000 Loss: 0.009809676747861377
Epoch: 31 Idx: 0 Loss: 0.0156542020185267
Epoch: 31 Idx: 5000 Loss: 0.013107878823336982
Epoch: 32 Idx: 0 Loss: 0.018301956600318515
Epoch: 32 Idx: 5000 Loss: 0.005284426998901406
Epoch: 33 Idx: 0 Loss: 0.019597819458433623
Epoch: 33 Idx: 5000 Loss: 0.03963892924301803
Epoch: 34 Idx: 0 Loss: 0.003163103129596516
Epoch: 34 Idx: 5000 Loss: 0.01504975136412965
Epoch: 35 Idx: 0 Loss: 0.017946510834862126
Epoch: 35 Idx: 5000 Loss: 0.020793502275813176
Epoch: 36 Idx: 0 Loss: 0.010847800452419202
Epoch: 36 Idx: 5000 Loss: 0.02089358561697889
Epoch: 37 Idx: 0 Loss: 0.03172247596774678
Epoch: 37 Idx: 5000 Loss: 0.015069828571604115
Epoch: 38 Idx: 0 Loss: 0.027267816557976596
Epoch: 38 Idx: 5000 Loss: 0.040416407197205226
Epoch: 39 Idx: 0 Loss: 0.02082022806558474
Epoch: 39 Idx: 5000 Loss: 0.017425289960839835
Epoch: 40 Idx: 0 Loss: 0.00661356726187727
Epoch: 40 Idx: 5000 Loss: 0.014436982240231846
Epoch: 41 Idx: 0 Loss: 0.007683291652338839
Epoch: 41 Idx: 5000 Loss: 0.014419945506674954
Epoch: 42 Idx: 0 Loss: 0.008073578359958167
Epoch: 42 Idx: 5000 Loss: 0.021981042647798734
Epoch: 43 Idx: 0 Loss: 0.027750585686035084
Epoch: 43 Idx: 5000 Loss: 0.015866022910813876
Epoch: 44 Idx: 0 Loss: 0.0114761602110719
Epoch: 44 Idx: 5000 Loss: 0.010795803328596427
Epoch: 45 Idx: 0 Loss: 0.017851185660547733
Epoch: 45 Idx: 5000 Loss: 0.004980238729296678
Epoch: 46 Idx: 0 Loss: 0.01858199605703837
Epoch: 46 Idx: 5000 Loss: 0.023316588494964856
Epoch: 47 Idx: 0 Loss: 0.02094118873045945
Epoch: 47 Idx: 5000 Loss: 0.015682379019907083
Epoch: 48 Idx: 0 Loss: 0.00983477992091876
Epoch: 48 Idx: 5000 Loss: 0.01411438539962053
Epoch: 49 Idx: 0 Loss: 0.008528049440564086
Epoch: 49 Idx: 5000 Loss: 0.018151590270031014
Len (direct inputs):  429
Inputs len 2744 15 3856
Len (direct inputs):  1127
Starting sliding window evaluation...
Step 2/7
Val onto:  [('conference', 'ekaw')] test_onto:  [('cmt', 'sigkdd')]
Training size: 111450 Testing size: 2364
Epoch: 0 Idx: 0 Loss: 0.15121992969421638
Epoch: 0 Idx: 5000 Loss: 0.048786365130735226
Epoch: 1 Idx: 0 Loss: 0.009110228963105342
Epoch: 1 Idx: 5000 Loss: 0.0275804168134547
Epoch: 2 Idx: 0 Loss: 0.011287396950867324
Epoch: 2 Idx: 5000 Loss: 0.007281452047739339
Epoch: 3 Idx: 0 Loss: 0.022378411360697976
Epoch: 3 Idx: 5000 Loss: 0.024425969812168984
Epoch: 4 Idx: 0 Loss: 0.01277817354932613
Epoch: 4 Idx: 5000 Loss: 0.008137607427354952
Epoch: 5 Idx: 0 Loss: 0.010015906550422504
Epoch: 5 Idx: 5000 Loss: 0.013945492302043744
Epoch: 6 Idx: 0 Loss: 0.021194151742501673
Epoch: 6 Idx: 5000 Loss: 0.013210724659500704
Epoch: 7 Idx: 0 Loss: 0.03846330824589072
Epoch: 7 Idx: 5000 Loss: 0.01749083080997422
Epoch: 8 Idx: 0 Loss: 0.007808029458017705
Epoch: 8 Idx: 5000 Loss: 0.011609192118198802
Epoch: 9 Idx: 0 Loss: 0.038911658950119526
Epoch: 9 Idx: 5000 Loss: 0.019044167734693935
Epoch: 10 Idx: 0 Loss: 0.0060029290786870705
Epoch: 10 Idx: 5000 Loss: 0.011729817296101622
Epoch: 11 Idx: 0 Loss: 0.017409665505805662
Epoch: 11 Idx: 5000 Loss: 0.006075671506494877
Epoch: 12 Idx: 0 Loss: 0.012983743067509348
Epoch: 12 Idx: 5000 Loss: 0.011115838363893852
Epoch: 13 Idx: 0 Loss: 0.01814688444170318
Epoch: 13 Idx: 5000 Loss: 0.02781041407334075
Epoch: 14 Idx: 0 Loss: 0.0059659201638245
Epoch: 14 Idx: 5000 Loss: 0.011566468934547961
Epoch: 15 Idx: 0 Loss: 0.008784337094196492
Epoch: 15 Idx: 5000 Loss: 0.032359064444631984
Epoch: 16 Idx: 0 Loss: 0.013132802975458598
Epoch: 16 Idx: 5000 Loss: 0.03251210651074041
Epoch: 17 Idx: 0 Loss: 0.013167846900186615
Epoch: 17 Idx: 5000 Loss: 0.013040824146356518
Epoch: 18 Idx: 0 Loss: 0.053596328508032955
Epoch: 18 Idx: 5000 Loss: 0.015756280718768143
Epoch: 19 Idx: 0 Loss: 0.016764666400393834
Epoch: 19 Idx: 5000 Loss: 0.017317222201209577
Epoch: 20 Idx: 0 Loss: 0.014300413490701432
Epoch: 20 Idx: 5000 Loss: 0.015242319018129696
Epoch: 21 Idx: 0 Loss: 0.012960380259839132
Epoch: 21 Idx: 5000 Loss: 0.028228607991314547
Epoch: 22 Idx: 0 Loss: 0.01255456684614287
Epoch: 22 Idx: 5000 Loss: 0.01527994291875907
Epoch: 23 Idx: 0 Loss: 0.04499110368647064
Epoch: 23 Idx: 5000 Loss: 0.02271327879701934
Epoch: 24 Idx: 0 Loss: 0.01670620794404184
Epoch: 24 Idx: 5000 Loss: 0.01309475486902921
Epoch: 25 Idx: 0 Loss: 0.02100309661145735
Epoch: 25 Idx: 5000 Loss: 0.013920203762985727
Epoch: 26 Idx: 0 Loss: 0.023700237791282875
Epoch: 26 Idx: 5000 Loss: 0.016076206161043584
Epoch: 27 Idx: 0 Loss: 0.011028655780582635
Epoch: 27 Idx: 5000 Loss: 0.004702701205312639
Epoch: 28 Idx: 0 Loss: 0.018078052189518197
Epoch: 28 Idx: 5000 Loss: 0.010206077255991854
Epoch: 29 Idx: 0 Loss: 0.020734730827801372
Epoch: 29 Idx: 5000 Loss: 0.012802184108120552
Epoch: 30 Idx: 0 Loss: 0.038043960455453614
Epoch: 30 Idx: 5000 Loss: 0.03200229656796071
Epoch: 31 Idx: 0 Loss: 0.01430947516413519
Epoch: 31 Idx: 5000 Loss: 0.01470042900613554
Epoch: 32 Idx: 0 Loss: 0.012168555741586292
Epoch: 32 Idx: 5000 Loss: 0.044782107293632745
Epoch: 33 Idx: 0 Loss: 0.02800534063052084
Epoch: 33 Idx: 5000 Loss: 0.008975626094359434
Epoch: 34 Idx: 0 Loss: 0.010712086335252134
Epoch: 34 Idx: 5000 Loss: 0.00882806013347448
Epoch: 35 Idx: 0 Loss: 0.012582933519398112
Epoch: 35 Idx: 5000 Loss: 0.01420293959143059
Epoch: 36 Idx: 0 Loss: 0.010308445235545052
Epoch: 36 Idx: 5000 Loss: 0.013415680764548578
Epoch: 37 Idx: 0 Loss: 0.013084618930467572
Epoch: 37 Idx: 5000 Loss: 0.015379207174982288
Epoch: 38 Idx: 0 Loss: 0.01717758305902941
Epoch: 38 Idx: 5000 Loss: 0.00981600393208972
Epoch: 39 Idx: 0 Loss: 0.013936671803734285
Epoch: 39 Idx: 5000 Loss: 0.010572688025022911
Epoch: 40 Idx: 0 Loss: 0.030127922205262583
Epoch: 40 Idx: 5000 Loss: 0.04292496329895429
Epoch: 41 Idx: 0 Loss: 0.013019500669516178
Epoch: 41 Idx: 5000 Loss: 0.021686558429992037
Epoch: 42 Idx: 0 Loss: 0.0058950437551080195
Epoch: 42 Idx: 5000 Loss: 0.028276297921192642
Epoch: 43 Idx: 0 Loss: 0.01682864310527773
Epoch: 43 Idx: 5000 Loss: 0.015966803051275028
Epoch: 44 Idx: 0 Loss: 0.028247659180726963
Epoch: 44 Idx: 5000 Loss: 0.014557471267863466
Epoch: 45 Idx: 0 Loss: 0.010473876499149007
Epoch: 45 Idx: 5000 Loss: 0.006278909712167987
Epoch: 46 Idx: 0 Loss: 0.01817526703353605
Epoch: 46 Idx: 5000 Loss: 0.01636164953846481
Epoch: 47 Idx: 0 Loss: 0.008192130632759628
Epoch: 47 Idx: 5000 Loss: 0.017390560687791434
Epoch: 48 Idx: 0 Loss: 0.012839900875623287
Epoch: 48 Idx: 5000 Loss: 0.027501669346396584
Epoch: 49 Idx: 0 Loss: 0.009267467010863983
Epoch: 49 Idx: 5000 Loss: 0.025813708934028845
Len (direct inputs):  1737
Inputs len 1372 12 2352
Len (direct inputs):  992
Starting sliding window evaluation...
Step 4/7
Val onto:  [('ekaw', 'sigkdd')] test_onto:  [('cmt', 'conference')]
Training size: 111356 Testing size: 4145
Epoch: 0 Idx: 0 Loss: 0.13602403170901942
Epoch: 0 Idx: 5000 Loss: 0.01629488723974609
Epoch: 1 Idx: 0 Loss: 0.018627597228099892
Epoch: 1 Idx: 5000 Loss: 0.00443506172434853
Epoch: 2 Idx: 0 Loss: 0.023436989031109554
Epoch: 2 Idx: 5000 Loss: 0.013544680704639054
Epoch: 3 Idx: 0 Loss: 0.012237846517632926
Epoch: 3 Idx: 5000 Loss: 0.011180911904234063
Epoch: 4 Idx: 0 Loss: 0.011713999277091765
Epoch: 4 Idx: 5000 Loss: 0.009277214729224275
Epoch: 5 Idx: 0 Loss: 0.025422911716806038
Epoch: 5 Idx: 5000 Loss: 0.009169812825792054
Epoch: 6 Idx: 0 Loss: 0.013319853990429066
Epoch: 6 Idx: 5000 Loss: 0.007314946712018582
Epoch: 7 Idx: 0 Loss: 0.021211738799725106
Epoch: 7 Idx: 5000 Loss: 0.009351513126749266
Epoch: 8 Idx: 0 Loss: 0.008678245682923543
Epoch: 8 Idx: 5000 Loss: 0.01698204612355107
Epoch: 9 Idx: 0 Loss: 0.018249335439933283
Epoch: 9 Idx: 5000 Loss: 0.017400967293459725
Epoch: 10 Idx: 0 Loss: 0.010264871367976316
Epoch: 10 Idx: 5000 Loss: 0.01803438081605788
Epoch: 11 Idx: 0 Loss: 0.016887678540227893
Epoch: 11 Idx: 5000 Loss: 0.02560836060622839
Epoch: 12 Idx: 0 Loss: 0.020979263354143324
Epoch: 12 Idx: 5000 Loss: 0.0135662494433718
Epoch: 13 Idx: 0 Loss: 0.026633264579790296
Epoch: 13 Idx: 5000 Loss: 0.007929362372783017
Epoch: 14 Idx: 0 Loss: 0.01462992995180288
Epoch: 14 Idx: 5000 Loss: 0.02034324234011861
Epoch: 15 Idx: 0 Loss: 0.009470882838653662
Epoch: 15 Idx: 5000 Loss: 0.014702434878461892
Epoch: 16 Idx: 0 Loss: 0.013570766542857493
Epoch: 16 Idx: 5000 Loss: 0.00808596280075544
Epoch: 17 Idx: 0 Loss: 0.00941207480905109
Epoch: 17 Idx: 5000 Loss: 0.03598705652415785
Epoch: 18 Idx: 0 Loss: 0.01074788368739121
Epoch: 18 Idx: 5000 Loss: 0.021745952423697347
Epoch: 19 Idx: 0 Loss: 0.008826606448263961
Epoch: 19 Idx: 5000 Loss: 0.017100221710217568
Epoch: 20 Idx: 0 Loss: 0.015756633030796625
Epoch: 20 Idx: 5000 Loss: 0.01710066569877773
Epoch: 21 Idx: 0 Loss: 0.016933918131512
Epoch: 21 Idx: 5000 Loss: 0.02836361843703521
Epoch: 22 Idx: 0 Loss: 0.02809436600515875
Epoch: 22 Idx: 5000 Loss: 0.025245507943383434
Epoch: 23 Idx: 0 Loss: 0.037959721765052606
Epoch: 23 Idx: 5000 Loss: 0.009277395742514839
Epoch: 24 Idx: 0 Loss: 0.025867230473552723
Epoch: 24 Idx: 5000 Loss: 0.011272343473601343
Epoch: 25 Idx: 0 Loss: 0.007704502550533191
Epoch: 25 Idx: 5000 Loss: 0.014682490192551679
Epoch: 26 Idx: 0 Loss: 0.011138512420662695
Epoch: 26 Idx: 5000 Loss: 0.01568813889406996
Epoch: 27 Idx: 0 Loss: 0.012816255470454441
Epoch: 27 Idx: 5000 Loss: 0.02393408387422924
Epoch: 28 Idx: 0 Loss: 0.01253462677810081
Epoch: 28 Idx: 5000 Loss: 0.01911989225299541
Epoch: 29 Idx: 0 Loss: 0.008439618627259993
Epoch: 29 Idx: 5000 Loss: 0.020018589983003477
Epoch: 30 Idx: 0 Loss: 0.009507116236845903
Epoch: 30 Idx: 5000 Loss: 0.031372588410064434
Epoch: 31 Idx: 0 Loss: 0.013988314061190731
Epoch: 31 Idx: 5000 Loss: 0.03614463313525744
Epoch: 32 Idx: 0 Loss: 0.011782854567015489
Epoch: 32 Idx: 5000 Loss: 0.015187193347251698
Epoch: 33 Idx: 0 Loss: 0.010285097995238988
Epoch: 33 Idx: 5000 Loss: 0.009093160376136415
Epoch: 34 Idx: 0 Loss: 0.013246018692293741
Epoch: 34 Idx: 5000 Loss: 0.005901622582962568
Epoch: 35 Idx: 0 Loss: 0.014851664435992198
Epoch: 35 Idx: 5000 Loss: 0.01021785279069393
Epoch: 36 Idx: 0 Loss: 0.015536164340311623
Epoch: 36 Idx: 5000 Loss: 0.01898496485101618
Epoch: 37 Idx: 0 Loss: 0.015608826103344433
Epoch: 37 Idx: 5000 Loss: 0.005705349090026928
Epoch: 38 Idx: 0 Loss: 0.009326586793249839
Epoch: 38 Idx: 5000 Loss: 0.006532327804845908
Epoch: 39 Idx: 0 Loss: 0.009070701490809793
Epoch: 39 Idx: 5000 Loss: 0.019357912411883307
Epoch: 40 Idx: 0 Loss: 0.01647157067590696
Epoch: 40 Idx: 5000 Loss: 0.012321513410822126
Epoch: 41 Idx: 0 Loss: 0.006379755491574236
Epoch: 41 Idx: 5000 Loss: 0.011825403429646766
Epoch: 42 Idx: 0 Loss: 0.012007725990686759
Epoch: 42 Idx: 5000 Loss: 0.013371944355239718
Epoch: 43 Idx: 0 Loss: 0.010771757696049544
Epoch: 43 Idx: 5000 Loss: 0.023078693671086795
Epoch: 44 Idx: 0 Loss: 0.013769979544937319
Epoch: 44 Idx: 5000 Loss: 0.025215824840441246
Epoch: 45 Idx: 0 Loss: 0.020468433135928457
Epoch: 45 Idx: 5000 Loss: 0.00701925215337449
Epoch: 46 Idx: 0 Loss: 0.01246072642221915
Epoch: 46 Idx: 5000 Loss: 0.009530771890385585
Epoch: 47 Idx: 0 Loss: 0.019784182317746386
Epoch: 47 Idx: 5000 Loss: 0.015583046596179673
Epoch: 48 Idx: 0 Loss: 0.013836108722644562
Epoch: 48 Idx: 5000 Loss: 0.03171030559679136
Epoch: 49 Idx: 0 Loss: 0.015538057682593361
Epoch: 49 Idx: 5000 Loss: 0.008530619709613054
Len (direct inputs):  561
Inputs len 1568 15 4130
Len (direct inputs):  2577
Starting sliding window evaluation...
Step 6/7
Val onto:  [('edas', 'sigkdd')] test_onto:  [('conference', 'edas')]
Training size: 106045 Testing size: 7817
Epoch: 0 Idx: 0 Loss: 0.2277619836271041
Epoch: 0 Idx: 5000 Loss: 0.030422741284485154
Epoch: 1 Idx: 0 Loss: 0.012037790644937993
Epoch: 1 Idx: 5000 Loss: 0.02585704411399073
Epoch: 2 Idx: 0 Loss: 0.012173962334939009
Epoch: 2 Idx: 5000 Loss: 0.01580552188682148
Epoch: 3 Idx: 0 Loss: 0.01763517325449695
Epoch: 3 Idx: 5000 Loss: 0.008626722677886525
Epoch: 4 Idx: 0 Loss: 0.0150654173302141
Epoch: 4 Idx: 5000 Loss: 0.011825021216337142
Epoch: 5 Idx: 0 Loss: 0.01094295723552352
Epoch: 5 Idx: 5000 Loss: 0.007151722822825375
Epoch: 6 Idx: 0 Loss: 0.04034182663589122
Epoch: 6 Idx: 5000 Loss: 0.007637815831975916
Epoch: 7 Idx: 0 Loss: 0.007314022057189432
Epoch: 7 Idx: 5000 Loss: 0.008600858551932166
Epoch: 8 Idx: 0 Loss: 0.012443330111982148
Epoch: 8 Idx: 5000 Loss: 0.01784083101049325
Epoch: 9 Idx: 0 Loss: 0.016313036498376548
Epoch: 9 Idx: 5000 Loss: 0.007209013815051757
Epoch: 10 Idx: 0 Loss: 0.017433378144694586
Epoch: 10 Idx: 5000 Loss: 0.031127806512690818
Epoch: 11 Idx: 0 Loss: 0.012350077055517293
Epoch: 11 Idx: 5000 Loss: 0.018858336380783006
Epoch: 12 Idx: 0 Loss: 0.020780133332245418
Epoch: 12 Idx: 5000 Loss: 0.02313782811446591
Epoch: 13 Idx: 0 Loss: 0.015265275700516095
Epoch: 13 Idx: 5000 Loss: 0.06167543563389867
Epoch: 14 Idx: 0 Loss: 0.011205843099536199
Epoch: 14 Idx: 5000 Loss: 0.01619583023120251
Epoch: 15 Idx: 0 Loss: 0.0144091537615587
Epoch: 15 Idx: 5000 Loss: 0.00843998873903586
Epoch: 16 Idx: 0 Loss: 0.03796213841499636
Epoch: 16 Idx: 5000 Loss: 0.010020946390134235
Epoch: 17 Idx: 0 Loss: 0.011755086669077847
Epoch: 17 Idx: 5000 Loss: 0.020059539042822865
Epoch: 18 Idx: 0 Loss: 0.009260137995890353
Epoch: 18 Idx: 5000 Loss: 0.017121590885959673
Epoch: 19 Idx: 0 Loss: 0.010841565719705378
Epoch: 19 Idx: 5000 Loss: 0.008542181905576103
Epoch: 20 Idx: 0 Loss: 0.018833827871997774
Epoch: 20 Idx: 5000 Loss: 0.029703599155317298
Epoch: 21 Idx: 0 Loss: 0.00875270267930122
Epoch: 21 Idx: 5000 Loss: 0.012564983041373112
Epoch: 22 Idx: 0 Loss: 0.02695043744912128
Epoch: 22 Idx: 5000 Loss: 0.018155770267868368
Epoch: 23 Idx: 0 Loss: 0.01220205932314009
Epoch: 23 Idx: 5000 Loss: 0.015595414758553444
Epoch: 24 Idx: 0 Loss: 0.007106033331889448
Epoch: 24 Idx: 5000 Loss: 0.014653517559778229
Epoch: 25 Idx: 0 Loss: 0.01860031761173303
Epoch: 25 Idx: 5000 Loss: 0.014424026986276367
Epoch: 26 Idx: 0 Loss: 0.02195684340389062
Epoch: 26 Idx: 5000 Loss: 0.029123761221348646
Epoch: 27 Idx: 0 Loss: 0.011536400282454486
Epoch: 27 Idx: 5000 Loss: 0.010898233820622305
Epoch: 28 Idx: 0 Loss: 0.007568940456390193
Epoch: 28 Idx: 5000 Loss: 0.015997656736492012
Epoch: 29 Idx: 0 Loss: 0.00907102202528256
Epoch: 29 Idx: 5000 Loss: 0.01810632496696021
Epoch: 30 Idx: 0 Loss: 0.02162028600147127
Epoch: 30 Idx: 5000 Loss: 0.009810466054530132
Epoch: 31 Idx: 0 Loss: 0.012330892391383925
Epoch: 31 Idx: 5000 Loss: 0.0039946814973975744
Epoch: 32 Idx: 0 Loss: 0.028878251025758583
Epoch: 32 Idx: 5000 Loss: 0.016045061631209025
Epoch: 33 Idx: 0 Loss: 0.010556581577046035
Epoch: 33 Idx: 5000 Loss: 0.025538665279620693
Epoch: 34 Idx: 0 Loss: 0.01612873769854397
Epoch: 34 Idx: 5000 Loss: 0.01562637372816883
Epoch: 35 Idx: 0 Loss: 0.014719051732222151
Epoch: 35 Idx: 5000 Loss: 0.03431624991180979
Epoch: 36 Idx: 0 Loss: 0.01933982581812526
Epoch: 36 Idx: 5000 Loss: 0.008399217768587947
Epoch: 37 Idx: 0 Loss: 0.008810937267534192
Epoch: 37 Idx: 5000 Loss: 0.014625103136013158
Epoch: 38 Idx: 0 Loss: 0.022896269054878286
Epoch: 38 Idx: 5000 Loss: 0.011423753829907126
Epoch: 39 Idx: 0 Loss: 0.007100745476357631
Epoch: 39 Idx: 5000 Loss: 0.013387105720447897
Epoch: 40 Idx: 0 Loss: 0.02060838559478984
Epoch: 40 Idx: 5000 Loss: 0.02185116186455291
Epoch: 41 Idx: 0 Loss: 0.021594965078475487
Epoch: 41 Idx: 5000 Loss: 0.00792720950031854
Epoch: 42 Idx: 0 Loss: 0.03044903351394523
Epoch: 42 Idx: 5000 Loss: 0.01347230447469958
Epoch: 43 Idx: 0 Loss: 0.01354575888781464
Epoch: 43 Idx: 5000 Loss: 0.015330460388453758
Epoch: 44 Idx: 0 Loss: 0.011408698466573821
Epoch: 44 Idx: 5000 Loss: 0.011676970846416394
Epoch: 45 Idx: 0 Loss: 0.021090869737329113
Epoch: 45 Idx: 5000 Loss: 0.014043586929874882
Epoch: 46 Idx: 0 Loss: 0.004752787043994251
Epoch: 46 Idx: 5000 Loss: 0.012773629798725974
Epoch: 47 Idx: 0 Loss: 0.024700392492986466
Epoch: 47 Idx: 5000 Loss: 0.011269883454115727
Epoch: 48 Idx: 0 Loss: 0.010971934130184265
Epoch: 48 Idx: 5000 Loss: 0.03565604557667313
Epoch: 49 Idx: 0 Loss: 0.014917780140750275
Epoch: 49 Idx: 5000 Loss: 0.012328256775494668
Len (direct inputs):  877
Inputs len 5600 17 7800
Len (direct inputs):  2217
Starting sliding window evaluation...
Step 8/7
Val onto:  [('cmt', 'iasted')] test_onto:  [('confof', 'sigkdd')]
Training size: 111351 Testing size: 2336
Epoch: 0 Idx: 0 Loss: 0.21309677240528213
Epoch: 0 Idx: 5000 Loss: 0.013045721458729564
Epoch: 1 Idx: 0 Loss: 0.008839989429164384
Epoch: 1 Idx: 5000 Loss: 0.004093720520275547
Epoch: 2 Idx: 0 Loss: 0.016900962305034142
Epoch: 2 Idx: 5000 Loss: 0.0074188703686766996
Epoch: 3 Idx: 0 Loss: 0.015045870458384352
Epoch: 3 Idx: 5000 Loss: 0.016171110547289032
Epoch: 4 Idx: 0 Loss: 0.012094144741424973
Epoch: 4 Idx: 5000 Loss: 0.008379871709636062
Epoch: 5 Idx: 0 Loss: 0.01628587207471402
Epoch: 5 Idx: 5000 Loss: 0.008962511261149997
Epoch: 6 Idx: 0 Loss: 0.026721844878749984
Epoch: 6 Idx: 5000 Loss: 0.00941294706480545
Epoch: 7 Idx: 0 Loss: 0.00745733061144181
Epoch: 7 Idx: 5000 Loss: 0.0210923018154165
Epoch: 8 Idx: 0 Loss: 0.008156658769574924
Epoch: 8 Idx: 5000 Loss: 0.010871318957053212
Epoch: 9 Idx: 0 Loss: 0.009227510016390524
Epoch: 9 Idx: 5000 Loss: 0.02357061602196818
Epoch: 10 Idx: 0 Loss: 0.014955191659082162
Epoch: 10 Idx: 5000 Loss: 0.025292599617814458
Epoch: 11 Idx: 0 Loss: 0.021793453683133207
Epoch: 11 Idx: 5000 Loss: 0.01597373647114717
Epoch: 12 Idx: 0 Loss: 0.01417790885797845
Epoch: 12 Idx: 5000 Loss: 0.014606939170850732
Epoch: 13 Idx: 0 Loss: 0.013221939501538903
Epoch: 13 Idx: 5000 Loss: 0.008170421901664974
Epoch: 14 Idx: 0 Loss: 0.006937158977482899
Epoch: 14 Idx: 5000 Loss: 0.024257198542101514
Epoch: 15 Idx: 0 Loss: 0.014769368587948072
Epoch: 15 Idx: 5000 Loss: 0.01823826538391706
Epoch: 16 Idx: 0 Loss: 0.016171828005247052
Epoch: 16 Idx: 5000 Loss: 0.0215689123122256
Epoch: 17 Idx: 0 Loss: 0.013221581061748002
Epoch: 17 Idx: 5000 Loss: 0.014617535734519824
Epoch: 18 Idx: 0 Loss: 0.019874848751446246
Epoch: 18 Idx: 5000 Loss: 0.010080032058598885
Epoch: 19 Idx: 0 Loss: 0.04645551471156099
Epoch: 19 Idx: 5000 Loss: 0.012200095507948327
Epoch: 20 Idx: 0 Loss: 0.016623484455143903
Epoch: 20 Idx: 5000 Loss: 0.015408326508166204
Epoch: 21 Idx: 0 Loss: 0.023278586400380608
Epoch: 21 Idx: 5000 Loss: 0.019817864215264373
Epoch: 22 Idx: 0 Loss: 0.014733572516321486
Epoch: 22 Idx: 5000 Loss: 0.018082958285859513
Epoch: 23 Idx: 0 Loss: 0.01098545739837008
Traceback (most recent call last):
  File "main.py", line 517, in <module>
    loss.backward()
  File "/u/harshitk/miniconda3/envs/py37/lib/python3.7/site-packages/torch/tensor.py", line 185, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/u/harshitk/miniconda3/envs/py37/lib/python3.7/site-packages/torch/autograd/__init__.py", line 127, in backward
    allow_unreachable=True)  # allow_unreachable flag
KeyboardInterrupt

------------------------------------------------------------
Sender: LSF System <rer@dccxc260>
Subject: Job 4066898: <python main.py 10 2 True True> in cluster <dcc> Exited

Job <python main.py 10 2 True True> was submitted from host <dccxl004> by user <harshitk> in cluster <dcc> at Tue Sep 15 15:48:41 2020
Job was executed on host(s) <dccxc260>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep 15 15:49:40 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/VeeAlign/src> was used as the working directory.
Started at Tue Sep 15 15:49:40 2020
Terminated at Wed Sep 16 04:38:40 2020
Results reported at Wed Sep 16 04:38:40 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python main.py 10 2 True True
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   46074.45 sec.
    Max Memory :                                 2816 MB
    Average Memory :                             2661.83 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40601.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                13
    Run time :                                   46168 sec.
    Turnaround time :                            46199 sec.

The output (if any) is above this job summary.

