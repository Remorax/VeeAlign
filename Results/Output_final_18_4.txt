2020-09-15 15:48:43.664842: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-09-15 15:48:50.139264: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1
2020-09-15 15:48:50.259054: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:1e:00.0 name: Tesla K80 computeCapability: 3.7
coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s
2020-09-15 15:48:50.259118: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-09-15 15:48:50.261486: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
2020-09-15 15:48:50.303653: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2020-09-15 15:48:50.353501: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2020-09-15 15:48:50.406618: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2020-09-15 15:48:50.439766: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10
2020-09-15 15:48:50.440035: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudnn.so.7'; dlerror: libcudnn.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/share/lsf-9.1.3/10.1/linux3.10-glibc2.17-x86_64/lib:=/opt/share/gcc-4.9.2_rhel6/x86_64/lib/:/opt/share/gcc-4.9.2_rhel6/x86_64/lib64:/opt/share/Python-3.6.2/x86_64/lib:=/opt/share/gcc-5.4.0/x86_64/lib/:/opt/share/gcc-5.4.0/x86_64/lib64:/opt/share/isl-0.17/x86_64/lib/:/opt/share/protobuf-3.1.0/x86_64/lib/:/opt/share/leveldb-1.19/x86_64/lib/:/opt/share/boost-1.62.0/x86_64/lib/:/opt/share/torch-7/x86_64/install/lib:/opt/share/Python-2.7.12/x86_64/lib:/opt/share/Python-3.5.2/x86_64/lib:/opt/share/cuDNN-v5.1-8.0/cuda/lib64:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/opt/share/cuda-8.0/
2020-09-15 15:48:50.440056: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1753] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2020-09-15 15:48:50.440438: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-09-15 15:48:50.474019: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2600120000 Hz
2020-09-15 15:48:50.474258: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55acd57b7fd0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-09-15 15:48:50.474278: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-09-15 15:48:50.476640: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-09-15 15:48:50.476660: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      
Prefix path:  /dccstor/cogfin/arvind/da/VeeAlign/
Ontologies being aligned are:  [('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/confOf.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/conference.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/conference.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/conference.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/conference.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/confOf.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/conference.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/confOf.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/confOf.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/confOf.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl')]
Total number of extracted unique classes and properties from entire RA set:  829
Constructing abbrevation resolution dict....
Results after abbreviation resolution:  {'PC': 'Program Committee', 'OC': 'Organizing Committee'}
Resolving abbreviations...
Number of entities: 119639
Starting sliding window evaluation...
Step 0/7
Val onto:  [('confof', 'ekaw')] test_onto:  [('conference', 'sigkdd')]
Training size: 112565 Testing size: 3871
Epoch: 0 Idx: 0 Loss: 0.18093154821269566
Epoch: 0 Idx: 5000 Loss: 0.017988196043374205
Epoch: 1 Idx: 0 Loss: 0.016604793738175437
Epoch: 1 Idx: 5000 Loss: 0.019723597293152432
Epoch: 2 Idx: 0 Loss: 0.01973001943582682
Epoch: 2 Idx: 5000 Loss: 0.021551922207018993
Epoch: 3 Idx: 0 Loss: 0.009428301661021647
Epoch: 3 Idx: 5000 Loss: 0.013640805278581152
Epoch: 4 Idx: 0 Loss: 0.017588435014155863
Epoch: 4 Idx: 5000 Loss: 0.007640558501079371
Epoch: 5 Idx: 0 Loss: 0.01933234231938259
Epoch: 5 Idx: 5000 Loss: 0.02753430099062562
Epoch: 6 Idx: 0 Loss: 0.03493310500674679
Epoch: 6 Idx: 5000 Loss: 0.009593934044735174
Epoch: 7 Idx: 0 Loss: 0.013692228811779185
Epoch: 7 Idx: 5000 Loss: 0.011498928989517803
Epoch: 8 Idx: 0 Loss: 0.024169155729155383
Epoch: 8 Idx: 5000 Loss: 0.012313883570476499
Epoch: 9 Idx: 0 Loss: 0.042070417588401916
Epoch: 9 Idx: 5000 Loss: 0.009283177831480803
Epoch: 10 Idx: 0 Loss: 0.020051988056511393
Epoch: 10 Idx: 5000 Loss: 0.02080264228501332
Epoch: 11 Idx: 0 Loss: 0.012688960735581317
Epoch: 11 Idx: 5000 Loss: 0.012093619335716326
Epoch: 12 Idx: 0 Loss: 0.00863587588173522
Epoch: 12 Idx: 5000 Loss: 0.014591813343561308
Epoch: 13 Idx: 0 Loss: 0.005735943966853143
Epoch: 13 Idx: 5000 Loss: 0.011962618791237806
Epoch: 14 Idx: 0 Loss: 0.015523241256137802
Epoch: 14 Idx: 5000 Loss: 0.00862797106302473
Epoch: 15 Idx: 0 Loss: 0.019800512201595125
Epoch: 15 Idx: 5000 Loss: 0.02899923024674317
Epoch: 16 Idx: 0 Loss: 0.007998552269874788
Epoch: 16 Idx: 5000 Loss: 0.007666710249671959
Epoch: 17 Idx: 0 Loss: 0.017485600970481466
Epoch: 17 Idx: 5000 Loss: 0.023283637474997614
Epoch: 18 Idx: 0 Loss: 0.023122203693207107
Epoch: 18 Idx: 5000 Loss: 0.01512018451144851
Epoch: 19 Idx: 0 Loss: 0.023057984959155915
Epoch: 19 Idx: 5000 Loss: 0.030609198122055332
Epoch: 20 Idx: 0 Loss: 0.009230057513844002
Epoch: 20 Idx: 5000 Loss: 0.01399318003679055
Epoch: 21 Idx: 0 Loss: 0.010647924228995666
Epoch: 21 Idx: 5000 Loss: 0.019421590040560145
Epoch: 22 Idx: 0 Loss: 0.01295139832460728
Epoch: 22 Idx: 5000 Loss: 0.012470171962640593
Epoch: 23 Idx: 0 Loss: 0.014624484877638818
Epoch: 23 Idx: 5000 Loss: 0.017228834203390494
Epoch: 24 Idx: 0 Loss: 0.029229498946466315
Epoch: 24 Idx: 5000 Loss: 0.010979360429446236
Epoch: 25 Idx: 0 Loss: 0.019476069162075085
Epoch: 25 Idx: 5000 Loss: 0.013257503633329556
Epoch: 26 Idx: 0 Loss: 0.014699073591871784
Epoch: 26 Idx: 5000 Loss: 0.03101324488755025
Epoch: 27 Idx: 0 Loss: 0.00788541667157712
Epoch: 27 Idx: 5000 Loss: 0.033390414698909146
Epoch: 28 Idx: 0 Loss: 0.01666546732617975
Epoch: 28 Idx: 5000 Loss: 0.02016139730639671
Epoch: 29 Idx: 0 Loss: 0.022017468560346247
Epoch: 29 Idx: 5000 Loss: 0.005897852933772442
Epoch: 30 Idx: 0 Loss: 0.014120299118501727
Epoch: 30 Idx: 5000 Loss: 0.005900132017279035
Epoch: 31 Idx: 0 Loss: 0.013455944775963378
Epoch: 31 Idx: 5000 Loss: 0.017941713506929025
Epoch: 32 Idx: 0 Loss: 0.01237102358553388
Epoch: 32 Idx: 5000 Loss: 0.009706469380020165
Epoch: 33 Idx: 0 Loss: 0.015043009833436738
Epoch: 33 Idx: 5000 Loss: 0.0229325618428637
Epoch: 34 Idx: 0 Loss: 0.037493526024247686
Epoch: 34 Idx: 5000 Loss: 0.028388155156541833
Epoch: 35 Idx: 0 Loss: 0.017150779798846258
Epoch: 35 Idx: 5000 Loss: 0.016122479467327384
Epoch: 36 Idx: 0 Loss: 0.014636693147598514
Epoch: 36 Idx: 5000 Loss: 0.006025479632788827
Epoch: 37 Idx: 0 Loss: 0.03169898394787741
Epoch: 37 Idx: 5000 Loss: 0.03405335982826592
Epoch: 38 Idx: 0 Loss: 0.020802014699061726
Epoch: 38 Idx: 5000 Loss: 0.024786890457233077
Epoch: 39 Idx: 0 Loss: 0.011468418540230268
Epoch: 39 Idx: 5000 Loss: 0.008773786750307799
Epoch: 40 Idx: 0 Loss: 0.008868918622023592
Epoch: 40 Idx: 5000 Loss: 0.01301732412602836
Epoch: 41 Idx: 0 Loss: 0.014154464207915423
Epoch: 41 Idx: 5000 Loss: 0.005375202326277114
Epoch: 42 Idx: 0 Loss: 0.012038608637166041
Epoch: 42 Idx: 5000 Loss: 0.013122572423685086
Epoch: 43 Idx: 0 Loss: 0.021559810491594687
Epoch: 43 Idx: 5000 Loss: 0.014019090291698361
Epoch: 44 Idx: 0 Loss: 0.010162644891108888
Epoch: 44 Idx: 5000 Loss: 0.015013447311991612
Epoch: 45 Idx: 0 Loss: 0.03005077358513699
Epoch: 45 Idx: 5000 Loss: 0.015136060970764555
Epoch: 46 Idx: 0 Loss: 0.009029350971721563
Epoch: 46 Idx: 5000 Loss: 0.03278589019839786
Epoch: 47 Idx: 0 Loss: 0.01215632723658024
Epoch: 47 Idx: 5000 Loss: 0.004964098231214215
Epoch: 48 Idx: 0 Loss: 0.00866177420184567
Epoch: 48 Idx: 5000 Loss: 0.018458849933429663
Epoch: 49 Idx: 0 Loss: 0.008913662247738676
Epoch: 49 Idx: 5000 Loss: 0.01385976703839551
Len (direct inputs):  429
Inputs len 2744 15 3856
Len (direct inputs):  1127
Starting sliding window evaluation...
Step 2/7
Val onto:  [('conference', 'ekaw')] test_onto:  [('cmt', 'sigkdd')]
Training size: 111450 Testing size: 2364
Epoch: 0 Idx: 0 Loss: 0.12942003294397927
Epoch: 0 Idx: 5000 Loss: 0.013032615138663804
Epoch: 1 Idx: 0 Loss: 0.00955314896197074
Epoch: 1 Idx: 5000 Loss: 0.01580590533101065
Epoch: 2 Idx: 0 Loss: 0.008758728004007413
Epoch: 2 Idx: 5000 Loss: 0.014434306384404374
Epoch: 3 Idx: 0 Loss: 0.04721604891679744
Epoch: 3 Idx: 5000 Loss: 0.008279365387636614
Epoch: 4 Idx: 0 Loss: 0.010302678913373983
Epoch: 4 Idx: 5000 Loss: 0.018513777929935228
Epoch: 5 Idx: 0 Loss: 0.013556089486482209
Epoch: 5 Idx: 5000 Loss: 0.01801452850666349
Epoch: 6 Idx: 0 Loss: 0.011466258545137902
Epoch: 6 Idx: 5000 Loss: 0.011842821992801667
Epoch: 7 Idx: 0 Loss: 0.030981376993648838
Epoch: 7 Idx: 5000 Loss: 0.01983627121527464
Epoch: 8 Idx: 0 Loss: 0.016756038892911905
Epoch: 8 Idx: 5000 Loss: 0.013969631455313838
Epoch: 9 Idx: 0 Loss: 0.02251562891487663
Epoch: 9 Idx: 5000 Loss: 0.012150850263685897
Epoch: 10 Idx: 0 Loss: 0.006887223266384716
Epoch: 10 Idx: 5000 Loss: 0.00894298292787704
Epoch: 11 Idx: 0 Loss: 0.01928287378418828
Epoch: 11 Idx: 5000 Loss: 0.017287939921530618
Epoch: 12 Idx: 0 Loss: 0.004187471345119026
Epoch: 12 Idx: 5000 Loss: 0.016612993592568855
Epoch: 13 Idx: 0 Loss: 0.006876265525659496
Epoch: 13 Idx: 5000 Loss: 0.012678512559622764
Epoch: 14 Idx: 0 Loss: 0.005272695133819754
Epoch: 14 Idx: 5000 Loss: 0.013398178192556431
Epoch: 15 Idx: 0 Loss: 0.011778299918026175
Epoch: 15 Idx: 5000 Loss: 0.025432552482347135
Epoch: 16 Idx: 0 Loss: 0.013786860096375548
Epoch: 16 Idx: 5000 Loss: 0.017498502860023264
Epoch: 17 Idx: 0 Loss: 0.017295629779416958
Epoch: 17 Idx: 5000 Loss: 0.02020503506691295
Epoch: 18 Idx: 0 Loss: 0.012915500792346643
Epoch: 18 Idx: 5000 Loss: 0.016974263606358196
Epoch: 19 Idx: 0 Loss: 0.01959768809038559
Epoch: 19 Idx: 5000 Loss: 0.007502358202084715
Epoch: 20 Idx: 0 Loss: 0.02007199796002486
Epoch: 20 Idx: 5000 Loss: 0.007654895352464736
Epoch: 21 Idx: 0 Loss: 0.009593570261727087
Epoch: 21 Idx: 5000 Loss: 0.00936286266156797
Epoch: 22 Idx: 0 Loss: 0.011949040960733699
Epoch: 22 Idx: 5000 Loss: 0.025901647725267887
Epoch: 23 Idx: 0 Loss: 0.03781517195633657
Epoch: 23 Idx: 5000 Loss: 0.022206335931703916
Epoch: 24 Idx: 0 Loss: 0.033184876385636386
Epoch: 24 Idx: 5000 Loss: 0.02133372558180255
Epoch: 25 Idx: 0 Loss: 0.016711524315218173
Epoch: 25 Idx: 5000 Loss: 0.03074423953150114
Epoch: 26 Idx: 0 Loss: 0.019912189760573695
Epoch: 26 Idx: 5000 Loss: 0.015152025334981795
Epoch: 27 Idx: 0 Loss: 0.01561216348102231
Epoch: 27 Idx: 5000 Loss: 0.006024134766479147
Epoch: 28 Idx: 0 Loss: 0.026846434567427864
Epoch: 28 Idx: 5000 Loss: 0.020421372795240793
Epoch: 29 Idx: 0 Loss: 0.012043664472447663
Epoch: 29 Idx: 5000 Loss: 0.009169793612869289
Epoch: 30 Idx: 0 Loss: 0.014890862359891628
Epoch: 30 Idx: 5000 Loss: 0.029687793948222956
Epoch: 31 Idx: 0 Loss: 0.03673162919344067
Epoch: 31 Idx: 5000 Loss: 0.014051159249475172
Epoch: 32 Idx: 0 Loss: 0.013271220490983806
Epoch: 32 Idx: 5000 Loss: 0.013435473684663536
Epoch: 33 Idx: 0 Loss: 0.015226940038159144
Epoch: 33 Idx: 5000 Loss: 0.009754852963333218
Epoch: 34 Idx: 0 Loss: 0.01153600285539466
Epoch: 34 Idx: 5000 Loss: 0.013262759194471226
Epoch: 35 Idx: 0 Loss: 0.009673785627992027
Epoch: 35 Idx: 5000 Loss: 0.011480831336807528
Epoch: 36 Idx: 0 Loss: 0.012100376673370524
Epoch: 36 Idx: 5000 Loss: 0.019403862188774455
Epoch: 37 Idx: 0 Loss: 0.008263908138452191
Epoch: 37 Idx: 5000 Loss: 0.012746429701689353
Epoch: 38 Idx: 0 Loss: 0.0075455785724787144
Epoch: 38 Idx: 5000 Loss: 0.010274947449170225
Epoch: 39 Idx: 0 Loss: 0.02056096750067951
Epoch: 39 Idx: 5000 Loss: 0.030804202319764717
Epoch: 40 Idx: 0 Loss: 0.009423591233596719
Epoch: 40 Idx: 5000 Loss: 0.01934190110078739
Epoch: 41 Idx: 0 Loss: 0.0202687419136637
Epoch: 41 Idx: 5000 Loss: 0.01016652915454678
Epoch: 42 Idx: 0 Loss: 0.015528800757925235
Epoch: 42 Idx: 5000 Loss: 0.015518146634451129
Epoch: 43 Idx: 0 Loss: 0.010150893519485456
Epoch: 43 Idx: 5000 Loss: 0.012513352474115093
Epoch: 44 Idx: 0 Loss: 0.010899781711326703
Epoch: 44 Idx: 5000 Loss: 0.017661315958275806
Epoch: 45 Idx: 0 Loss: 0.01117270840674291
Epoch: 45 Idx: 5000 Loss: 0.025076251658187085
Epoch: 46 Idx: 0 Loss: 0.007799656562813982
Epoch: 46 Idx: 5000 Loss: 0.0218505281114424
Epoch: 47 Idx: 0 Loss: 0.005104998689197696
Epoch: 47 Idx: 5000 Loss: 0.014835104807802365
Epoch: 48 Idx: 0 Loss: 0.020913326942959345
Epoch: 48 Idx: 5000 Loss: 0.011124927867914804
Epoch: 49 Idx: 0 Loss: 0.010502854462438087
Epoch: 49 Idx: 5000 Loss: 0.01849615255337403
Len (direct inputs):  1737
Inputs len 1372 12 2352
Len (direct inputs):  992
Starting sliding window evaluation...
Step 4/7
Val onto:  [('ekaw', 'sigkdd')] test_onto:  [('cmt', 'conference')]
Training size: 111356 Testing size: 4145
Epoch: 0 Idx: 0 Loss: 0.14603081717407562
Epoch: 0 Idx: 5000 Loss: 0.016417873270545707
Epoch: 1 Idx: 0 Loss: 0.008798131768995299
Epoch: 1 Idx: 5000 Loss: 0.010078439619686349
Epoch: 2 Idx: 0 Loss: 0.015229917538979575
Epoch: 2 Idx: 5000 Loss: 0.011905658202675348
Epoch: 3 Idx: 0 Loss: 0.006689428823368464
Epoch: 3 Idx: 5000 Loss: 0.011256865508572777
Epoch: 4 Idx: 0 Loss: 0.03705640331407114
Epoch: 4 Idx: 5000 Loss: 0.010892380213407734
Epoch: 5 Idx: 0 Loss: 0.026312352626142774
Epoch: 5 Idx: 5000 Loss: 0.009979717342472716
Epoch: 6 Idx: 0 Loss: 0.006768433408767595
Epoch: 6 Idx: 5000 Loss: 0.02271931646777673
Epoch: 7 Idx: 0 Loss: 0.011761573594199177
Epoch: 7 Idx: 5000 Loss: 0.01419483004788358
Epoch: 8 Idx: 0 Loss: 0.02173731449058455
Epoch: 8 Idx: 5000 Loss: 0.01687353127062704
Epoch: 9 Idx: 0 Loss: 0.021592400963804485
Epoch: 9 Idx: 5000 Loss: 0.03844406279275629
Epoch: 10 Idx: 0 Loss: 0.014545604706772982
Epoch: 10 Idx: 5000 Loss: 0.01048781995177871
Epoch: 11 Idx: 0 Loss: 0.021329109735788825
Epoch: 11 Idx: 5000 Loss: 0.012127024948299406
Epoch: 12 Idx: 0 Loss: 0.018493893337997166
Epoch: 12 Idx: 5000 Loss: 0.01285972505126972
Epoch: 13 Idx: 0 Loss: 0.025851536625124894
Epoch: 13 Idx: 5000 Loss: 0.009936726104845464
Epoch: 14 Idx: 0 Loss: 0.02601172991112983
Epoch: 14 Idx: 5000 Loss: 0.01165978310720645
Epoch: 15 Idx: 0 Loss: 0.004922923476032091
Epoch: 15 Idx: 5000 Loss: 0.035630231031142996
Epoch: 16 Idx: 0 Loss: 0.015876503728348288
Epoch: 16 Idx: 5000 Loss: 0.012372135722479609
Epoch: 17 Idx: 0 Loss: 0.014755351644308882
Epoch: 17 Idx: 5000 Loss: 0.015375174984452088
Epoch: 18 Idx: 0 Loss: 0.04288060822306024
Epoch: 18 Idx: 5000 Loss: 0.009508317455708954
Epoch: 19 Idx: 0 Loss: 0.020535054273002777
Epoch: 19 Idx: 5000 Loss: 0.016287330846552213
Epoch: 20 Idx: 0 Loss: 0.014319448730332446
Epoch: 20 Idx: 5000 Loss: 0.012651619968663413
Epoch: 21 Idx: 0 Loss: 0.02716216716445178
Epoch: 21 Idx: 5000 Loss: 0.01100999234014128
Epoch: 22 Idx: 0 Loss: 0.010299631199716018
Epoch: 22 Idx: 5000 Loss: 0.010104928742893659
Epoch: 23 Idx: 0 Loss: 0.012605673729556599
Epoch: 23 Idx: 5000 Loss: 0.009894354898640973
Epoch: 24 Idx: 0 Loss: 0.011464066539247128
Epoch: 24 Idx: 5000 Loss: 0.02064194060685671
Epoch: 25 Idx: 0 Loss: 0.013730503747226494
Epoch: 25 Idx: 5000 Loss: 0.006922908461528174
Epoch: 26 Idx: 0 Loss: 0.014088506284925618
Epoch: 26 Idx: 5000 Loss: 0.011856588234741765
Epoch: 27 Idx: 0 Loss: 0.025936391921518732
Epoch: 27 Idx: 5000 Loss: 0.023629182087935897
Epoch: 28 Idx: 0 Loss: 0.05351517536082011
Epoch: 28 Idx: 5000 Loss: 0.015976285751388063
Epoch: 29 Idx: 0 Loss: 0.010187503359840207
Epoch: 29 Idx: 5000 Loss: 0.014108146337346847
Epoch: 30 Idx: 0 Loss: 0.0067045749481712425
Epoch: 30 Idx: 5000 Loss: 0.014589714975421733
Epoch: 31 Idx: 0 Loss: 0.0074310771296847355
Epoch: 31 Idx: 5000 Loss: 0.013459121617278347
Epoch: 32 Idx: 0 Loss: 0.022622203359674638
Epoch: 32 Idx: 5000 Loss: 0.009057348630753762
Epoch: 33 Idx: 0 Loss: 0.008321354098735977
Epoch: 33 Idx: 5000 Loss: 0.014497936187558885
Epoch: 34 Idx: 0 Loss: 0.015130721227541308
Epoch: 34 Idx: 5000 Loss: 0.015094238467869888
Epoch: 35 Idx: 0 Loss: 0.022486014719157746
Epoch: 35 Idx: 5000 Loss: 0.021007302281809064
Epoch: 36 Idx: 0 Loss: 0.008485371052110589
Epoch: 36 Idx: 5000 Loss: 0.012765462175887633
Epoch: 37 Idx: 0 Loss: 0.014258660321090133
Epoch: 37 Idx: 5000 Loss: 0.01789755335861062
Epoch: 38 Idx: 0 Loss: 0.009936807520056816
Epoch: 38 Idx: 5000 Loss: 0.007252933300001425
Epoch: 39 Idx: 0 Loss: 0.011650077322576613
Epoch: 39 Idx: 5000 Loss: 0.012757076480574073
Epoch: 40 Idx: 0 Loss: 0.01988375467399892
Epoch: 40 Idx: 5000 Loss: 0.021100775389733574
Epoch: 41 Idx: 0 Loss: 0.005191004958239365
Epoch: 41 Idx: 5000 Loss: 0.014462214171927973
Epoch: 42 Idx: 0 Loss: 0.017364549915728396
Epoch: 42 Idx: 5000 Loss: 0.020731888599590065
Epoch: 43 Idx: 0 Loss: 0.008769335577221234
Epoch: 43 Idx: 5000 Loss: 0.008943261008052657
Epoch: 44 Idx: 0 Loss: 0.02391104413461837
Epoch: 44 Idx: 5000 Loss: 0.016309268359169845
Epoch: 45 Idx: 0 Loss: 0.014006431936672996
Epoch: 45 Idx: 5000 Loss: 0.020662273149750277
Epoch: 46 Idx: 0 Loss: 0.025647007432069203
Epoch: 46 Idx: 5000 Loss: 0.03656652939717725
Epoch: 47 Idx: 0 Loss: 0.008171476524944543
Epoch: 47 Idx: 5000 Loss: 0.023802114244545567
Epoch: 48 Idx: 0 Loss: 0.013619623702587954
Epoch: 48 Idx: 5000 Loss: 0.010694171514852979
Epoch: 49 Idx: 0 Loss: 0.025610926035036882
Epoch: 49 Idx: 5000 Loss: 0.0077052287831899284
Len (direct inputs):  561
Inputs len 1568 15 4130
Len (direct inputs):  2577
Starting sliding window evaluation...
Step 6/7
Val onto:  [('edas', 'sigkdd')] test_onto:  [('conference', 'edas')]
Training size: 106045 Testing size: 7817
Epoch: 0 Idx: 0 Loss: 0.21784089347338587
Epoch: 0 Idx: 5000 Loss: 0.013432748729086584
Epoch: 1 Idx: 0 Loss: 0.013034583990443112
Epoch: 1 Idx: 5000 Loss: 0.009479430689412854
Epoch: 2 Idx: 0 Loss: 0.008835806886758429
Epoch: 2 Idx: 5000 Loss: 0.027705460004916388
Epoch: 3 Idx: 0 Loss: 0.023283777571441136
Traceback (most recent call last):
  File "main.py", line 518, in <module>
    optimizer.step()
  File "/u/harshitk/miniconda3/envs/py37/lib/python3.7/site-packages/torch/autograd/grad_mode.py", line 15, in decorate_context
    return func(*args, **kwargs)
  File "/u/harshitk/miniconda3/envs/py37/lib/python3.7/site-packages/torch/optim/adam.py", line 96, in step
    grad = grad.add(p, alpha=group['weight_decay'])
KeyboardInterrupt

------------------------------------------------------------
Sender: LSF System <rer@dccxc248>
Subject: Job 4066835: <python main.py 4 18 False False> in cluster <dcc> Exited

Job <python main.py 4 18 False False> was submitted from host <dccxl004> by user <harshitk> in cluster <dcc> at Tue Sep 15 15:48:37 2020
Job was executed on host(s) <dccxc248>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep 15 15:48:38 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/VeeAlign/src> was used as the working directory.
Started at Tue Sep 15 15:48:38 2020
Terminated at Wed Sep 16 04:38:38 2020
Results reported at Wed Sep 16 04:38:38 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python main.py 4 18 False False
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   46186.44 sec.
    Max Memory :                                 2898 MB
    Average Memory :                             2678.37 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40519.00 MB
    Max Swap :                                   2 MB
    Max Processes :                              3
    Max Threads :                                13
    Run time :                                   46200 sec.
    Turnaround time :                            46201 sec.

The output (if any) is above this job summary.

