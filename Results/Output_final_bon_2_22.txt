2020-09-15 15:49:41.854411: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-09-15 15:49:45.081282: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1
2020-09-15 15:49:45.202859: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:1b:00.0 name: Tesla K80 computeCapability: 3.7
coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s
2020-09-15 15:49:45.202932: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-09-15 15:49:45.204888: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
2020-09-15 15:49:45.206493: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2020-09-15 15:49:45.206985: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2020-09-15 15:49:45.209034: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2020-09-15 15:49:45.210519: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10
2020-09-15 15:49:45.210762: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudnn.so.7'; dlerror: libcudnn.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/share/lsf-9.1.3/10.1/linux3.10-glibc2.17-x86_64/lib:=/opt/share/gcc-4.9.2_rhel6/x86_64/lib/:/opt/share/gcc-4.9.2_rhel6/x86_64/lib64:/opt/share/Python-3.6.2/x86_64/lib:=/opt/share/gcc-5.4.0/x86_64/lib/:/opt/share/gcc-5.4.0/x86_64/lib64:/opt/share/isl-0.17/x86_64/lib/:/opt/share/protobuf-3.1.0/x86_64/lib/:/opt/share/leveldb-1.19/x86_64/lib/:/opt/share/boost-1.62.0/x86_64/lib/:/opt/share/torch-7/x86_64/install/lib:/opt/share/Python-2.7.12/x86_64/lib:/opt/share/Python-3.5.2/x86_64/lib:/opt/share/cuDNN-v5.1-8.0/cuda/lib64:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/opt/share/cuda-8.0/
2020-09-15 15:49:45.210785: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1753] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2020-09-15 15:49:45.211108: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-09-15 15:49:45.218998: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2600010000 Hz
2020-09-15 15:49:45.219171: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55e2d2845190 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-09-15 15:49:45.219193: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-09-15 15:49:45.221024: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-09-15 15:49:45.221048: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      
Prefix path:  /dccstor/cogfin/arvind/da/VeeAlign/
Ontologies being aligned are:  [('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/confOf.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/conference.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/conference.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/conference.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/conference.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/confOf.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/conference.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/confOf.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/cmt.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/confOf.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/sigkdd.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/edas.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/ekaw.owl'), ('/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/confOf.owl', '/dccstor/cogfin/arvind/da/VeeAlign/datasets/conference/ontologies/iasted.owl')]
Total number of extracted unique classes and properties from entire RA set:  829
Constructing abbrevation resolution dict....
Results after abbreviation resolution:  {'PC': 'Program Committee', 'OC': 'Organizing Committee'}
Resolving abbreviations...
Number of entities: 119639
Starting sliding window evaluation...
Step 0/7
Val onto:  [('confof', 'ekaw')] test_onto:  [('conference', 'sigkdd')]
Training size: 112565 Testing size: 3871
Epoch: 0 Idx: 0 Loss: 0.19325756844406228
Epoch: 0 Idx: 5000 Loss: 0.003375233409232451
Epoch: 1 Idx: 0 Loss: 0.017658363874268893
Epoch: 1 Idx: 5000 Loss: 0.010942332018031735
Epoch: 2 Idx: 0 Loss: 0.00582197373759487
Epoch: 2 Idx: 5000 Loss: 0.016231577459812647
Epoch: 3 Idx: 0 Loss: 0.015798692324489952
Epoch: 3 Idx: 5000 Loss: 0.012148554063165
Epoch: 4 Idx: 0 Loss: 0.009261329828442279
Epoch: 4 Idx: 5000 Loss: 0.011872072211950805
Epoch: 5 Idx: 0 Loss: 0.012085019790401452
Epoch: 5 Idx: 5000 Loss: 0.011226762252555044
Epoch: 6 Idx: 0 Loss: 0.012649238649773641
Epoch: 6 Idx: 5000 Loss: 0.031197197685718332
Epoch: 7 Idx: 0 Loss: 0.02175811526291683
Epoch: 7 Idx: 5000 Loss: 0.0068964493213695
Epoch: 8 Idx: 0 Loss: 0.02618296023275811
Epoch: 8 Idx: 5000 Loss: 0.014329481782644003
Epoch: 9 Idx: 0 Loss: 0.023512931736059117
Epoch: 9 Idx: 5000 Loss: 0.01281898414238208
Epoch: 10 Idx: 0 Loss: 0.013226073611027228
Epoch: 10 Idx: 5000 Loss: 0.010914946021618825
Epoch: 11 Idx: 0 Loss: 0.014726219953050425
Epoch: 11 Idx: 5000 Loss: 0.027718776680373355
Epoch: 12 Idx: 0 Loss: 0.012318470507695576
Epoch: 12 Idx: 5000 Loss: 0.009095641773319325
Epoch: 13 Idx: 0 Loss: 0.023511186669542196
Epoch: 13 Idx: 5000 Loss: 0.031704361273800497
Epoch: 14 Idx: 0 Loss: 0.03812853283633623
Epoch: 14 Idx: 5000 Loss: 0.009640317425897459
Epoch: 15 Idx: 0 Loss: 0.016562356592071524
Epoch: 15 Idx: 5000 Loss: 0.016706674752181176
Epoch: 16 Idx: 0 Loss: 0.014556432509986464
Epoch: 16 Idx: 5000 Loss: 0.031976690927596696
Epoch: 17 Idx: 0 Loss: 0.019949264934879092
Epoch: 17 Idx: 5000 Loss: 0.013840403936258606
Epoch: 18 Idx: 0 Loss: 0.017219112486851918
Epoch: 18 Idx: 5000 Loss: 0.008246659747013274
Epoch: 19 Idx: 0 Loss: 0.009012695346512747
Epoch: 19 Idx: 5000 Loss: 0.01217469813857396
Epoch: 20 Idx: 0 Loss: 0.026701925596325768
Epoch: 20 Idx: 5000 Loss: 0.008463405153225453
Epoch: 21 Idx: 0 Loss: 0.014502408821652279
Epoch: 21 Idx: 5000 Loss: 0.00934816033513128
Epoch: 22 Idx: 0 Loss: 0.02040812598393319
Epoch: 22 Idx: 5000 Loss: 0.0070253415520797795
Epoch: 23 Idx: 0 Loss: 0.025937796679982895
Epoch: 23 Idx: 5000 Loss: 0.011647360535937706
Epoch: 24 Idx: 0 Loss: 0.010330341507873122
Epoch: 24 Idx: 5000 Loss: 0.025396526689588944
Epoch: 25 Idx: 0 Loss: 0.009424705512985434
Epoch: 25 Idx: 5000 Loss: 0.014553059763112938
Epoch: 26 Idx: 0 Loss: 0.011424701503904355
Epoch: 26 Idx: 5000 Loss: 0.016321126949867818
Epoch: 27 Idx: 0 Loss: 0.012562490435824076
Epoch: 27 Idx: 5000 Loss: 0.015236166523733373
Epoch: 28 Idx: 0 Loss: 0.013678071014996662
Epoch: 28 Idx: 5000 Loss: 0.0264663560453212
Epoch: 29 Idx: 0 Loss: 0.011514665912801994
Epoch: 29 Idx: 5000 Loss: 0.013458785775377416
Epoch: 30 Idx: 0 Loss: 0.019896111981844353
Epoch: 30 Idx: 5000 Loss: 0.005813491105119063
Epoch: 31 Idx: 0 Loss: 0.01833928858536164
Epoch: 31 Idx: 5000 Loss: 0.007561498099732677
Epoch: 32 Idx: 0 Loss: 0.02666406497217655
Epoch: 32 Idx: 5000 Loss: 0.019134941627219397
Epoch: 33 Idx: 0 Loss: 0.012543194285698682
Epoch: 33 Idx: 5000 Loss: 0.011162394717643876
Epoch: 34 Idx: 0 Loss: 0.029432085851858494
Epoch: 34 Idx: 5000 Loss: 0.019295839878725754
Epoch: 35 Idx: 0 Loss: 0.011058188071553263
Epoch: 35 Idx: 5000 Loss: 0.016296068002996853
Epoch: 36 Idx: 0 Loss: 0.02237537897867997
Epoch: 36 Idx: 5000 Loss: 0.012028574975142575
Epoch: 37 Idx: 0 Loss: 0.027004320992590883
Epoch: 37 Idx: 5000 Loss: 0.030417402552052684
Epoch: 38 Idx: 0 Loss: 0.010525877805605488
Epoch: 38 Idx: 5000 Loss: 0.017687284059886538
Epoch: 39 Idx: 0 Loss: 0.018904395071579636
Epoch: 39 Idx: 5000 Loss: 0.00829683291065837
Epoch: 40 Idx: 0 Loss: 0.015084465394977117
Epoch: 40 Idx: 5000 Loss: 0.018758755649829772
Epoch: 41 Idx: 0 Loss: 0.011735408301896387
Epoch: 41 Idx: 5000 Loss: 0.012440444679683287
Epoch: 42 Idx: 0 Loss: 0.011414101692000793
Epoch: 42 Idx: 5000 Loss: 0.010864890191378679
Epoch: 43 Idx: 0 Loss: 0.02443741436302274
Epoch: 43 Idx: 5000 Loss: 0.022911379525063995
Epoch: 44 Idx: 0 Loss: 0.010515547204009287
Epoch: 44 Idx: 5000 Loss: 0.014455197222583264
Epoch: 45 Idx: 0 Loss: 0.011957027471574684
Epoch: 45 Idx: 5000 Loss: 0.009905214268495444
Epoch: 46 Idx: 0 Loss: 0.022979893951389024
Epoch: 46 Idx: 5000 Loss: 0.03914651830133786
Epoch: 47 Idx: 0 Loss: 0.030285869396915317
Epoch: 47 Idx: 5000 Loss: 0.011228120492962313
Epoch: 48 Idx: 0 Loss: 0.022960419095896394
Epoch: 48 Idx: 5000 Loss: 0.02920947710789104
Epoch: 49 Idx: 0 Loss: 0.02614799424687867
Epoch: 49 Idx: 5000 Loss: 0.014180756581499643
Len (direct inputs):  429
Inputs len 2744 15 3856
Len (direct inputs):  1127
Starting sliding window evaluation...
Step 2/7
Val onto:  [('conference', 'ekaw')] test_onto:  [('cmt', 'sigkdd')]
Training size: 111450 Testing size: 2364
Epoch: 0 Idx: 0 Loss: 0.13521108976941443
Epoch: 0 Idx: 5000 Loss: 0.03717127424160417
Epoch: 1 Idx: 0 Loss: 0.01475309727861377
Epoch: 1 Idx: 5000 Loss: 0.03151419000619766
Epoch: 2 Idx: 0 Loss: 0.017858662867217108
Epoch: 2 Idx: 5000 Loss: 0.02939507092435699
Epoch: 3 Idx: 0 Loss: 0.03220521184093026
Epoch: 3 Idx: 5000 Loss: 0.01929725281499569
Epoch: 4 Idx: 0 Loss: 0.017070872273025717
Epoch: 4 Idx: 5000 Loss: 0.02965398673522026
Epoch: 5 Idx: 0 Loss: 0.04326687995886808
Epoch: 5 Idx: 5000 Loss: 0.010934659155586364
Epoch: 6 Idx: 0 Loss: 0.0235998835073501
Epoch: 6 Idx: 5000 Loss: 0.01674578511481159
Epoch: 7 Idx: 0 Loss: 0.02357099596788398
Epoch: 7 Idx: 5000 Loss: 0.006795708664896795
Epoch: 8 Idx: 0 Loss: 0.026187319434641237
Epoch: 8 Idx: 5000 Loss: 0.018678692222327972
Epoch: 9 Idx: 0 Loss: 0.020287388801475542
Epoch: 9 Idx: 5000 Loss: 0.014917930460732103
Epoch: 10 Idx: 0 Loss: 0.026189472534507388
Epoch: 10 Idx: 5000 Loss: 0.012601801766401565
Epoch: 11 Idx: 0 Loss: 0.013875552147357324
Epoch: 11 Idx: 5000 Loss: 0.017591951740676733
Epoch: 12 Idx: 0 Loss: 0.008852507498307561
Epoch: 12 Idx: 5000 Loss: 0.02112627724328475
Epoch: 13 Idx: 0 Loss: 0.01191157077234833
Epoch: 13 Idx: 5000 Loss: 0.01105172529062998
Epoch: 14 Idx: 0 Loss: 0.007556013309068641
Epoch: 14 Idx: 5000 Loss: 0.008784427805818755
Epoch: 15 Idx: 0 Loss: 0.014763154019907873
Epoch: 15 Idx: 5000 Loss: 0.012242018465021275
Epoch: 16 Idx: 0 Loss: 0.025298263130917986
Epoch: 16 Idx: 5000 Loss: 0.010452775794073006
Epoch: 17 Idx: 0 Loss: 0.060723177441453384
Epoch: 17 Idx: 5000 Loss: 0.026087571114713153
Epoch: 18 Idx: 0 Loss: 0.02034316316390551
Epoch: 18 Idx: 5000 Loss: 0.00878970335851187
Epoch: 19 Idx: 0 Loss: 0.02857947029890509
Epoch: 19 Idx: 5000 Loss: 0.023164953233044037
Epoch: 20 Idx: 0 Loss: 0.014231728128776466
Epoch: 20 Idx: 5000 Loss: 0.013174347794528524
Epoch: 21 Idx: 0 Loss: 0.01947139986093402
Epoch: 21 Idx: 5000 Loss: 0.014909972415334285
Epoch: 22 Idx: 0 Loss: 0.0197939766309446
Epoch: 22 Idx: 5000 Loss: 0.019156527693641998
Epoch: 23 Idx: 0 Loss: 0.011416147252311812
Epoch: 23 Idx: 5000 Loss: 0.01959435890024729
Epoch: 24 Idx: 0 Loss: 0.013124607254779111
Epoch: 24 Idx: 5000 Loss: 0.02815278260726881
Epoch: 25 Idx: 0 Loss: 0.04211840994452455
Epoch: 25 Idx: 5000 Loss: 0.007709386634073905
Epoch: 26 Idx: 0 Loss: 0.01525745719190808
Epoch: 26 Idx: 5000 Loss: 0.01753816570433126
Epoch: 27 Idx: 0 Loss: 0.01619351512203409
Epoch: 27 Idx: 5000 Loss: 0.016276641489783302
Epoch: 28 Idx: 0 Loss: 0.03465045257823415
Epoch: 28 Idx: 5000 Loss: 0.01916054433859459
Epoch: 29 Idx: 0 Loss: 0.014809481522311058
Epoch: 29 Idx: 5000 Loss: 0.014949759484225809
Epoch: 30 Idx: 0 Loss: 0.01158835773737395
Epoch: 30 Idx: 5000 Loss: 0.018737022393044245
Epoch: 31 Idx: 0 Loss: 0.016830570584896606
Epoch: 31 Idx: 5000 Loss: 0.016310817896385665
Epoch: 32 Idx: 0 Loss: 0.007918338286612389
Epoch: 32 Idx: 5000 Loss: 0.027688846133987897
Epoch: 33 Idx: 0 Loss: 0.026355248155957703
Epoch: 33 Idx: 5000 Loss: 0.010392937152274235
Epoch: 34 Idx: 0 Loss: 0.0055768662403024635
Epoch: 34 Idx: 5000 Loss: 0.015915198159293598
Epoch: 35 Idx: 0 Loss: 0.011949170862330664
Epoch: 35 Idx: 5000 Loss: 0.009169376537759281
Epoch: 36 Idx: 0 Loss: 0.013128130429704248
Epoch: 36 Idx: 5000 Loss: 0.020176229952687516
Epoch: 37 Idx: 0 Loss: 0.004908533937162414
Epoch: 37 Idx: 5000 Loss: 0.007582334430823834
Epoch: 38 Idx: 0 Loss: 0.018318593234863005
Epoch: 38 Idx: 5000 Loss: 0.012151815658506461
Epoch: 39 Idx: 0 Loss: 0.011255731873489442
Epoch: 39 Idx: 5000 Loss: 0.018185732303979248
Epoch: 40 Idx: 0 Loss: 0.010777774163827098
Epoch: 40 Idx: 5000 Loss: 0.012592790782328101
Epoch: 41 Idx: 0 Loss: 0.019279145095936884
Epoch: 41 Idx: 5000 Loss: 0.008819205191005522
Epoch: 42 Idx: 0 Loss: 0.009277069622870985
Epoch: 42 Idx: 5000 Loss: 0.01228359581331908
Epoch: 43 Idx: 0 Loss: 0.011153758744069393
Epoch: 43 Idx: 5000 Loss: 0.008508877049037977
Epoch: 44 Idx: 0 Loss: 0.02999612012792731
Epoch: 44 Idx: 5000 Loss: 0.016916756095188293
Epoch: 45 Idx: 0 Loss: 0.04459223417987433
Epoch: 45 Idx: 5000 Loss: 0.010121649907961602
Epoch: 46 Idx: 0 Loss: 0.01643774421653766
Epoch: 46 Idx: 5000 Loss: 0.012805646324912514
Epoch: 47 Idx: 0 Loss: 0.011549468376614516
Epoch: 47 Idx: 5000 Loss: 0.036398621989390256
Epoch: 48 Idx: 0 Loss: 0.015874797009075577
Epoch: 48 Idx: 5000 Loss: 0.011417048294221781
Epoch: 49 Idx: 0 Loss: 0.01919129076307631
Epoch: 49 Idx: 5000 Loss: 0.014795520682064278
Len (direct inputs):  1737
Inputs len 1372 12 2352
Len (direct inputs):  992
Starting sliding window evaluation...
Step 4/7
Val onto:  [('ekaw', 'sigkdd')] test_onto:  [('cmt', 'conference')]
Training size: 111356 Testing size: 4145
Epoch: 0 Idx: 0 Loss: 0.1364921963274765
Epoch: 0 Idx: 5000 Loss: 0.039001702982754266
Epoch: 1 Idx: 0 Loss: 0.030607111229948712
Epoch: 1 Idx: 5000 Loss: 0.016005020287331462
Epoch: 2 Idx: 0 Loss: 0.02654404636225078
Epoch: 2 Idx: 5000 Loss: 0.017659331776362044
Epoch: 3 Idx: 0 Loss: 0.010784847949397956
Epoch: 3 Idx: 5000 Loss: 0.020138933486047682
Epoch: 4 Idx: 0 Loss: 0.014427668912800147
Epoch: 4 Idx: 5000 Loss: 0.0067299204968829995
Epoch: 5 Idx: 0 Loss: 0.024201491233393067
Epoch: 5 Idx: 5000 Loss: 0.00999501515514459
Epoch: 6 Idx: 0 Loss: 0.007674630869928179
Epoch: 6 Idx: 5000 Loss: 0.021091942677241182
Epoch: 7 Idx: 0 Loss: 0.019523753274575888
Epoch: 7 Idx: 5000 Loss: 0.01917536888319532
Epoch: 8 Idx: 0 Loss: 0.01327533566854087
Epoch: 8 Idx: 5000 Loss: 0.008314449603820355
Epoch: 9 Idx: 0 Loss: 0.021198859493363142
Epoch: 9 Idx: 5000 Loss: 0.01308225040719183
Epoch: 10 Idx: 0 Loss: 0.012765110562117463
Epoch: 10 Idx: 5000 Loss: 0.033852880212112434
Epoch: 11 Idx: 0 Loss: 0.009558963261232426
Epoch: 11 Idx: 5000 Loss: 0.020285980334185293
Epoch: 12 Idx: 0 Loss: 0.010416056462718959
Epoch: 12 Idx: 5000 Loss: 0.014239518070128031
Epoch: 13 Idx: 0 Loss: 0.01519363987769548
Epoch: 13 Idx: 5000 Loss: 0.010064079716491389
Epoch: 14 Idx: 0 Loss: 0.036278166396329775
Epoch: 14 Idx: 5000 Loss: 0.011391955310533323
Epoch: 15 Idx: 0 Loss: 0.01685031117996679
Epoch: 15 Idx: 5000 Loss: 0.02355487951055873
Epoch: 16 Idx: 0 Loss: 0.015065246466330815
Epoch: 16 Idx: 5000 Loss: 0.014949962077685086
Epoch: 17 Idx: 0 Loss: 0.018643601891155603
Epoch: 17 Idx: 5000 Loss: 0.017200244269194812
Epoch: 18 Idx: 0 Loss: 0.012445884643169383
Epoch: 18 Idx: 5000 Loss: 0.017736444753135547
Epoch: 19 Idx: 0 Loss: 0.007247084123497213
Epoch: 19 Idx: 5000 Loss: 0.02103007226509948
Epoch: 20 Idx: 0 Loss: 0.013552206472176795
Epoch: 20 Idx: 5000 Loss: 0.025105928334725823
Epoch: 21 Idx: 0 Loss: 0.010985929782577813
Epoch: 21 Idx: 5000 Loss: 0.012345032356294701
Epoch: 22 Idx: 0 Loss: 0.02522797186919372
Epoch: 22 Idx: 5000 Loss: 0.009565106221661231
Epoch: 23 Idx: 0 Loss: 0.014483520702958764
Epoch: 23 Idx: 5000 Loss: 0.016995845275303244
Epoch: 24 Idx: 0 Loss: 0.024918270101068892
Epoch: 24 Idx: 5000 Loss: 0.036986562110567854
Epoch: 25 Idx: 0 Loss: 0.00837323137792416
Epoch: 25 Idx: 5000 Loss: 0.015316664233134235
Epoch: 26 Idx: 0 Loss: 0.016755954391567365
Epoch: 26 Idx: 5000 Loss: 0.011734111941724842
Epoch: 27 Idx: 0 Loss: 0.020661517135737367
Epoch: 27 Idx: 5000 Loss: 0.015012788239099373
Epoch: 28 Idx: 0 Loss: 0.014276646678013523
Epoch: 28 Idx: 5000 Loss: 0.015057319132750067
Epoch: 29 Idx: 0 Loss: 0.011794370500759316
Epoch: 29 Idx: 5000 Loss: 0.014058837624024214
Epoch: 30 Idx: 0 Loss: 0.015557196736940351
Epoch: 30 Idx: 5000 Loss: 0.02539378479921757
Epoch: 31 Idx: 0 Loss: 0.012434274473681714
Epoch: 31 Idx: 5000 Loss: 0.013354496537828877
Epoch: 32 Idx: 0 Loss: 0.012851671366300962
Epoch: 32 Idx: 5000 Loss: 0.006726989393491344
Epoch: 33 Idx: 0 Loss: 0.01414971220274644
Epoch: 33 Idx: 5000 Loss: 0.017326334350377406
Epoch: 34 Idx: 0 Loss: 0.026399721672927055
Epoch: 34 Idx: 5000 Loss: 0.008681151447050398
Epoch: 35 Idx: 0 Loss: 0.008660312050122734
Epoch: 35 Idx: 5000 Loss: 0.012796507771947675
Epoch: 36 Idx: 0 Loss: 0.009472139308314732
Epoch: 36 Idx: 5000 Loss: 0.02040910373625427
Epoch: 37 Idx: 0 Loss: 0.02174775354991159
Epoch: 37 Idx: 5000 Loss: 0.013250188343864419
Epoch: 38 Idx: 0 Loss: 0.00860112565883858
Epoch: 38 Idx: 5000 Loss: 0.020488353029520144
Epoch: 39 Idx: 0 Loss: 0.0135778263119507
Epoch: 39 Idx: 5000 Loss: 0.007107836759629107
Epoch: 40 Idx: 0 Loss: 0.019355427429991804
Epoch: 40 Idx: 5000 Loss: 0.016771432919098078
Epoch: 41 Idx: 0 Loss: 0.00947452973537936
Epoch: 41 Idx: 5000 Loss: 0.020758148278970184
Epoch: 42 Idx: 0 Loss: 0.024607278220699684
Epoch: 42 Idx: 5000 Loss: 0.013949572732243173
Epoch: 43 Idx: 0 Loss: 0.012182371676021861
Epoch: 43 Idx: 5000 Loss: 0.007578327369982267
Epoch: 44 Idx: 0 Loss: 0.03305583324397478
Epoch: 44 Idx: 5000 Loss: 0.011999052933047833
Epoch: 45 Idx: 0 Loss: 0.01532768117607526
Epoch: 45 Idx: 5000 Loss: 0.015606534420575864
Epoch: 46 Idx: 0 Loss: 0.018802121839651985
Epoch: 46 Idx: 5000 Loss: 0.017911131790531422
Epoch: 47 Idx: 0 Loss: 0.012574265474654626
Epoch: 47 Idx: 5000 Loss: 0.014128698359195944
Epoch: 48 Idx: 0 Loss: 0.03609595336493718
Epoch: 48 Idx: 5000 Loss: 0.01062629459954619
Epoch: 49 Idx: 0 Loss: 0.01656154192575317
Epoch: 49 Idx: 5000 Loss: 0.013677657939737162
Len (direct inputs):  561
Inputs len 1568 15 4130
Len (direct inputs):  2577
Starting sliding window evaluation...
Step 6/7
Val onto:  [('edas', 'sigkdd')] test_onto:  [('conference', 'edas')]
Training size: 106045 Testing size: 7817
Epoch: 0 Idx: 0 Loss: 0.223239139055865
Traceback (most recent call last):
  File "main.py", line 514, in <module>
    outputs = model(node_elems, inp_elems)
  File "/u/harshitk/miniconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "main.py", line 312, in forward
    feature_emb = self.name_embedding(features[i]) #  dim: (2, batch_size, max_types, max_paths, max_pathlen, 512)
  File "/u/harshitk/miniconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/u/harshitk/miniconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/sparse.py", line 126, in forward
    self.norm_type, self.scale_grad_by_freq, self.sparse)
  File "/u/harshitk/miniconda3/envs/py37/lib/python3.7/site-packages/torch/nn/functional.py", line 1814, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
KeyboardInterrupt

------------------------------------------------------------
Sender: LSF System <rer@dccxc213>
Subject: Job 4066907: <python main.py 22 2 True False> in cluster <dcc> Exited

Job <python main.py 22 2 True False> was submitted from host <dccxl004> by user <harshitk> in cluster <dcc> at Tue Sep 15 15:48:41 2020
Job was executed on host(s) <dccxc213>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep 15 15:49:40 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/VeeAlign/src> was used as the working directory.
Started at Tue Sep 15 15:49:40 2020
Terminated at Wed Sep 16 04:38:37 2020
Results reported at Wed Sep 16 04:38:37 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python main.py 22 2 True False
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   46132.57 sec.
    Max Memory :                                 2881 MB
    Average Memory :                             2707.61 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40536.00 MB
    Max Swap :                                   2 MB
    Max Processes :                              3
    Max Threads :                                13
    Run time :                                   46137 sec.
    Turnaround time :                            46196 sec.

The output (if any) is above this job summary.

